topsep
ex
pt
examples
examples
example
Example
section
hang
blue
hang
blue
hang
blue
hang
blue
hang
blue
Genifer
-an
artificial
general
intelligence
YKY
general
intelligence
Gmail
com
with
input
from
Abram
Demski
Ben
Goertzel
Martin
Magnusson
William
Taysom
Russell
Wallace
Pei
Wang
latest
revision
plain
pt
Preface
executive
summary
to
do
list
This
book
is
perpetual
draft
My
personal
reason
for
developing
AGI
is
to
achieve
life
extension
The
source
code
of
is
hosted
on
http
code
google
com
genifer
Google
Code
including
some
very
easy
http
code
google
com
genifer
downloads
list
tutorial
slides
Also
feel
free
to
mailto
Generic
Intelligence
Gmail
com
contact
me
-YKY
Executive
summary
em
Inference
Genifer
descended
from
classical
logic
based
Its
modes
of
inference
are
deduction
abduction
explaining
and
induction
learning
This
is
common
to
NARS
OpenCog
Cyc
Logic
Genifer
is
based
on
an
algebra
of
concept
composition
which
replaces
predicate
logic
as
the
internal
structure
of
propositions
KB
Genifer's
KB
stores
logic
formulas
similar
to
classical
systems
such
as
Cyc
and
NARS
OpenCog
is
an
exception
in
that
it
stores
its
knowledge
as
hypergraph
called
AtomSpace
Uncertainty
Genifer
uses
fuzzy
probabilistic
logic
the
probabilistic
part
is
an
exact
algorithm
for
belief
propagation
in
Bayesian
networks
The
fuzzy
probabilistic
calculus
is
created
by
YKY
based
on
the
Beta
distribution
Bootstrapping
Genifer
will
be
written
in
its
own
language
which
is
logical
functional
programming
language
based
on
Genifer's
logic
and
an
existing
functional
programming
language
such
as
Clojure
or
Haskell
em
To
do
Ch
Introduction
Explain
the
new
ideas
that
learned
about
the
relationship
between
propositional
logic
and
topological
logic
Ch
Architecture
Explain
AIXI
algorithmic
complexity
Solomonoff
induction
etc
Explain
distributive
architecture
New
idea
that
bootstrapping
is
possible
Ch
KR
-ok
-Ch
Logic
New
logic
of
concept
composition
Ideas
about
equational
unification
and
concepts
Explain
background
notions
eg
paradoxes
Ch
Add
new
idea
on
the
"Java
girl
paradox
"which
is
in
draft
paper
Ch
Inference
Copy
and
paste
Bayesian
inference
and
factor
graph
stuff
from
the
Lisp
code
to
here
Ch
Pattern
recognition
Matrix
technique
on
similarity
Ch
Learning
lot
of
new
material
is
in
the
slides
Ch
NL
New
idea
of
semantic
parsing
New
diagrams
from
GUI
Ch
Memory
Explain
hierarchical
clustering
idea
ontology
Ch
Planning
May
need
re
think
Ch
Implementation
Bootstrap
Genifer
in
its
own
language
Appendix
Recommend
more
books
for
AGI
sub
areas
Especially
math
books
Introduction
am
an
enthusiast
but
not
crank
in
the
sense
that
have
some
pet
theories
as
to
the
proper
construction
of
flying
machine
wish
to
avail
myself
of
all
that
is
already
known
and
then
if
possible
add
my
mite
to
help
on
the
future
worker
who
will
attain
final
success
-Wilbur
Wright
Everything
should
be
made
as
simple
as
possible
but
no
simpler
-Albert
Einstein
rephrased
When
subject
becomes
totally
obsolete
we
make
it
required
course
-Peter
Drucker
This
book
describes
theory
of
AGI
Artificial
General
Intelligence
that
is
still
being
developed
think
the
AGI
problem
can
be
decomposed
into
computational
issues
and
we
will
somehow
integrate
them
together
cm
knowledge
representation
fuzzy
probabilistic
logic
deduction
abductive
reasoning
inductive
learning
pattern
recognition
categorization
belief
revision
memory
organization
natural
language
sensory
processing
cm
My
approach
is
predominantly
logic
based
but
it
also
employs
redundancy
in
knowledge
representation
including
sub
symbolic
knowledge
and
therefore
is
somewhat
like
neural
networks
Also
it
may
merge
with
neural
networks
at
the
sensory
level
This
approach
can
be
called
"neo
classical
AI
"Some
background
of
AI
The
word
"logic
"comes
from
logos
which
can
mean
"word
""thought
"or
"reason
"The
study
of
logic
is
the
study
of
the
mechanisms
of
thinking
Aristotle
ca
BC
is
often
credited
with
the
first
substantial
study
of
logic
with
focus
on
syllogisms
Our
current
system
of
logics
was
developed
by
people
such
as
De
Morgan
Boole
Frege
Begriffsschrift
Russell
and
Whitehead
Principia
Mathematica
and
others
including
Liebniz
"algebra
of
thought
"whose
work
remain
undiscovered
till
the
Logic
based
AI
Thus
it
is
not
so
surprising
that
the
design
of
AI
can
be
based
on
logic
John
McCarthy
who
coined
the
term
"AI
"in
Darthmouth
conference
was
first
to
propose
the
use
of
formal
logic
in
AI
Herbrand
created
basis
of
provability
in
predicate
logic
In
Robinson
discovered
the
resolution
method
for
logical
inference
which
enabled
the
creation
of
logic
programming
and
the
language
Prolog
Kowalski
and
Colmerauer
Connectionism
In
McCulloch
and
Pitts
formulated
formal
model
of
neurons
leading
to
Rosenblatt's
Perceptron
and
later
the
computational
paradigm
of
artificial
neural
networks
ANNs
In
the
there
was
resurgence
of
interests
in
ANNs
and
connectionism
due
to
the
invention
of
the
backpropagation
algorithm
for
multilayer
perceptrons
At
this
point
it
was
recognized
that
connectionism
has
the
advantages
of
robustness
and
graded
response
in
contrast
to
logic
based
AI's
brittleness
and
bivalence
Recent
trends
In
the
there
was
rapid
development
in
statistical
learning
It
was
then
realized
that
ANN
learning
algorithms
are
special
case
of
statistical
learning
Recent
development
in
AI
is
distanced
from
"GOFAI
"Good
Old
Fashioned
AI
by
their
use
of
statistical
learning
sub
symbolic
representations
and
optimization
methods
computational
intelligence
Computational
intelligence
includes
new
paradigms
such
as
evolutionary
computing
drawing
inspiration
from
sexual
reproduction
and
swarm
intelligence
drawing
from
social
interactions
Why
logic
started
designing
AGI
using
the
neural
network
approach
for
few
years
until
discovered
some
fundamental
difficulties
in
the
NN
approach
so
switched
to
more
logic
based
one
(Though
my
approach
is
not
purely
logic
based
)Logic
based
AI
went
out
of
vogue
beginning
in
the
because
of
the
advent
of
connectionism
and
later
statistical
learning
methods
As
result
many
researchers
nowadays
are
unfamiliar
with
even
the
basics
of
logic
based
AI
In
order
to
understand
my
approach
it
is
extremely
important
to
be
familiar
with
first
order
logic
FOL
and
to
understand
the
difference
between
first
order
representations
and
propositional
ones
Propositional
vs
first
order
This
is
typical
propositional
statement
Compare
it
with
typical
first
order
statement
The
chief
distinction
is
the
use
of
variables
in
first
order
logic
which
greatly
increases
its
expressiveness
The
vast
majority
of
statistical
learning
literature
assumes
that
the
data
is
represented
by
points
in
high
dimensional
space
call
this
the
"spatial
"approach
which
includes
methods
like
nearest
neighbor
support
vector
machines
SVMs
principal
component
analysis
PCA
and
artificial
neural
networks
ANNs
Spatial
datasets
are
equivalent
to
propositional
representations
First
order
logic
is
symbolic
or
relational
(Relational
representations
are
subclass
of
first
order
representations
that
do
not
allow
functors
and
structured
terms
)approach
which
is
qualitatively
very
different
There
is
strong
evidence
that
some
datasets
can
be
easily
learned
by
relational
methods
but
are
very
difficult
if
not
impossible
to
learn
with
spatial
methods
Thornton
(He
demonstrated
this
with
the
example
of
"checkerboard
"pattern
of
and
that
can
be
easily
learned
by
logical
formula
but
would
be
very
difficult
for
neural
network
learner
This
is
actually
the
age
old
debate
between
symbolic
AI
and
connectionism
given
new
twist
in
the
context
of
machine
learning
)provides
an
interesting
and
detailed
analysis
of
this
issue
To
put
it
more
bluntly
suspect
that
propositional
approaches
are
rather
useless
in
the
logic
based
AI
framework
but
I'd
be
pleasantly
surprised
to
learn
otherwise
Notice
in
the
figure
below
that
all
spatial
classifiers
work
by
"chopping
"the
space
of
data
points
shown
in
here
into
various
regions
with
the
use
of
hyper
planes
as
line
in
or
some
curved
boundaries
This
is
very
different
from
how
first
order
logic
classifies
data
Spatial
vs
logical
classification
To
illustrate
this
with
an
example
Let's
think
of
how
child
AGI
or
human
learns
the
concept
of
blood
relatives
The
child
Jane
would
be
given
some
examples
of
people
around
her
and
whether
they
are
her
relatives
eg
father
jane
john
relative
john
mother
jane
mary
relative
mary
uncle
jane
pete
relative
pete
neighbor
jane
joe
relative
joe
friend
jane
kate
relative
kate
and
the
task
is
to
learn
general
rule
for
relatives
The
solution
can
be
stated
quite
succinctly
(This
solution
is
not
entirely
correct
as
relatedness
can
grow
unbounded
and
everyone
would
be
ultimately
blood
related
Perhaps
this
problem
can
be
resolved
by
fuzziness
and
other
mechanisms
such
as
non
monotonicity
but
my
point
here
is
to
show
that
the
situation
for
propositional
representation
is
even
worse
as
the
problem
appears
insurmountable
in
that
case
)in
first
order
logic
relative
parent
relative
sibling
relative
married
relative
relative
relative
but
it
is
very
difficult
to
express
in
propositional
logic
unless
we
limit
the
domain
of
entities
to
few
people
Also
we
can
see
that
spatial
statistical
learning
will
fail
to
learn
this
rule
because
The
dataset
cannot
be
represented
as
numerical
values
in
vector
space
or
it
could
be
done
only
very
awkwardly
Even
when
the
dataset
is
cast
in
vector
space
the
learning
algorithm
can
mostly
learn
to
classify
existing
examples
but
the
generalizations
would
be
wrong
-this
is
because
the
formulae
in
first
order
logic
can
entail
discrete
examples
that
are
not
necessarily
located
in
localized
region
in
the
numerical
space
Even
if
you
carve
the
space
into
ridiculously
complex
regions
the
next
example
would
still
be
an
exception
because
"spatial
compactness
"is
simply
absent
in
the
underlying
concept
The
child's
world
typically
has
very
few
people
in
it
yet
she
is
able
to
learn
the
concept
In
ANNs
and
statistical
learning
the
sample
size
is
typically
at
least
but
logic
based
learning
can
learn
the
concept
with
just
few
examples
Although
first
order
representations
can
be
converted
to
propositional
ones
via
the
process
of
propositionalization
such
algorithms
cost
exponential
time
and
space
This
is
not
difficult
to
see
FOL
allows
us
to
express
knowledge
very
succinctly
There
are
some
techniques
that
ameliorate
the
combinatorial
explosion
such
as
partial
instantiation
Chandru
or
sparse
matrix
Domingos
But
still
think
it
is
easier
and
more
intuitive
to
working
on
FOL
KB
directly
especially
for
AGI
And
despite
propositionalization
the
class
of
spatial
statistical
learning
techniques
still
seem
to
be
unsuitable
for
logic
based
AGI
because
propositionalization
does
not
cure
the
fundamental
lack
of
"compressiveness
"of
predicate
logic
that
pointed
out
above
After
propositionalization
some
fast
propositional
SATisfiability
algorithms
can
be
invoked
but
they
are
still
qualitatively
different
from
the
spatial
learning
algorithms
From
this
consideration
my
current
strategy
is
to
focus
on
algorithms
specifically
for
FOL
HOL
predicate
logic
The
application
of
kernel
methods
to
logic
seems
to
rely
on
syntactic
distance
rather
than
semantic
distance
Eg
developed
support
vector
ILP
method
describes
method
that
constructs
kernels
for
first
order
and
higher
order
logic
formulae
based
on
representation
of
logic
by
typed
lambda
calculus
in
It
involves
the
use
of
"matching
kernel
"that
measures
syntactic
distance
only
So
far
have
not
seen
an
effective
way
to
estimate
semantic
distance
without
performing
logical
inference
Some
new
techniques
have
been
developed
to
lift
neural
networks
to
first
order
representations
but
have
not
examined
them
in
detail
eg
To
do
I've
gained
some
new
insight
into
the
issue
of
mapping
predicate
logic
to
continuous
space
via
algebraic
logic
Why
not
neural
network
There
are
several
reasons
why
think
the
NN
approach
may
be
less
promising
First
order
logic
is
more
powerful
representation
scheme
than
feed
forward
neural
networks
sec
why
logic
whereas
dynamic
neural
networks
are
very
difficult
to
work
with
neuron
is
fixed
within
network
and
cannot
move
around
which
seems
to
make
it
difficult
to
perform
invariant
pattern
recognition
eg
translational
rotational
and
scale
invariance
in
vision
The
brain
has
to
use
neurons
due
to
biological
constraints
but
it
seems
more
effective
to
use
other
pattern
matching
methods
on
von
Neumann
machines
Scale
invariance
is
particularly
difficult
for
ANNs
see
Neural
learning
is
slower
and
require
larger
amount
of
examples
Logic
based
learning
is
coarse
grained
and
thus
require
fewer
examples
to
induce
the
correct
representation
sometimes
as
few
as
example
As
Ben
Goertzel
pointed
out
some
years
ago
network
of
redundant
propositions
can
be
reduced
to
minimum
number
of
non
redundant
propositions
without
loss
of
information
the
only
thing
that
is
lost
is
fault
tolerance
However
neural
networks
may
be
used
for
handling
low
level
vision
especially
at
the
feature
extraction
level
Recursive
self
improvement
RSI
refers
to
the
ability
of
an
AGI
to
reprogram
itself
Some
authors
predict
that
the
RSI
point
will
trigger
the
Technological
Singularity
eg
think
the
way
to
reach
the
RSI
point
with
the
least
amount
of
efforts
would
be
to
build
an
automatic
program
synthesis
tool
that
accepts
natural
language
commands
or
goal
specifications
From
then
on
we
can
use
this
tool
to
rewrite
the
tool
itself
and
to
evolve
it
semi
automatically
into
full
AGI
system
Chicken
and
egg
problem
chicken
and
egg
problem
Anyone
who
has
thought
about
AGI
long
enough
will
be
aware
of
this
problem
The
idea
is
to
use
"program
evolver
"that
takes
an
input
program
and
improves
it
to
according
to
some
user
specifications
We
put
the
evolver
through
itself
thus
building
up
its
intelligence
recursively
without
doing
any
programming
except
for
the
initial
evolver
This
idea
at
least
naively
does
not
work
because
the
initial
evolver
needs
to
have
very
good
background
knowledge
about
programming
or
else
it
cannot
perform
its
job
in
reasonable
time
sec
self
programming
architecture
discusses
how
to
make
it
feasible
Natural
reasoning
Other
names
for
natural
reasoning
are
common
sense
reasoning
human
like
reasoning
informal
reasoning
Natural
reasoning
is
an
extension
of
logical
reasoning
with
ability
to
recognize
natural
concepts
which
posit
requires
fuzzy
pattern
recognition
ability
to
use
metaphors
similes
analogies
and
similarity
based
reasoning
An
example
of
natural
reasoning
is
cm
Suppose
need
to
write
program
to
"break
English
sentences
into
words
"I'd
need
to
declare
function
to
do
this
What
would
be
the
input
and
output
of
this
function
cm
Note
that
in
the
above
reasoning
think
of
the
function
as
"box
"with
something
that
goes
in
and
something
out
Natural
reasoning
is
required
to
turn
informal
natural
language
statements
into
formal
statements
This
is
especially
important
to
formal
program
synthesis
Architecture
We
will
first
look
at
various
basic
architectures
and
then
try
to
decide
on
final
design
Logical
reasoner
blackboard
architecture
This
is
an
older
design
explored
around
chiefly
to
put
together
several
logic
based
algorithms
on
blackboard
The
following
page
is
schematic
diagram
showing
various
modules
of
the
logical
sub
system
My
intuition
is
that
such
logical
sub
system
is
essential
to
any
AGI
but
it
should
be
built
on
top
of
more
basic
procedural
layer
cf
sec
proc
subsumes
decl
"Procedural
subsumes
Declarative
"Here
is
simple
top
level
algorithm
to
process
incoming
sensory
events
Top
level
sensory
processing
raw
sensory
input
updated
KB
alg
top
level
sensory
processing
Upon
the
arrival
of
raw
sensory
input
perform
pattern
recognition
ch
pattern
recognition
which
is
the
same
as
forward
chaining
sec
deduction
NL
input
can
bypass
this
step
The
result
will
be
set
of
new
facts
and
will
be
put
in
Working
Memory
Forward
chaining
can
be
performed
several
times
on
top
of
previous
results
Perform
consistency
check
sec
consistency
check
on
the
new
facts
against
the
KB
If
new
fact
is
inconsistent
with
the
current
KB
invoke
conflict
resolution
sec
conflict
resolution
Perform
abduction
sec
abduction
on
the
new
facts
so
WM
will
make
appropriate
assumptions
to
account
for
them
Invoke
the
inductive
learner
sec
inductive
learner
ie
try
to
compress
the
new
facts
KB
The
abduction
algorithm
is
almost
identical
to
the
consistency
check
algorithm
so
they
may
be
merged
The
abduction
algorithm
is
also
very
similar
to
the
induction
algorithm
cf
sec
abduction
so
all
may
be
merged
BDI
architecture
BDI
belief
desire
intention
is
one
of
the
most
successful
agent
architectures
ever
proposed
It
was
first
investigated
by
Michael
Bratman
as
theory
of
human
practical
reasoning
Bratman
and
later
formulated
as
an
agent
architecture
Bratman
The
main
concern
of
BDI
is
how
to
plan
in
the
face
of
dynamic
environment
and
with
bounded
resources
Key
to
BDI
is
the
idea
of
commitment
Once
an
agent
commits
to
plan
the
plan
will
constrain
means
end
reasoning
thus
making
planning
processes
more
efficient
Beliefs
are
the
contents
of
the
KB
Desires
are
unconstrained
they
need
not
be
achievable
or
even
consistent
eg
one
can
desire
smoking
and
good
health
at
the
same
time
Intentions
are
goals
that
the
agent
commits
to
they
are
believed
to
be
achievable
and
must
be
consistent
schematic
diagram
of
the
BDI
architecture
Figure
is
schematic
diagram
of
BDI
The
key
processing
steps
are
This
is
the
Knowledge
Declarative
Sub
system
which
we
will
consider
at
length
in
the
rest
of
the
book
Changes
in
the
environment
cause
changes
in
beliefs
which
in
turn
reveal
new
possibilities
to
satisfy
desires
Thus
the
Opportunity
Analyzer
tries
to
propose
new
options
Once
the
options
are
produced
either
by
the
Opportunity
Analyzer
or
the
Means
end
Reasoner
they
are
subject
to
filtering
The
Compatibility
Filter
checks
if
options
are
compatible
with
existing
plans
Surviving
options
are
weighted
against
each
other
by
the
Deliberation
Process
which
produces
intentions
to
be
incorporated
as
plans
The
Means
end
Reasoner
searches
for
the
next
available
options
taking
into
consideration
the
current
plans
Evolutionary
architecture
TO
DO
Hayek
is
not
the
correct
designation
for
this
architecture
Hayek
is
an
artificial
economy
proposed
by
Eric
Baum
Baum
as
an
AGI
architecture
slight
variant
of
Hayek
can
be
described
as
follows
The
AGI
is
composed
of
large
number
of
small
programs
Cooperativity
The
programs
may
call
each
other
Persistent
memories
Individual
programs
can
remember
things
across
time
slices
meta
controller
runs
these
programs
according
to
some
schedule
allotting
each
program
time
slice
Credit
assignment
If
the
program
answers
question
correctly
or
performs
good
action
judged
by
some
external
critic
the
meta
controller
will
credit
the
programs
that
have
contributed
to
the
result
Human
programmers
may
contribute
to
this
pool
of
programs
high
level
programming
language
such
as
Lisp
seems
to
be
more
suitable
for
this
purpose
Note
that
even
very
complex
algorithms
can
be
implemented
in
this
architecture
For
example
best
first
search
algorithm
can
remember
its
search
state
in
an
external
memory
store
Then
it
just
waits
for
its
time
slice
to
resume
searching
Thus
human
programmers
can
seed
the
artificial
economy
with
highly
competent
programs
Decision
theoretic
architecture
This
is
mathematically
more
elegant
than
the
BDI
architecture
Some
important
elements
that
should
be
present
in
the
architecture
percepts
are
raw
unprocessed
sensory
events
beliefs
are
the
contents
of
the
KB
states
-all
possible
perceived
states
of
the
environment
including
the
current
state
States
are
special
terms
in
the
logic
actions
utility
function
-function
Utilities
may
be
defined
implicitly
though
This
architecture
may
depend
on
logical
sub
system
that
recognizes
environmental
states
eg
"the
apple
is
on
the
table
"from
raw
percepts
eg
pixel
values
from
camera
recognizes
possible
actions
eg
"can
put
the
apple
on
the
plate
"The
goal
is
to
maximize
expected
utility
Planning
becomes
discrete
optimization
problem
in
this
setting
Reinforcement
learning
can
be
naturally
included
in
this
architecture
Self
programming
architecture
Consider
this
variant
of
the
chicken
and
egg
problem
sec
chicken
and
egg
where
we
distinguish
between
the
Synthesizer
and
the
AGI
Currently
we
know
the
following
program
synthesis
paradigms
human
programming
natural
reasoning
NR
ie
common
sense
human
like
reasoning
automated
theorem
proving
ATP
stochastic
local
search
SLS
including
evolutionary
algorithms
EA
reinforcement
learning
RL
We
will
consider
each
paradigm
in
turn
key
question
is
how
an
initially
dumb
AGI
can
improve
the
performance
of
the
Synthesizer
even
slightly
Human
programming
ie
brute
force
software
engineering
without
RSI
The
problem
with
this
approach
is
that
it
does
not
exploit
the
advantages
of
machine
program
synthesis
and
thus
is
likely
to
be
sub
optimal
Natural
reasoning
cf
sec
natural
reasoning
The
problem
is
that
NR
does
not
yet
exist
but
it
is
sine
qua
non
desideratum
of
AGI
So
the
question
is
how
to
enable
an
initially
weak
form
of
NR
to
synthesize
programs
One
possibility
is
to
use
creativity
the
AGI
will
generate
programs
by
partly
reasoning
and
partly
random
acting
An
agent
with
NR
is
likely
to
have
RL
as
well
An
NR
agent
can
invoke
ATP
An
NR
agent
can
self
program
without
invoking
ATP
simply
by
deductive
planning
sec
deductive
planning
ATP
It
requires
formal
specifications
of
programs
or
sub
routines
which
are
often
very
hard
to
write
for
humans
Search
in
proof
space
can
be
very
slow
There
already
exists
ATP
based
program
synthesis
software
which
can
be
used
externally
Stochastic
local
search
Is
based
on
generate
and
test
"Generate
"is
random
and
the
program
space
is
typically
huge
"Test
"is
often
slow
because
it
requires
the
entire
AGI
to
answer
large
number
of
benchmark
queries
possible
remedy
is
to
specify
input
out
benchmarks
for
AGI
components
but
then
the
burden
is
on
humans
to
design
the
modular
architecture
large
population
of
relatively
low
score
programs
has
to
be
maintained
in
order
to
allow
sufficient
time
for
cooperativity
to
evolve
For
this
reason
suspect
that
the
evolutionary
approach
is
rather
slow
further
problems
are
How
can
an
initially
dumb
AGI
improve
the
stochastic
searcher
(In
MOSES
cite
an
attempt
is
made
at
"representation
building
")And
how
can
this
transfer
of
rationality
from
the
AGI
to
the
Synthesizer
be
coded
once
and
for
all
Reinforcement
learning
The
goal
of
RL
is
to
learn
policies
of
how
to
act
in
certain
environmental
states
It
can
synthesize
programs
if
the
environment
is
that
of
programming
RL
can
be
augmented
with
logical
reasoning
RL
can
implement
the
idea
of
expected
utility
maximization
Combining
all
methods
Apparently
we
can
combine
all
the
above
methodologies
without
conflict
The
question
is
to
choose
the
most
effective
method
of
AGI
self
programming
red
lines
My
intuition
is
that
the
evolutionary
method
is
much
less
efficient
than
the
other
it
is
easier
to
explain
to
children
to
do
something
than
to
train
animals
by
reinforcement
which
is
in
turn
easier
than
breeding
animals
for
the
innate
ability
to
do
that
thing
provided
that
some
rudimentary
ability
of
reasoning
is
present
Maybe
the
most
cost
effective
method
from
the
human
labor
perspective
is
to
combine
RL
with
logical
reasoning
and
deductive
planning
-for
instance
to
allow
RL
to
invoke
IE
inference
engine
or
vice
versa
AIXI
Distributive
architecture
This
is
monolithic
view
of
the
inference
engine
For
simplicity's
sake
only
the
deduction
part
is
shown
since
deduction
is
the
most
essential
core
of
the
engine
This
is
distributive
architecture
for
deduction
Each
agent
responds
to
queries
and
spits
out
solutions
For
example
an
agent
may
have
rule
in
its
local
KB
that
professors
who
wear
sandals
are
nice
to
students
The
agent
listens
to
queries
trying
to
find
something
it
can
answer
query
such
as
who
is
nice
to
students
would
be
hit
Then
the
agent
either
returns
an
answer
if
it
knows
as
fact
that
XYZ
is
nice
to
students
applys
rule
and
returns
one
of
more
sub
goals
In
this
case
the
sub
goal
is
does
XYZ
wear
sandals
Wait
for
other
agents
to
answer
that
In
case
if
another
agent
provides
an
answer
Professor
Matt
Mahoney
wears
sandals
say
with
TV
and
sends
it
back
to
the
first
agent
then
the
first
agent
decides
how
to
calculate
the
TV
of
the
conclusion
given
that
TV
of
premise
The
only
calculation
it
needs
to
perform
is
for
the
rule
that
it
owns
Then
it
returns
the
answer
to
the
asker
This
architecture
is
so
wonderful
because
there
is
no
need
to
construct
the
proof
tree
anymore
The
proof
tree
seems
to
have
disappeared
but
it
is
really
implicitly
constructed
among
the
network
of
agents
and
the
messages
Thanks
to
Matt
Mahoney
for
proposing
the
CMR
competitive
message
routing
architecture
Knowledge
representation
Introduction
An
AGI
uses
KR
structure
to
represent
the
external
world
and
this
structure
is
built
with
limited
computational
resources
and
as
such
must
be
approximate
We
have
lot
of
freedom
to
choose
the
KR
format
chose
logic
as
KR
because
it
offers
direct
way
to
translate
natural
language
texts
into
machine
knowledge
This
is
of
critical
importance
because
the
overall
feasibility
of
AGI
hinges
on
the
efficiency
of
machine
learning
and
the
most
effective
machine
learning
method
is
"learn
by
being
told
"sec
learn
by
being
told
common
misconception
is
"How
can
complex
ideas
such
as
'John
loves
Mary
be
reduced
to
logic
formulae
like
loves
john
mary
"One
school
of
thought
see
eg
posits
that
human
reasoning
is
based
on
"mental
models
"but
it
is
unclear
how
exactly
they
can
be
constructed
My
view
of
logic
based
AI
is
that
of
using
logic
as
computational
structure
for
constructing
mental
models
Multiplicity
of
knowledge
representation
schemes
This
is
how
think
of
the
issue
of
knowledge
representation
There
are
various
KR
schemes
For
example
in
NL
there
are
various
grammar
formulations
may
be
Phrase
Structure
Grammar
may
be
Fluid
Construction
Grammar
may
be
Categorial
Grammar
may
be
Genifer's
machine
learned
grammar
which
may
be
stochastic
and
inscrutable
to
humans
may
be
some
spatio
temporal
KR
scheme
such
as
temporal
logic
event
calculus
situation
calculus
etc
And
then
there
would
be
transformations
between
the
grammatical
formalisms
The
problem
is
whether
we
can
mix
multiple
KR
schemes
like
and
The
commonsense
KB
would
be
different
under
either
or
and
the
entire
KB
needs
to
be
transformed
by
some
If
we
use
at
the
same
time
confusion
may
arise
during
machine
learning
and
reasoning
Perhaps
if
we
make
Genifer
be
aware
of
transformations
like
then
maybe
it
can
deal
with
multiple
KR
schemes
at
the
same
time
Therefore
we
maybe
don't
need
to
choose
or
commit
to
any
particular
KR
scheme
at
this
stage
in
other
words
any
would
be
fine
Natural
language
Please
also
see
the
chapter
on
natural
language
ch
natural
language
Composition
of
concepts
In
the
young
Haskell
Curry
started
working
on
combinatory
logic
CL
as
logica
universalis
but
it
ran
into
inconsistency
problems
(In
Genifer
we
try
to
use
fuzzy
probabilistic
truth
values
to
get
around
this
problem
see
sec
paradox
)One
special
feature
of
CL
is
combinatorial
completeness
-meaning
that
any
concept
can
in
principle
be
applied
to
any
other
concept
One
can
readily
see
that
when
the
concept
of
self
application
is
applied
to
itself
it
can
lead
to
Russell's
paradox
sec
paradox
Nevertheless
combinatorial
completeness
is
something
desirable
in
universal
logic
It
enables
logic
to
reason
about
logical
paradoxes
themselves
Bertrand
Russell
invented
type
theory
to
get
around
the
problem
of
paradoxes
basically
by
banning
all
circular
references
But
as
result
of
that
type
theory
and
thus
the
higher
order
logic
built
on
top
of
it
cannot
be
used
to
reason
about
paradoxes
Curry's
combinatory
logic
can
represent
arbitrary
combinations
of
concepts
in
manner
such
as
wisdom
socrates
the
wisdom
of
socrates
One
of
my
latest
insights
is
that
the
combination
of
concepts
can
be
computed
by
the
same
process
as
unification
of
terms
in
logic
For
more
details
refer
to
sec
unification
In
JA
Robinson
discovered
resolution
which
is
really
unification
propositional
refutation
Unification
decides
if
terms
can
be
made
equationally
identical
Propositional
refutation
is
an
inference
step
that
deals
with
the
calculus
of
thinking
at
the
sentence
level
Around
the
George
Boole
CS
Peirce
Gottlob
Frege
amongst
others
developed
predicate
logic
which
is
later
popularized
by
such
people
as
Hilbert
Ackermann
Russell
and
Whitehead
Predacate
logic
has
really
been
popular
for
only
about
years
versus
syllogistic
logic
that
has
been
around
since
Aristotle's
time
Predicate
logic
differs
from
propositional
logic
by
giving
propositions
internal
structure
-proposition
is
broken
down
into
predicate
and
one
or
more
objects
However
this
decomposition
seems
inadequate
to
deal
with
arbitrarily
free
combinations
of
concepts
Combinatory
logic
provides
free
way
to
compose
terms
via
application
The
key
is
to
regard
terms
as
compound
concepts
For
example
in
Genifer's
notation
tall
handsome
guy
is
the
combination
of
the
concepts
tall
handsome
guy
Now
few
examples
tall
handsome
guy
is
equivalent
to
handsome
tall
guy
very
tall
guy
implies
tall
guy
but
very
tall
guy
does
not
equal
tall
very
guy
Thus
the
unification
is
modulo
some
special
rules
such
as
commutativity
associativity
etc
and
certain
reduction
rules
In
other
words
unification
modulo
theory
the
calculus
of
concepts
So
we
have
neat
decomposition
calculus
of
thoughts
calculus
of
concepts
calculus
of
propositions
In
category
theory
one
enforces
the
associative
composition
of
arrows
(We
often
omit
the
application
symbol
)Likewise
observed
that
if
we
enforce
the
associative
composition
of
concepts
the
logical
form
becomes
very
elegant
Due
to
associativity
any
pair
of
concepts
in
an
application
must
be
meaningful
in
meaningful
sentence
consequence
of
enforcing
associativity
is
that
we
can
no
longer
use
Currying
For
example
John
loves
Mary
would
be
rendered
in
Genifer's
logic
as
mary
loves
john
rather
than
as
suggested
by
Currying
loves
mary
john
Thus
in
Genifer's
logic
the
primitive
operations
are
application
pairing
or
cross
product
with
the
associative
and
distributive
rules
One
can
recognize
that
this
logic
has
the
exact
form
as
category
I'm
curious
as
to
the
role
of
exponentiation
in
this
category
For
more
about
how
natural
language
sentences
are
translated
into
logical
form
see
the
section
on
natural
language
and
Geniform
sec
geniform
Dynamic
interpretation
of
semantics
Look
at
these
examples
glass
slippers
are
made
of
glass
door
knob
is
part
of
door
street
prostitutes
work
on
the
streets
However
glass
slippers
do
not
work
in
glasses
door
knob
is
not
made
of
doors
street
prostitutes
are
not
part
of
the
streets
This
suggests
that
the
semantics
of
the
compound
concepts
depend
on
external
pieces
of
knowledge
and
hence
must
be
interpreted
dynamically
This
is
essentially
the
same
idea
as
the
abductive
interpretation
of
natural
language
sec
abduction
as
interpretation
Reification
Reification
means
"to
turn
into
objects
"For
example
John
loves
Mary
can
be
rendered
as
loves
john
mary
but
John
loves
Mary
deeply
would
have
to
rendered
as
love
love
love
is
an
instance
of
love
subject
love
john
object
love
mary
deep
love
Notice
that
in
the
reified
form
the
original
base
level
formula
loves
john
mary
is
lost
This
is
rather
unsatisfactory
Geniform
solves
this
problem
by
eliminating
reification
Representing
time
As
Einstein
would
have
said
the
representation
scheme
for
space
and
time
should
be
fundamentally
the
same
As
have
developed
vision
theory
ch
vision
think
temporal
representations
can
follow
similar
scheme
Contexts
Background
An
excellent
survey
of
contexts
in
logic
based
AI
is
The
book
Chapter
is
also
excellent
and
contains
additional
insights
about
contexts
Also
the
book
In
Genifer
context
can
be
regarded
as
set
of
propositions
that
are
true
in
that
context
For
example
context
would
be
the
set
of
propositions
It
is
late
at
night
on
earth
am
hungry
The
fridge
is
empty
There
is
no
money
in
my
wallet
Such
contexts
are
very
useful
in
the
clustering
of
the
KB
for
inference
speed
up
sec
hi
oracle
Some
contexts
are
partial
propositions
such
as
In
the
kitchen
John
believes
that
As
far
as
I'm
concerned
Theoretically
speaking
In
Genifer
we
calculate
contexts
as
component
of
the
truth
values
of
formulas
The
reason
we
do
so
is
Inference
in
contexts
In
general
there
are
possible
cases
An
assertion
in
an
outer
context
without
contradiction
can
apply
to
an
inner
context
For
example
In
children's
stories
animals
can
talk
Inner
context
Cats
are
animals
Truth
in
outer
context
In
children's
stories
cats
can
talk
An
assertion
in
an
inner
context
even
without
contradiction
cannot
apply
to
outer
contexts
For
example
John
believes
he
can
fly
Inner
context
John
can
fly
Outer
context
When
assertions
are
in
contradiction
the
one
in
the
more
specific
context
wins
For
example
cf
Example
Mary
does
not
know
Kent
is
Superman
Inner
context
Kent
is
Superman
Outer
context
Superman
can
fly
Outer
context
Kent
can
fly
Outer
context
Mary
does
not
know
Kent
can
fly
Inner
context
wins
When
contexts
are
incomparable
an
assertion
cannot
apply
from
one
context
to
the
other
For
example
In
John's
mind
Mary
is
dumb
Inner
context
In
Mary's
mind
Mary
is
dumb
Inner
context
These
rules
are
stipulated
only
for
the
sake
of
facilitating
compression
indeed
the
ultimate
purpose
of
using
formal
logic
is
to
facilitate
algorithmic
compression
Remember
Pei
Wang's
idea
ch
confidence
is
to
assign
confidence
to
formulas
that
allows
defeasible
reasoning
Basically
for
formulas
Our
general
rules
can
be
summarized
as
follows
In
context
where
means
"overrides
defeats
"is
the
null
formula
ie
the
absence
of
any
formula
context
is
the
current
context
and
means
incomparable
Notice
the
reversal
of
sign
in
the
rd
rule
which
is
why
cannot
compress
all
rules
into
single
one
question
is
whether
Pei
Wang's
NARS
confidence
can
be
merged
with
contexts
-it
seems
that
there
are
independent
truth
values
probability
fuzziness
NARS
confidence
and
context
Even
if
and
can
be
merged
we
still
need
to
generalize
confidence
to
partial
orders
so
that
and
may
be
incomparable
Context
management
Contexts
are
nodes
of
global
Context
Tree
and
can
be
encoded
by
numeric
labels
of
the
form
with
Now
the
question
is
how
to
assign
contexts
to
formulas
The
problem
seems
similar
to
reference
resolution
in
natural
language
processing
but
is
complicated
by
the
need
to
manipulate
contexts
with
logical
rules
if
we
want
to
use
logic
to
parse
natural
language
Assumptions
and
counterfactuals
How
to
make
assumptions
during
inference
Assuming
mom
is
at
home
call
her
home
phone
number
Example
of
counterfactual
conditional
If
Oswald
did
not
kill
Kennedy
someone
else
would
have
Logic
The
only
way
to
rectify
our
reasonings
is
to
make
them
as
tangible
as
those
of
the
Mathematicians
so
that
we
can
find
our
error
at
glance
and
when
there
are
disputes
among
persons
we
can
simply
say
Let
us
calculate
calculemus
without
further
ado
to
see
who
is
right
-Leibniz
Note
This
chapter
is
currently
very
chaotic
because
of
re
organization
effort
Fuzziness
is
no
longer
considered
foundational
part
of
the
logic
Background
Turing
universality
Undecidability
of
FOL
calculus
See
Wikipedia
http
en
wikipedia
org
wiki
Lambda
calculus
Lambda
calculus
Bound
variables
In
the
abstraction
we
call
the
bound
variable
and
the
body
Every
occurrence
of
in
is
bound
by
the
abstraction
Conversely
an
occurrence
of
variable
is
free
if
it
is
not
bound
eg
in
Head
normal
form
term
is
in
head
normal
form
if
for
and
it
can
be
expressed
as
The
variable
may
either
be
free
or
bound
one
of
De
Bruijn
indexes
Director
strings
Functional
programming
Combinatory
logic
See
Wikipedia
http
en
wikipedia
org
wiki
Combinatory
logic
Combinatory
logic
Term
rewriting
systems
Simple
type
theory
Higher
order
logic
HOL
is
roughly
synonymous
with
type
theory
with
the
addition
of
axioms
that
define
logical
primitives
Standard
semantics
and
general
semantics
of
higher
order
logic
How
is
Henkin
semantics
reducible
to
FOL
Model
theory
Model
theory
is
the
study
of
the
truth
of
syntactic
formulas
defined
via
their
correspondence
to
certain
mathematical
structures
known
as
models
Originated
by
Alfred
Tarski
Proof
theory
Deduction
systems
Hilbert
systems
Sequent
calculus
Natural
deduction
Tableau
Resolution
Concept
composition
Fragments
Is
it
possible
to
have
inference
at
the
sub
propositional
level
Inference
rules
that
operate
upon
fragments
of
propositions
Sometimes
our
human
minds
can
focus
on
fragments
or
concepts
that
are
not
complete
sentences
For
example
we
may
think
of
"spaghetti
with
meat
balls
"instead
of
"spaghetti
with
meat
balls
are
tasty
"Or
we
may
think
"To
lose
more
weight
need
to
"without
being
able
to
complete
the
sentence
immediately
It
seems
possible
to
extend
the
idea
of
logical
inference
from
propositions
to
fragments
Completion
means
to
extend
fragment
to
become
longer
fragment
or
complete
proposition
Jump
means
to
jump
from
fragment
to
another
fragment
or
proposition
in
the
KB
Notice
that
completion
and
jump
are
supported
by
the
KB
in
the
sense
that
they
draw
information
from
the
KB
in
order
to
"push
"through
the
arrow
just
like
in
deduction
The
following
figure
illustrates
their
similarity
Note
that
in
all
situations
the
logic
rules
are
supplied
by
the
KB
which
allows
an
"arrow
"to
be
there
in
the
first
place
The
premises
are
also
supplied
by
the
KB
but
is
not
shown
in
the
figure
The
point
is
that
the
conclusions
derived
from
fragments
are
not
arbitrary
they
depend
on
truths
in
the
KB
formula
with
the
generalized
arrow
is
form
of
truth
and
the
premise
fragment
is
also
form
of
truth
-it
can
be
interpreted
as
"Fragment
is
worth
paying
attention
to
at
this
moment
"Now
we
can
see
the
symmetry
of
the
forms
and
that
can
be
seen
as
special
case
of
jump
This
leads
to
the
unification
of
propositional
and
sub
propositional
logic
with
unified
"arrow
"Thus
our
set
of
logical
operators
can
be
minimized
to
composition
product
union
sum
and
exponentiation
arrow
An
interesting
question
is
to
see
whether
the
logic
can
be
made
CCC
Cartesian
closed
category
which
seems
to
be
"nice
"property
to
have
Uncertainty
Reasoning
under
uncertainty
is
vast
and
nightmarishly
complex
topic
in
AI
Simon
Parsons's
book
contains
very
good
survey
of
uncertain
reasoning
but
even
that
is
not
exhaustive
We
may
look
at
the
following
taxonomy
of
"ignorance
"proposed
by
taxonomy
of
ignorance
At
first
blush
classical
logic
appears
to
be
sufficient
for
AGI
Some
AGI
designers
prefer
to
use
crisp
logic
as
the
base
and
plan
to
build
and
upon
crisp
logic
But
this
may
be
sign
of
not
facing
problems
and
not
having
sufficient
understanding
of
fuzzy
probabilistic
logics
To
represent
and
logic
in
crisp
logic
is
like
writing
an
entire
inference
engine
in
Prolog
complicated
by
the
fact
that
the
crisp
logic
would
be
somewhat
different
from
Prolog
and
so
may
be
even
harder
to
program
in
Also
the
"truths
"known
by
the
AGI
will
then
be
different
from
the
truths
as
represented
in
the
crisp
logic
-the
former
would
be
"floating
"above
the
latter
This
creates
unnecessary
indirectness
will
give
other
reasons
in
sec
whyZ
and
Why
not
include
other
uncertainty
measures
besides
and
There
are
other
theories
of
uncertainty
such
as
possibility
belief
functions
and
rough
sets
The
reason
chose
and
is
because
they
are
simple
and
best
understood
There
has
been
attempts
to
create
systems
where
the
user
can
create
flexible
number
of
uncertainty
measures
but
one
problem
of
such
systems
has
been
pointed
out
in
If
we
have
uncertainty
measures
say
"cloud
""mist
"and
"fog
"there
would
be
need
to
provide
mixed
inference
rules
for
"cloud
mist
""cloud
fog
"etc
total
of
possibilities
have
worked
out
the
combination
of
and
and
found
that
it
involved
considerable
efforts
Nonmonotonicity
and
defeasible
reasoning
One
can
create
defeasible
logics
from
classical
logic
for
example
Reiter's
default
logic
and
McCarthy's
circumscription
But
the
classical
approaches
seem
to
require
enumerating
all
possible
exceptions
which
is
impractical
examples
John
is
usually
very
punctual
Therefore
John
will
arrive
at
the
airport
on
time
John
has
an
accident
en
route
to
the
airport
and
dies
Therefore
John
will
NOT
arrive
on
time
Will
John
arrive
on
time
Mary
has
cybersex
with
many
partners
Cybersex
is
kind
of
sex
Therefore
Mary
has
many
sex
partners
person
who
has
many
sex
partners
has
high
chance
of
STDs
Therefore
Mary
has
high
chance
of
STDs
What's
wrong
with
example
On
the
one
hand
we
should
admit
that
cybersex
is
sex
it
is
borderline
case
but
it
lacks
certain
prominent
features
of
sex
such
as
physical
contact
which
is
not
necessarily
defining
feature
of
sex
Thus
if
we
carry
on
reasoning
with
the
idea
that
cybersex
is
sex
we
may
get
unsound
conclusions
The
key
to
resolving
this
problem
is
to
recognize
"cybersex
is
sex
"with
qualifications
such
as
"it
is
sex
without
physical
contact
"If
we
have
rule
that
"sex
transmits
certain
diseases
"we
may
have
to
attach
the
exception
"only
if
the
sex
involves
physical
contact
"In
the
end
our
rules
may
be
inundated
with
possibly
infinitely
many
exceptions
How
can
we
get
out
of
this
problem
Wang
in
NARS
provided
solution
His
idea
is
not
to
store
the
exceptions
to
rules
but
instead
allow
multitude
of
rules
to
fire
calculate
the
"confidence
"sec
confidence
of
each
conclusion
and
pick
the
conclusion
with
the
highest
confidence
This
allows
us
to
handle
exceptions
relatively
easily
For
the
example
Pei
Wang's
idea
is
to
calculate
the
combined
conditional
probability
by
weighing
each
individual
conditional
probability
with
their
associated
confidences
sec
confidence
which
is
nice
idea
(Except
that
NARS
truth
values
do
not
conform
to
probability
theory
)but
Abram
Demski
came
up
with
an
alternative
idea
that
is
more
in
accord
with
Bayesianism
The
idea
is
to
construct
from
the
marginal
conditionals
and
and
priors
and
This
is
achieved
by
applying
Bayes
rule
twice
It
may
be
possible
to
use
Abram's
method
to
construct
all
joint
CPTs
instead
of
using
contrived
probabilistic
formulations
of
AND
and
OR
An
example
John
is
usually
punctual
therefore
John
will
arrive
at
the
airport
on
time
John
died
en
route
to
the
airport
therefore
John
will
NOT
arrive
on
time
punctual
john
punctual
john
arrive
john
dead
john
arrive
john
Abram's
method
Unification
Unification
is
the
process
of
making
terms
identical
via
substitutions
For
example
we
can
use
substitution
to
transform
term
like
this
Unification
is
the
process
of
finding
the
substitution
given
the
initial
and
final
terms
So
When
we
use
logic
to
encode
natural
language
and
common
sense
reasoning
terms
are
representations
of
simple
or
compound
concepts
cf
sec
composition
Thus
unification
plays
the
role
of
deciding
if
concepts
are
the
same
in
other
words
the
unification
algorithm
embodies
the
calculus
of
concepts
Two
important
relations
of
concepts
are
Equality
For
example
clark
kent
superman
Can
be
generalized
to
fuzzy
equality
For
example
cat
dog
cats
are
similar
to
dogs
because
they
are
both
pet
animals
Inclusion
in
classical
AI
also
known
as
the
is
relation
For
example
dog
animal
The
relation
affects
unification
because
substitution
of
equals
for
equals
is
valid
We
use
set
of
equations
called
an
equational
theory
to
define
what
are
considered
equal
Then
we
have
so
called
unification
modulo
equational
theory
or
unification
The
algorithm
for
equational
unification
is
called
narrowing
see
sec
narrowing
below
complication
is
that
should
be
replaced
by
the
more
general
and
we
need
to
modify
the
traditional
algorithms
to
handle
diminishing
fuzzy
truth
values
when
is
repeatedly
applied
Higher
order
unification
The
standard
algorithm
for
higher
order
unification
was
formulated
by
Huet
in
Huet's
algorithm
Equational
unification
narrowing
Narrowing
is
term
rewriting
with
unification
The
matching
process
of
rewriting
is
replaced
by
unification
where
both
the
rewrite
rule
and
the
term
to
be
rewritten
can
be
instantiated
Some
basic
texts
on
narrowing
are
The
equational
theory
is
replaced
by
rewriting
system
by
orienting
the
equations
in
specific
direction
The
one
step
narrowing
relation
is
induced
by
TRS
term
rewriting
system
We
write
to
denote
substitution
that
enables
the
term
to
be
rewritten
as
using
one
rewriting
rule
from
To
rewrite
when
given
the
equational
theory
we
can
make
the
substitution
yielding
This
step
makes
the
term
narrower
hence
the
name
guess
Narrowing
is
search
process
with
high
complexity
because
at
each
step
there
are
kinds
of
choices
to
be
made
which
rewrite
rule
in
to
try
and
which
position
within
to
try
Equality
Morning
star
evening
star
problem
problem
with
equality
is
illustrated
by
the
classic
example
"Morning
Star
Evening
Star
"The
following
is
similar
example
with
its
rendition
in
logic
possible
solution
is
to
distinguish
clark
kent
in
quotes
ie
the
Clark
Kent
that
Mary
knows
and
clark
kent
without
quotes
ie
the
real
Clark
Kent
This
idea
is
similar
to
the
so
called
use
mention
distinction
The
problem
is
in
general
due
to
the
presence
of
"contexts
"cf
sec
contexts
For
example
In
the
context
of
children's
tales
animals
can
talk
In
the
context
of
Star
Wars
Darth
Vader
is
Luke's
father
In
the
context
of
Mary's
beliefs
Superman
and
Kent
are
different
persons
Modal
logic
Modality
si
Modal
logic
no
-John
McCarthy
My
view
is
to
use
predicates
to
represent
modality
instead
of
using
modal
logics
This
view
was
first
advocated
by
One
argument
for
the
use
of
modal
logic
arises
from
the
previous
Example
but
Mary
may
not
know
that
Superman
is
Clark
Kent
This
problem
can
be
resolved
as
in
the
last
section
sec
equality
Logical
paradoxes
Russell's
or
Liar's
paradox
Curry's
paradox
Skolem's
paradox
Tarski's
paradox
Non
axiomatizability
"First
order
logic
has
an
effective
notion
of
proof
which
is
complete
the
intended
interpretation
This
is
the
content
of
Godel's
completeness
theorem
As
result
the
set
of
Godel
numbers
of
universally
valid
first
order
formulas
is
recursively
enumerable
"But
"the
set
of
second
order
validities
is
not
arithmetically
definable
let
alone
recursively
enumerable
and
hence
that
an
effective
and
complete
axiomatization
of
second
order
validity
is
impossible
"Shortcomings
of
the
current
logic
Binary
vs
It
may
be
desirable
to
have
binary
logic
alongside
logic
reasoning
is
suitable
for
common
sense
concepts
whereas
binary
reasoning
is
good
for
programmatic
(Ie
binary
logic
makes
programming
easier
)or
computational
reasons
However
one
unsolved
problem
is
that
many
common
sense
concepts
appear
to
be
binary
but
are
fuzzy
upon
close
inspection
eg
male
female
dead
alive
The
question
is
how
to
let
binary
and
fuzzy
concepts
coexist
in
the
same
logic
Meta
reasoning
think
think
therefore
think
am
The
term
"meta
reasoning
"may
refer
to
things
The
ability
to
reason
about
reasoning
which
is
what
this
chapter
is
concerned
with
Scheduling
reasoning
tasks
to
achieve
best
results
with
limited
computational
resources
have
not
thought
about
this
problem
yet
An
excellent
survey
of
meta
reasoning
in
the
sense
is
In
the
Tell
Learn
loop
we
see
that
we
need
special
predicate
credible
to
increase
the
probability
of
statement
via
side
effect
But
that
may
not
be
the
only
meta
reasoning
move
we
can
make
Another
example
is
the
conversion
of
"traitor
"and
"patriot
"sec
PZ
meta
reasoning
Higher
order
logic
Lloyd
developed
inductive
learning
based
on
HOL
typed
lambda
calculus
and
also
logic
programming
language
Escher
Lloyd
uses
form
of
logic
that
is
similar
to
what
do
with
logic
ie
its
statements
are
of
the
form
where
is
the
head
and
is
the
body
In
essence
this
is
the
Prolog
Horn
tradition
TO
DO
formulate
HOL
One
example
of
meta
reasoning
pertinent
to
fuzziness
is
"You
are
either
patriot
or
traitor
""No
can
be
slightly
patriotic
or
slightly
traitorous
"In
the
predicates
patriot
and
traitor
have
binary
character
In
they
have
fuzzy
character
Algebraic
logic
My
motivation
for
studying
algebraic
logic
is
to
develop
better
inference
and
learning
algorithms
for
logic
In
the
the
invention
of
SVMs
by
Vapnik
revolutionized
the
field
of
statistical
learning
and
SVMs
remain
to
be
the
best
general
spatial
learner
today
What
if
we
can
invent
something
similar
to
logic
as
what
SVM
is
to
spatial
domains
Survey
of
algebraic
logic
Halmos
Appendix
Lawvere's
idea
that
the
existential
and
universal
quantifiers
are
adjuncts
to
each
other
What
use
is
it
Speeding
up
inference
and
learning
For
all
logics
that
know
of
deduction
is
performed
by
iterating
deductive
steps
Each
step
is
the
application
of
an
inference
rule
If
formula
with
quantification
is
involved
some
kind
of
pattern
matching
or
unification
is
required
apply
the
formula
Such
an
iterative
process
is
equivalent
to
searching
in
large
proof
space
My
idea
is
perhaps
we
can
"map
"logic
to
some
continuous
space
and
somehow
approximate
the
proof
process
by
spatial
computational
techniques
One
natural
idea
is
We
know
that
for
some
concepts
predicates
the
shape
of
the
regions
defining
the
predicate
can
be
extremely
irregular
or
even
incomputable
Our
motivation
is
twofold
How
can
we
find
intersections
of
regions
efficiently
Can
we
altogether
avoid
performing
deduction
via
iterative
steps
One
idea
is
to
have
multiple
"domain
grids
"We'd
be
sorting
the
source
domains
into
various
clusters
That
will
speeed
up
the
look
up
of
intersections
However
this
idea
only
gives
us
geometric
interpretation
but
it
doesn't
tell
us
how
to
compute
the
shapes
of
regions
efficiently
The
only
way
to
compute
the
regions
is
to
go
back
to
the
basic
inference
rules
of
the
logic
ie
iteratively
and
syntactically
Another
idea
is
the
"dual
"Vagueness
An
approximate
answer
to
the
right
question
is
better
than
precise
answer
to
the
wrong
question
-John
Tukey
rephrased
NOTE
this
section
will
be
re
formulated
soon
In
the
new
version
only
make
assumptions
There
are
some
quantities
known
as
degrees
and
they
are
normalized
to
There
exist
relations
amongst
these
quantities
captured
by
statistical
models
think
the
new
formulation
will
be
fairly
unblemishable
and
uncontroversial
as
opposed
to
earlier
forms
of
fuzzy
logic
which
many
people
are
skeptical
of
But
the
new
version
also
falls
out
of
the
classification
as
logic
and
is
more
of
fuzzy
probabilistic
calculus
use
instead
of
to
denote
fuzziness
partly
in
recognition
of
Lotfi
Zadeh's
contributions
and
also
because
and
are
often
used
for
the
CDF
and
PDF
in
probability
theory
which
may
cause
confusion
when
probabilities
and
fuzziness
are
used
together
note
on
the
examples
used
in
this
book
some
of
them
are
politically
incorrect
or
somewhat
embarrassing
prefer
examples
that
are
simple
realistic
and
relevant
to
human
emotions
because
they
help
us
think
more
clearly
cf
the
Wason
selection
task
Usually
just
choose
the
most
obvious
examples
that
come
to
mind
Vague
phenomena
There
seems
to
be
types
of
concepts
predicates
The
first
type
is
discrete
in
nature
for
example
(will
later
argue
that
even
these
concepts
are
not
completely
binary
)sex
of
person
male
female
hermaphrodite
trans
sexual
marital
status
single
engaged
married
divorced
The
second
type
represents
numerical
measures
for
example
height
weight
age
The
third
type
is
not
strictly
numerical
but
is
often
associated
with
"gradedness
"There
is
no
doubt
that
these
predicates
can
be
modified
with
degrees
like
"very
"or
"slightly
"But
we
do
not
know
of
any
exact
measures
of
their
degrees
For
example
IQ
has
been
proposed
as
an
inexact
measure
of
intelligence
Most
people
would
agree
that
there
is
some
general
consensus
as
to
what
is
intelligent
Such
concepts
exhibit
vague
phenomena
which
are
characterized
by
borderline
cases
lack
of
sharp
boundaries
susceptible
to
sorites
paradox
(solution
to
the
sorites
paradox
is
given
in
using
fuzzy
logic
and
the
notion
of
decaying
validity
)open
texture
ie
if
is
borderline
case
one
can
assert
either
or
in
different
contexts
In
philosophical
logic
there
are
number
of
theories
of
vagueness
see
eg
logic
is
kind
of
Degree
Theory
Edgington
that
uses
numerical
truth
values
to
represent
vagueness
There
are
other
non
numerical
theories
such
as
Epistemicism
Campbell
and
Supervaluation
Fine
Why
vagueness
is
needed
for
AGI
One
of
the
critical
capabilities
of
AGI
is
self
programming
Also
it
is
imperative
to
be
able
to
instruct
an
AGI
to
write
programs
according
to
natural
language
specifications
and
with
robust
common
sense
background
knowledge
common
sense
reasoning
and
natural
language
understanding
depend
on
the
use
of
vague
concepts
(Vague
concepts
are
not
required
for
formal
reasoning
tasks
such
as
formal
mathematics
and
programming
according
to
formal
specifications
)Look
at
any
technical
or
non
technical
natural
language
text
and
one
finds
that
explicit
or
implicit
vague
concepts
are
ubiquitous
It
seems
to
be
occur
even
more
frequently
than
the
explicit
or
implicit
use
of
probabilities
Ubiquity
of
vagueness
in
common
sense
reasoning
Some
examples
are
Time
Mother
died
few
days
ago
Space
One
bird
flew
over
the
cuckoo's
nest
lacking
exact
boundaries
Physics
of
liquids
liquid
in
bulk
and
at
rest
has
horizontal
surface
Is
soup
liquid
"In
bulk
"is
fuzzy
concept
Is
the
sea
at
rest
Surface
tension
can
cause
surface
to
curve
Physics
of
solids
solid
object
cannot
go
from
inside
to
outside
closed
box
"Solid
object
"is
fuzzy
concept
eg
block
of
ice
cat
bomb
card
box
may
have
small
holes
The
lid
may
be
slightly
ajar
Why
numerical
vagueness
The
controversy
is
whether
vagueness
should
be
managed
quantitatively
such
as
or
qualitatively
One
objection
is
that
fuzzy
logic
can
sometimes
lead
to
unsound
conclusions
This
problem
is
discussed
in
sec
exceptions
on
nonmonotonic
reasoning
Having
numerical
vagueness
allows
us
to
Represent
quantitative
rules
For
example
Add
up
graded
factors
For
example
"John
is
eloquent
humorous
insightful
and
creative
"smart
"John
is
humorous
and
nothing
else
"smart
Accrue
contributing
factors
of
concept
over
long
period
of
time
"From
my
long
experience
with
John
he
is
smart
"On
the
other
hand
if
we
do
not
use
numerical
vagueness
we
face
these
problems
Failure
to
recognize
"partial
"concepts
For
example
"John
is
smart
"or
"tomato
is
fruit
"Conversely
failure
to
ignore
"very
weak
"partial
concepts
Each
statement
must
be
attached
with
many
qualifications
and
they
keep
accumulating
Each
rule
would
have
to
recognize
large
"exception
set
"The
solution
adopt
is
to
use
both
qualitative
and
quantitative
information
by
representing
facts
redundantly
sec
exceptions
For
example
"John
is
smart
"quantitative
"But
he
is
only
penny
wise
"qualitative
Lastly
what
if
we
don't
need
the
precision
of
numerical
vagueness
in
some
situations
For
example
"Is
John
really
that
smart
""Not
quite
"we
don't
know
the
exact
degree
of
smartness
or
"John
is
slightly
smarter
than
Peter
"but
we
don't
know
the
exact
difference
In
my
theory
these
cases
can
be
handled
by
combining
and
Semantics
of
There
is
some
confusion
about
the
interpretation
of
fuzziness
which
will
try
to
clarify
here
am
influenced
by
Pei
Wang's
ideas
is
measure
of
degree
or
gradedness
Standard
fuzzy
theory
employs
the
"membership
function
"to
represent
the
degree
to
which
an
element
belongs
to
class
but
there
is
no
consensus
as
to
how
the
membership
functions
are
defined
Let's
think
of
"the
degree
to
which
person
is
smart
"is
it
really
arbitrary
While
there
are
no
exact
procedures
to
measure
vague
concepts
eg
smartness
it
is
mandatory
that
common
sense
machine
should
possess
some
ways
of
assessing
them
just
like
the
human
brain
does
In
my
AGI
this
is
achieved
by
having
comprehensive
knowledgebase
of
rules
acquired
via
machine
learning
that
compute
numerical
degrees
of
concepts
Such
rules
are
mini
algorithms
or
"micro
theories
"(The
term
is
used
informally
not
referring
to
Guha's
notion
of
micro
theories
in
Cyc
)that
are
emergent
properties
of
intelligent
learning
systems
They
establish
distributed
and
approximate
consensus
of
how
to
measure
vague
concepts
From
another
perspective
PCA
principal
component
analysis
can
calculate
the
component
of
dataset
that
represents
its
greatest
variance
For
example
PCA
may
identify
the
male
female
axis
of
set
of
movie
goers
or
the
liberal
conservative
axis
of
group
of
politicians
When
we
map
this
axis
to
we
can
get
fuzzy
value
of
the
concept
male
female
or
liberal
conservative
This
may
answer
the
question
"where
do
fuzzy
numbers
come
from
"(Abram
Demski
suggested
this
to
me
in
discussion
)If
we
regard
as
measure
of
degrees
theory
is
mathematically
as
rigorous
as
probability
theory
is
measure
of
degrees
just
as
the
height
is
measure
of
how
tall
an
object
is
Also
the
value
can
be
inexact
just
as
can
be
inexact
but
this
inexactitude
is
modeled
separately
by
distributing
probabilities
over
sec
combinePZ
is
not
an
approximation
of
probability
sec
probabilistic
interpretation
gives
probabilistic
interpretation
of
vagueness
but
in
practice
we
can
treat
and
as
orthogonal
to
each
other
Possibility
theory
defines
fuzziness
as
weaker
form
of
probability
as
non
additive
probability
but
this
is
not
the
approach
of
logic
Finally
want
to
dispel
the
myth
that
probability
is
mathematically
more
rigorously
defined
than
fuzziness
Quantum
mechanics
or
Heisenberg
uncertainty
does
not
prove
that
probabilities
exist
in
the
physical
world
On
the
other
hand
studies
in
chaos
and
complex
systems
reveal
that
macroscopic
descriptions
are
emergent
and
distinct
from
microscopic
descriptions
even
though
the
former
can
be
reduced
to
the
latter
Thus
the
recognition
of
fuzzy
macroscopic
patterns
is
every
bit
as
real
as
the
subjective
use
of
probabilities
to
describe
physical
systems
Vague
concepts
are
also
useful
in
thinking
about
pure
mathematics
-for
example
recognition
of
fuzzy
similarities
may
assist
mathematical
reasoning
Probabilistic
interpretation
of
vagueness
One
way
to
interpret
vagueness
as
probability
is
to
interpret
as
"Among
all
possible
contexts
is
true
with
probability
"For
example
if
then
John
is
smart
in
of
circumstances
More
examples
There
seems
to
be
two
problems
with
this
interpretation
The
first
concerns
the
min
max
calculus
sec
min
max
VS
sum
product
-if
is
really
probability
why
does
it
not
obey
the
probabilistic
sum
product
calculus
The
second
problem
is
very
subtle
and
concerns
the
nature
of
matters
of
degree
Consider
these
statements
"Mary
is
probably
ugly
"thus
chance
of
NOT
ugly
"Jane
is
ugly
"Suppose
John
must
find
pretty
girl
If
John
has
seen
Jane
and
judged
her
to
be
ugly
then
there
seems
to
be
no
uncertainty
about
it
in
this
context
John
being
the
judge
and
Jane
having
her
present
looks
etc
So
if
John
really
must
find
pretty
girl
then
he
should
prefer
Mary
whom
he
hasn't
met
and
has
chance
of
being
pretty
In
sec
defined
we
will
consider
probability
distributions
over
fuzziness
where
this
problem
reveals
subtle
difference
in
the
shapes
of
probability
distributions
TO
DO
proposed
probabilistic
way
to
handling
fuzziness
which
at
deep
level
is
identical
to
my
approach
Reference
classes
The
measure
of
value
is
dependent
upon
its
reference
class
For
example
if
we
want
to
say
how
"young
"person
is
the
reference
class
may
be
"all
people
"or
"all
tenured
professors
"The
measure
of
"youngness
"thus
varies
depending
on
the
reference
class
Once
the
reference
class
is
fixed
it
seems
that
is
not
context
dependent
For
example
we
can
say
"John
is
young
man
who
owns
an
old
dog
"The
dog
is
described
as
"old
"using
its
own
reference
class
dogs
not
John's
reference
class
humans
Numerical
scale
of
Many
"natural
"quantities
occur
in
the
range
For
example
the
height
weight
age
or
ugliness
of
person
can
theoretically
range
from
to
It
is
unnatural
to
set
artificial
upper
limits
to
these
measures
However
it
may
be
possible
to
extend
these
concepts
to
the
range
For
example
the
age
of
AGI
may
be
because
it
is
not
yet
born
reserve
this
possibility
but
will
use
for
now
is
defined
in
So
we
need
membership
functions
to
map
to
choose
sigmoid
function
with
parameter
because
it
has
some
nice
properties
We
need
two
orientations
because
some
concepts
get
more
and
more
positive
as
while
others
the
opposite
membership
functions
Negation
is
defined
as
As
consequence
of
this
for
any
concept
means
the
antithesis
of
that
concept
For
example
would
mean
old
Another
consequence
is
that
the
point
at
is
the
point
of
neutrality
which
is
where
concept
is
neither
true
nor
false
Some
concepts
such
as
"chair
""absurd
"do
not
have
natural
opposites
For
these
concepts
means
the
complete
absence
of
the
qualities
in
question
Negation
can
cause
some
confusion
because
"not
young
"can
mean
either
"middle
age
"or
"old
"The
only
treatment
of
fuzzy
negation
that
is
entirely
consistent
with
our
common
sense
is
to
distribute
probabilities
over
fuzziness
which
will
be
developed
after
sec
defined
The
interpretation
of
the
parameter
in
eqn
is
that
it
marks
the
point
of
neutrality
on
the
axis
for
which
This
is
illustrated
as
follows
we
map
the
human
age
to
the
concept
of
"young
"where
subjectively
define
"years
old
"as
the
neutral
point
of
"young
"neutral
point
This
means
after
one
gets
more
and
more
"not
young
"according
to
this
definition
Thus
the
numerical
scale
of
is
Note
question
has
been
raised
whether
we
can
define
opposite
concepts
with
predicates
and
with
and
joining
at
in
the
middle
This
is
not
entirely
satisfactory
because
the
negation
of
would
not
cover
nor
vice
versa
Therefore
the
choice
of
as
neutrality
point
is
quite
essential
Why
obeys
min
max
calculus
Probabilities
obey
sum
product
calculus
obeys
min
max
calculus
ie
My
justification
for
min
max
is
as
follows
As
an
example
consider
the
statement
"John
have
had
sex
with
women
"but
it
turns
out
that
all
those
women
had
only
had
cybersex
with
him
Most
people
would
agree
that
cybersex
is
not
quite
the
same
as
real
sex
some
may
say
it's
borderline
case
Suppose
we
subjectively
think
that
cybersex
is
real
sex
as
measure
of
degree
then
what
would
be
the
"degree
of
truthfulness
"of
the
statement
assuming
that
we
accept
the
fact
that
he
had
cybersex
with
women
If
we
use
the
sum
product
calculus
as
with
probabilities
the
answer
would
be
which
is
almost
zero
Whereas
if
we
use
the
min
max
calculus
the
answer
would
be
So
did
John
have
sex
with
women
answer
"Of
course
not
"answer
"Well
sort
of
"My
view
is
that
the
conjunction
of
vague
events
should
have
the
same
vagueness
as
the
individual
events
You
may
try
this
with
other
examples
of
graded
events
Axiomatic
description
In
summary
is
the
relaxation
of
the
classical
logic
view
that
statement
is
either
true
or
false
true
false
is
relaxed
to
Here
is
set
of
axioms
that
describes
varies
continuously
within
is
the
point
of
neutrality
obeys
min
max
calculus
when
applied
by
logic
conjunction
and
disjunction
The
meaning
of
is
yet
to
be
clarified
For
now
I'd
just
state
it
informally
Also
it
would
be
nice
to
formulate
calculus
in
way
similar
to
Cox's
postulates
for
probabilities
but
that
is
not
my
current
priority
fuzzy
paradox
common
problem
in
fuzzy
logic
is
concerning
the
truth
value
of
statements
such
as
"and
not
"It
can
be
resolved
using
our
understanding
of
negation
Suppose
which
means
that
John
is
slightly
tall
then
which
means
this
is
slightly
false
which
means
this
is
slightly
true
On
the
other
hand
if
which
means
that
John
is
slightly
short
then
which
means
this
is
slightly
false
which
means
this
is
slightly
true
All
these
are
reasonable
conclusions
Unifying
AND
and
OR
We
seek
to
unify
AND
and
OR
by
using
single
operator
with
parameter
such
that
when
it
reduces
to
AND
and
when
it
becomes
OR
simple
way
to
define
is
The
animation
Figure
shows
the
graph
of
as
varies
poster
text
Click
to
play
px
px
unified
AND
OR
swf
Graph
of
as
varies
You
need
to
have
PDF
viewer
that
supports
Flash
such
as
Adobe
Reader
Make
the
Reader
trust
this
document
For
maximum
pleasure
click
the
play
and
loop
options
"Soft
"min
max
and
concept
learning
TO
DO
have
some
doubts
about
this
section
the
argument
is
bit
unclear
Maybe
soft
min
max
are
unnecessary
afterall
As
an
example
these
are
exemplars
of
"chair
"that
most
people
consider
to
be
typical
Usually
the
old
fashioned
chair
is
legged
and
is
made
of
wood
and
the
office
swivel
chair
can
rotate
and
has
wheels
These
are
the
sfeatures
of
the
exemplars
stored
in
memory
It
would
be
atypical
for
wooden
chair
to
have
wheels
and
have
seat
that
can
rotate
above
the
legs
So
even
though
both
chairs
are
very
typical
chairs
their
features
cannot
be
exchanged
freely
while
maintaining
the
same
level
of
typicality
It
seems
that
crisp
min
and
max
cannot
represent
this
but
may
be
mistaken
about
this
point
Anyway
created
"soft
"version
of
min
max
the
idea
is
to
use
as
their
own
weights
in
weighted
average
It
can
be
verified
that
soft
min
and
max
satisfy
the
boundary
conditions
of
classical
logic
provided
that
we
make
in
the
min
case
comparison
of
max
and
soft
max
modifiers
To
make
logic
more
versatile
we
need
to
augment
it
with
modifiers
which
correspond
to
natural
language
hedges
like
extremely
very
moderately
slightly
so
we
can
express
things
like
In
general
we
can
define
modifier
as
function
We
further
restrict
the
class
of
to
make
the
system
simpler
suggest
to
use
Gaussian
functions
with
the
mean
as
parameter
and
the
variance
would
be
fixed
to
certain
constant
So
For
example
the
for
"slightly
"and
"very
"can
be
Fuzzy
modifiers
with
We
can
have
better
control
over
the
shapes
of
by
using
other
functions
and
having
more
parameters
but
suspect
that
such
sophistication
is
not
needed
for
common
sense
reasoning
For
example
we
can
define
"lukewarm
"as
"warm
"with
or
using
with
fixed
variances
The
result
is
the
blue
curve
on
the
left
We
get
the
interval
by
taking
as
true
and
thus
"lukewarm
"would
be
binary
predicate
Two
representations
of
"lukewarm
"On
the
other
hand
if
we
use
tailored
to
represent
"lukewarm
"on
the
right
it
would
be
predicate
with
continuous
values
like
"slightly
lukewarm
"and
"very
lukewarm
"This
seems
to
be
unnecessarily
sophisticated
TO
DO
One
problem
with
this
method
is
that
the
predicate
"lukewarm
"changes
abruptly
at
the
boundaries
Another
example
is
"middle
aged
"TO
DO
Prove
that
combinations
of
and
can
be
universal
approximators
conditionals
In
general
inference
is
driven
by
rules
In
logic
all
rules
take
the
form
of
conditionals
conditional
is
specified
by
combination
of
min's
and
max's
similar
to
DNFs
disjunctive
normal
forms
in
classical
logic
Notice
that
rule
directly
assigns
value
to
without
the
use
of
an
implication
operator
which
is
very
different
from
traditional
fuzzy
logics
Traditional
fuzzy
logic
fuzzy
implication
is
map
satisfying
these
boundary
conditions
from
binary
logic
fuzzy
implication
statement
means
that
the
fuzzy
values
obey
the
equation
where
is
the
truth
value
of
the
implication
statement
Compared
to
my
approach
this
has
an
extra
level
of
indirectness
Is
it
really
necessary
that
we
know
the
truth
value
of
an
implication
statement
Cf
sec
and
ClassicalLogic
In
probabilistic
logic
the
probability
conditional
serves
as
the
implication
statement
but
we
usually
do
not
ask
about
its
own
probability
One
trouble
with
traditional
fuzzy
logic
is
that
we
cannot
even
perform
modus
ponens
unless
we
allow
interval
fuzzy
values
(Suppose
we
define
the
operators
for
very
simple
fuzzy
logic
and
has
given
an
inference
algorithm
for
this
logic
but
it
is
very
complicated
and
involves
interval
fuzzy
values
and
so
is
not
very
suitable
for
further
complex
development
)Combining
The
truth
value
How
to
combine
and
The
answer
is
simple
because
there
is
no
other
choice
the
semantics
of
probabilities
dictate
that
must
be
distributed
over
events
In
the
current
system
events
are
either
or
the
latter
are
continuous
events
So
we
distribute
over
and
In
this
sense
fuzziness
is
more
fundamental
than
probabilities
If
value
is
uncertain
-for
example
we
may
not
be
sure
how
tall
Mary
is
the
value
of
her
tallness
may
be
say
so
we
can
assume
uniform
probability
distribution
over
the
interval
which
is
the
green
rectangle
below
-and
we
can
approximate
it
by
Beta
distribution
over
with
mean
at
and
the
same
variance
an
example
distribution
where
the
probability
density
should
sum
to
On
the
other
hand
if
value
is
uncertain
we
simply
ignore
its
error
eg
by
choosing
the
mid
point
of
interval
sec
PointValued
tried
to
justify
this
The
following
commutative
diagram
shows
the
relationship
between
and
The
inner
square
contains
the
types
of
truth
values
The
outer
square
are
the
corresponding
logics
showing
their
logical
operators
The
radial
arrows
point
to
the
"underlying
sets
"of
TVs
An
example
Consider
the
common
sense
notion
The
richer
person
the
more
powerful
he
is
Note
that
this
rule
is
inexact
there
are
exceptions
where
some
rich
people
are
not
powerful
and
some
powerful
people
are
not
rich
and
deviations
the
data
points
do
not
fall
on
straight
line
There
are
reasons
to
believe
that
the
identity
function
red
line
represents
the
least
error
regression
function
first
the
values
are
in
"natural
scale
"ie
corresponds
to
so
at
is
obvious
if
we
accept
the
original
statement
secondly
when
the
person
is
neither
rich
nor
poor
and
we
should
have
nothing
to
say
whether
the
person
is
powerful
or
not
as
far
as
the
original
statement
is
concerned
Unifying
all
truth
values
to
Up
to
now
there
are
possible
TV
types
find
that
they
can
be
unified
to
type
which
can
make
things
simpler
Below
is
how
to
represent
the
other
TV
types
as
Type
Some
variables
have
strong
"binary
flavor
"For
example
in
common
sense
person
is
either
dead
or
alive
although
more
nuanced
view
will
have
grades
of
being
dead
If
deadness
is
variable
would
be
"definitely
alive
"would
be
"definitely
dead
"eg
reduced
to
ash
and
would
correspond
to
state
that
is
difficult
to
classify
as
dead
or
alive
eg
brain
dead
vegetative
state
may
be
state
that
is
more
dead
than
brain
dead
and
yet
more
alive
than
ash
eg
-have
to
pause
for
while
to
think
of
an
example
-body
under
cryonic
preservation
And
may
be
some
kind
of
near
death
experience
Anyway
one
can
expect
the
probability
distribution
of
to
be
polarized
with
trough
in
the
middle
This
can
be
represented
by
Beta
distribution
with
large
variance
When
the
polarization
gets
extreme
eg
the
toss
of
coin
is
either
head
or
tail
the
distribution
becomes
shape
The
degree
of
polarization
ie
amount
of
variance
can
be
estimated
statistically
if
data
is
available
It
appears
that
all
common
sensical
variables
are
actually
polarized
variables
Another
example
is
person
being
either
married
or
not
married
and
there
are
grey
areas
like
gay
marriage
or
marriage
for
the
green
card
etc
Actually
the
Beta
distribution
eqn
is
capable
of
representing
types
of
characteristics
with
and
as
points
of
transition
separating
the
regimes
The
unary
type
represents
concepts
such
as
"normal
""broken
""healthy
"or
"natural
"In
such
concepts
the
state
at
can
be
clearly
defined
eg
John
was
perfectly
healthy
when
he
was
kid
but
as
we
approach
the
cases
become
very
improbable
and
inexhaustible
eg
it
is
impossible
to
find
person
who
is
"utmost
unhealthy
"because
it
is
always
possible
to
think
of
more
extreme
and
improbable
unhealthy
ways
Also
committing
suicide
is
not
the
limiting
case
-suicide
is
not
the
same
as
unhealthy
-so
the
interval
is
open
ended
About
the
mean
and
variance
In
the
fuzzy
regime
the
smaller
the
variance
the
shaper
the
peak
and
thus
the
more
confident
we
are
about
the
value
The
variance
is
measure
of
confidence
in
this
regime
In
the
binary
regime
the
greater
the
variance
the
more
polarized
the
distribution
becomes
The
variance
is
measure
of
binary
character
In
the
unary
regime
the
meaning
of
the
variance
is
unclear
To
illustrate
this
with
an
example
In
the
fuzzy
regime
can
say
"John
is
dead
because
he's
in
cryonic
state
"and
can
use
small
variance
to
indicate
that
am
confidence
that
John
is
in
cryonic
state
and
not
elsewhere
In
the
binary
regime
however
can
only
indicate
the
probability
of
"John
being
dead
or
not
"where
"dead
"is
an
"either
or
"condition
The
highest
probabilities
are
concentrated
at
"definitely
dead
"and
"definitely
alive
"and
the
cryonic
state
has
low
probability
Under
this
regime
there
is
no
way
to
accentuate
make
more
probable
the
cryonic
state
-one
can
only
do
so
in
the
fuzzy
regime
This
is
exactly
what
we
would
expect
with
binary
variable
Type
We
can
create
distribution
with
peak
around
The
variance
depends
on
how
confident
we
are
of
the
value
If
unspecified
we
can
assign
typical
variance
to
it
Type
For
example
"Mary
is
probably
married
"with
ie
In
general
for
any
variable
especially
in
the
binary
regime
one
can
set
the
probability
of
""viewed
as
binary
event
to
be
equal
to
the
probability
mass
for
which
is
given
by
the
CDF
of
the
Beta
distribution
ie
the
regularized
incomplete
Beta
function
where
is
the
incomplete
Beta
function
Thus
The
desired
mean
can
be
solved
from
An
interesting
observation
As
the
variance
increases
it
eventually
reaches
maximum
where
and
the
Beta
distribution
is
undefined
From
Mathematica
experiments
if
we
keep
fixed
This
implies
that
in
the
extreme
polarized
case
The
variance
represents
the
degree
of
polarization
of
the
variable
which
varies
from
one
variable
to
another
and
can
be
quite
arbitrary
if
unspecified
Inference
of
logic
will
be
treated
in
ch
inference
Probabilities
For
an
excellent
introduction
to
probabilistic
reasoning
and
Bayesian
networks
see
Judea
Pearl's
book
Why
is
needed
In
machine
learning
it
is
often
necessary
to
learn
facts
that
are
only
contingently
true
such
as
the
fact
that
"females
often
have
long
hair
"Learning
algorithms
typically
need
to
keep
track
of
the
frequencies
of
how
often
the
hypotheses
are
true
in
order
to
pick
the
highly
probable
ones
So
it
seems
that
probabilistic
logic
should
be
built
into
the
knowledge
representation
Gaussian
and
Beta
distributions
One
very
useful
fact
for
designing
AGI
is
that
many
quantities
that
occur
naturally
in
our
physical
world
seem
to
obey
Gaussian
distributions
This
follows
from
the
Central
Limit
Theorem
which
states
that
the
sum
of
large
number
of
independent
and
identically
distributed
random
variables
is
approximately
normally
distributed
The
upshot
of
this
is
that
the
majority
of
fuzzy
quantities
such
as
tallness
can
be
represented
using
Gaussian
distributions
For
example
the
height
of
an
unknown
woman
may
have
Gaussian
distribution
with
mean
of
"feet
"So
we
can
just
use
numbers
the
mean
and
variance
to
represent
the
distribution
instead
of
using
table
which
takes
up
more
memory
Since
values
are
defined
within
we
can
use
the
Beta
distribution
which
is
defined
in
the
unit
interval
It
has
parameters
and
is
very
versatile
and
flexible
distribution
Sandy
Zabell
proved
that
if
we
make
certain
assumptions
about
an
individual's
beliefs
then
that
individual
must
use
the
Beta
density
function
to
quantify
any
prior
beliefs
about
relative
frequency
Some
characteristics
of
its
shape
If
and
the
shape
is
unimodal
with
the
mode
at
If
and
it
is
shape
with
an
anti
mode
at
the
same
point
If
it
is
the
uniform
distribution
If
respectively
then
has
finite
non
zero
value
at
respectively
at
If
it
is
or
reverse
shaped
without
mode
or
anti
mode
If
the
pdf
becomes
symmetric
about
If
the
pdf
is
skewed
to
the
right
and
vice
versa
It
is
very
useful
that
the
mean
and
variance
are
given
by
and
their
inverse
can
be
solved
by
Maple
to
give
From
the
above
it
can
be
seen
that
the
maximum
variance
occurs
at
at
which
point
the
probability
mass
is
entirely
at
the
poles
Further
discussion
about
how
the
Beta
distribution
represents
variables
is
in
sec
unifying
Causality
Three
recent
books
that
tackle
the
problem
of
causality
from
an
AI
perspective
are
and
There
is
also
comprehensive
reference
As
Ch
explains
We
know
that
Bayesian
network
is
directed
but
the
direction
of
the
arrows
do
not
have
to
be
meaningful
They
can
even
be
anti
temporal
On
the
other
hand
it
is
common
wisdom
that
"good
"BN
structure
should
correspond
to
causality
in
that
an
edge
often
suggests
that
"causes
"either
directly
or
indirectly
Bayesian
networks
with
causal
structure
tends
to
be
sparser
and
more
natural
However
as
long
as
the
network
structure
is
capable
of
representing
the
underlying
joint
distribution
correctly
the
answers
that
we
obtain
to
probabilistic
queries
are
the
same
regardless
of
whether
the
network
structure
is
causal
or
not
It
seems
that
causal
relations
cannot
be
captured
simply
by
probabilistic
relations
but
require
some
form
of
inductive
algorithm
to
obtain
such
as
the
IC
for
"inductive
causality
"algorithm
proposed
by
Predicate
logic
and
Bayesian
networks
Early
developments
in
Bayesian
network
are
mainly
propositional
which
means
that
each
node
in
network
represents
proposition
without
variables
such
as
"the
fact
that
the
alarm
sounded
"It
has
long
been
recognized
that
"lifting
"Bayesian
networks
to
first
order
is
necessary
for
AI
to
be
able
to
deal
with
open
domains
where
relations
can
be
defined
over
many
objects
propositional
vs
first
order
Bayesian
networks
One
key
idea
in
first
order
Bayesian
networks
is
the
method
of
Knowledge
Based
Model
Construction
KBMC
first
developed
by
and
The
idea
is
to
store
knowledgebase
of
first
order
rules
that
can
be
used
to
construct
propositional
Bayesian
networks
on
demand
When
query
is
asked
Bayesian
network
is
generated
on
the
fly
to
answer
the
query
This
idea
helps
us
think
of
the
first
order
case
in
terms
of
the
propositional
case
though
it
may
not
be
the
most
efficient
implementation
in
practice
Also
have
chosen
the
"directed
"approach
based
on
Bayesian
networks
versus
the
"undirected
"approach
based
on
Markov
networks
eg
Markov
Logic
Network
MLN
It
seems
that
the
directed
approach
is
more
intuitive
in
which
probabilistic
conditionals
are
used
to
represent
causal
relations
See
the
book
for
collection
of
first
order
probabilistic
logic
approaches
Some
first
order
probabilistic
logics
Kersting
Bayesian
Logic
Program
BLP
Laskey
Multi
Entity
Bayesian
Network
MEBN
Getoor
Probabilistic
Relational
Model
PRM
Milch
Bayesian
Logic
BLOG
Other
first
order
probabilistic
logics
have
not
looked
into
Sato
PRISM
Muggleton
Stochastic
Logic
Program
SLP
Jaeger
Relational
Bayesian
Network
RBN
etc
Classical
logic
must
give
way
to
Bayesian
logic
Now
we
examine
the
relation
between
Bayesian
networks
and
classical
logic
Classical
implication
is
"out
"The
notion
of
implication
in
classical
logic
is
no
longer
applicable
in
probabilistic
setting
To
see
why
consider
the
classical
equivalence
of
implication
and
if
we
try
to
calculate
the
probability
truth
value
of
this
statement
we
get
using
basic
rules
of
probabilities
regardless
of
whether
are
independent
Now
we
have
things
but
they
cannot
be
fixed
independently
of
each
other
if
we
fix
any
the
th
will
also
be
fixed
What
is
the
problem
here
The
classical
implication
""serves
function
similar
to
the
probabilistic
conditional
but
they
constrain
probabilities
in
slightly
different
ways
so
they
conflict
with
each
other
If
we
keep
both
copies
of
and
in
our
KB
and
apply
machine
learning
to
learn
their
truth
values
the
values
may
fail
to
converge
It
seems
that
classical
implication
is
only
accidentally
equivalent
to
when
things
are
binary
In
the
probabilistic
setting
we
should
jettison
the
binary
implication
in
favor
of
the
probabilistic
conditional
Adding
probabilities
to
binary
logic
in
direct
way
ie
using
the
binary
material
implication
instead
of
will
result
in
very
awkward
inference
algorithms
cf
that
require
setting
up
sets
of
inequalities
for
probability
bounds
The
result
is
so
awkward
that
think
for
all
practical
purposes
this
formulation
can
be
said
to
be
wrong
The
Bayesian
network
formulation
using
conditional
probabilities
should
be
preferred
Translating
classical
logic
to
Bayesian
networks
Note
this
translation
is
inexact
but
it
preserves
the
intended
meaning
informally
First
we
can
translate
first
order
KB
into
Horn
form
This
is
generally
impossible
since
Horn
logic
is
strict
subset
of
first
order
logic
but
it
can
be
done
if
we
compile
the
knowledgebase
into
pseudo
Horn
form
and
use
special
inference
algorithm
this
is
done
in
Horn
formula
which
is
equivalent
to
Prolog
statement
having
the
form
would
corresponds
to
in
Bayesian
network
This
approach
is
the
same
one
adopted
by
BLP
MEBN
BLOG
and
PRM
see
sec
FOL
BN
The
probabilistic
quantifier
""In
addition
to
the
classical
quantifiers
"for
all
"and
"exists
"we
can
introduce
probabilistic
quantifier
"for
some
"It
returns
as
truth
value
the
probability
of
the
quantified
statement
For
example
the
truth
value
of
tall
"Some
x's
are
tall
"can
be
defined
as
Notice
that
the
reference
class
of
cannot
be
read
from
the
formula
above
it
must
be
specified
by
the
type
of
such
as
human
tall
How
can
the
computer
know
the
sizes
of
these
sets
It
seems
that
it
has
to
keep
record
of
the
sizes
every
time
new
fact
is
encountered
For
example
When
tall
john
human
enters
the
sensory
stream
we
should
note
that
john
belongs
to
the
classes
chinese
male
human
organism
etc
Each
of
these
reference
classes
remembers
its
average
tallness
stored
as
distribution
with
its
mean
and
variance
and
confidence
which
gives
the
size
of
its
support
These
records
will
be
updated
according
as
John's
tallness
compares
to
the
distributions
In
other
words
for
each
property
ie
predicate
we
have
to
record
the
distributions
for
at
least
the
typical
classes
that
the
predicate
applies
to
Interval
valued
probabilities
Sometimes
Bayesian
networks
fail
to
reproduce
analogous
results
in
classical
logic
unless
we
use
interval
probabilities
Consider
this
example
China
and
the
US
are
in
conflict
If
John
sides
with
the
US
he'll
be
traitor
If
he
sides
with
China
he'll
be
loser
Either
way
John
will
be
miserable
We
can
express
the
premises
as
conditional
probabilities
and
we
want
to
query
the
probability
but
it
is
unknown
whether
John
is
traitor
or
patriot
This
example
is
exactly
analogous
to
the
"resolution
rule
"in
classical
logic
The
classical
inference
step
is
traitor
miserable
traitor
miserable
-----------miserable
If
we
construct
Bayesian
network
we
will
have
the
following
CPT
conditional
probability
table
but
we
cannot
evaluate
since
is
not
known
This
is
problem
with
point
valued
Bayesian
networks
they
fail
to
draw
some
analogous
conclusions
of
classical
logic
However
according
to
probability
theory
Therefore
In
other
words
if
we
allow
the
use
of
interval
probability
we
can
infer
that
even
when
we
assume
that
ie
unknown
Thus
we
obtain
result
analogous
to
classical
resolution
We
need
Bayesian
network
inference
algorithm
that
can
handle
this
type
of
deduction
but
first
we
consider
an
important
simplification
in
the
next
section
The
final
algorithm
will
be
given
in
sec
inference
Why
point
valued
probability
is
sufficient
for
AGI
In
my
opinion
second
order
probability
such
as
interval
probability
or
the
indefinite
probability
developed
by
and
used
in
is
an
overkill
for
AGI
It
makes
the
deduction
algorithm
very
complex
and
since
the
machine
learning
algorithm
is
based
on
deduction
and
is
even
more
complex
the
latter
problem
becomes
practically
impossible
to
solve
in
those
settings
So
we
must
make
the
deduction
algorithm
as
simple
as
possible
Therefore
suggest
using
only
point
valued
probability
In
sec
intervalP
showed
that
interval
probability
is
needed
for
some
inference
steps
That
means
the
probability
itself
is
uncertain
and
it
lies
in
an
interval
The
exact
algorithm
for
interval
probability
inference
requires
us
to
set
up
the
bounds
of
various
probabilities
and
then
invoke
linear
programming
to
solve
for
the
probability
bounds
of
the
query
variable
This
method
was
first
outlined
by
and
then
developed
by
et
al
Recently
developed
faster
algorithms
for
it
but
still
the
complexity
of
these
algorithms
is
too
much
to
handle
if
we
want
to
design
learning
algorithms
based
on
them
What
propose
is
that
whenever
we
obtain
an
interval
value
we
should
convert
it
to
point
value
by
taking
the
mid
point
of
the
interval
(We
still
need
inference
algorithms
that
can
handle
intervals
as
demonstrated
in
sec
intervalP
but
the
intervals
will
be
instantly
converted
to
point
values
after
each
step
This
reduces
complexity
greatly
)For
example
John
may
be
unable
to
decide
whether
the
president
is
smart
or
dumb
He
may
ascribe
to
the
atom
According
to
my
scheme
he
can
use
as
compromise
This
is
like
saying
"there's
chance
"Is
this
approximation
too
bad
It
seems
that
many
people
think
like
this
anyway
I'd
be
surprised
if
the
human
brain
maintains
nd
order
probabilities
internally
Moreover
the
exact
values
of
often
do
not
affect
our
behavior
that
much
There
is
evidence
that
taking
the
centroid
the
center
of
mass
of
belief
distribution
can
yield
reasonably
good
results
in
second
order
probabilistic
decision
making
Sundgren
Also
shows
that
there
are
broad
classes
of
utility
functions
for
which
uncertainty
is
irrelevant
under
expected
utility
theory
and
only
mean
values
are
significant
TO
DO
There
is
hand
waving
here
Maybe
in
much
more
advanced
AGI
we
would
want
nd
order
precision
but
that
seems
not
to
be
the
right
priority
now
It
may
be
more
effective
for
an
AGI
to
improve
its
decisions
by
considering
more
factors
updating
probabilities
using
more
evidence
refining
explanations
causal
relations
etc
Using
point
valued
probabilities
without
knowing
their
error
is
not
such
big
sin
if
we
compare
this
with
what
we
do
in
fuzzy
logic
all
the
time
fuzzy
statement
such
as
"John
is
fairly
tall
"is
often
represented
simply
by
tall
john
which
is
analogous
to
representing
the
probabilistic
statement
"John
is
usually
punctual
"with
point
valued
probability
punctual
john
If
fuzziness
is
more
fundamental
phenomenon
in
our
knowledge
representation
we
should
be
making
more
fuss
about
fuzziness
than
probabilities
It
seems
that
we
ascribe
more
"prestige
"to
probability
theory
merely
because
of
psychological
reasons
Probabilistic
AND
and
OR
Specifying
the
CPT
conditional
probability
table
of
single
node
of
Bayesian
network
if
the
node
has
parents
would
require
entries
The
"noisy
"AND
and
OR
gates
are
designed
to
simplify
this
by
reducing
the
number
of
independent
entries
to
The
interpretation
of
the
noisy
OR
gate
is
that
each
parent
variable
is
associated
with
an
"inhibition
probability
"Pearl
or
This
is
the
textbook
definition
of
noisy
OR
and
created
noisy
AND
by
applying
DeMorgan's
law
(That
is
to
require
and
)It
seem
that
this
definition
of
noisy
AND
is
problematic
for
example
the
""there
should
be
close
to
TO
DO
Abram
Demski
pointed
out
that
the
textbook
definition
of
noisy
OR
is
actually
OK
and
there
is
no
need
to
invent
new
one
Anyway
defined
my
version
of
"probabilistic
"AND
and
OR
so
that
they
obey
DeMorgan's
laws
Each
variable
is
attached
with
"causal
strength
"such
that
when
they
reduce
to
classical
AND
and
OR
It
is
better
to
write
the
complement
as
superscript
So
the
pair
is
written
as
In
this
notation
my
definitions
can
be
expressed
simply
as
which
can
be
easily
generalized
to
When
expanded
they
yield
the
following
table
CPT
can
be
defined
by
combination
of
probabilistic
AND
OR's
where
the
are
parents
of
the
node
in
the
Bayesian
network
Notice
that
in
the
above
equation
each
connective
is
associated
with
pair
of
parameters
so
the
actual
equation
is
It
can
be
verified
that
association
holds
as
in
classical
logic
ie
fact
that
can
be
exploited
for
efficient
calculation
This
is
because
the
CPT
conditional
probability
table
has
size
where
is
the
number
of
variables
But
when
the
operator
is
associative
we
can
calculate
as
without
having
to
generate
the
full
CPT
Thus
the
space
complexity
remains
constant
It
is
also
possible
to
use
the
trick
in
sec
unifying
AND
and
OR
to
combine
AND
and
OR
with
the
parameter
Confidence
Pei
Wang's
uncertain
logic
is
particularly
elegant
adopt
two
ideas
from
his
theory
explained
below
but
the
way
use
those
numbers
differs
slightly
from
Wang's
cf
his
book
which
describes
an
AGI
called
NARS
Non
axiomatic
Reasoning
System
Positive
and
negative
evidence
In
Wang's
logic
each
statement
is
attached
with
numbers
number
of
positive
examples
number
of
negative
examples
In
an
AGI
system
they
are
the
number
of
times
statement
is
observed
to
be
true
or
false
respectively
For
example
for
the
statement
"if
is
female
probably
has
long
hair
"the
AGI
may
have
observed
females
with
long
hair
and
females
with
short
hair
The
total
support
for
the
statement
would
be
examples
Using
pair
of
numbers
allows
us
to
know
the
probability
of
statement
as
well
as
the
the
number
of
examples
that
support
that
statement
This
is
very
important
because
some
statements
may
be
supported
by
very
few
examples
and
thus
are
"weaker
"than
statements
with
more
support
major
advantage
of
this
number
approach
is
that
probabilities
can
be
updated
by
new
examples
For
example
if
the
AGI
encounters
new
female
with
long
hair
it
can
update
the
probability
easily
with
Such
updating
cannot
be
done
with
the
single
number
representation
of
probability
Confidence
Confidence
is
concept
borrowed
from
NARS
In
my
terminology
the
support
of
statement
is
defined
as
which
is
an
integer
from
to
As
Pei
Wang
did
in
NARS
the
confidence
is
the
value
obtained
by
squashing
into
using
nonlinear
transform
such
as
or
An
advantage
of
Wang's
method
is
that
it
allows
statements
to
have
substantial
confidence
earlier
so
it
gives
nascent
hypotheses
more
chance
to
succeed
The
advantage
of
my
method
is
that
it
allows
confidence
to
get
closer
to
sooner
so
the
incumbents
have
more
strength
The
choice
between
these
may
have
interesting
consequences
in
machine
learning
see
sec
confidenceInference
Confidence
and
probability
The
probability
or
frequency
of
statement
is
simply
Thus
contains
information
equivalent
to
the
frequency
and
confidence
pair
Notice
that
the
confidence
is
orthogonal
to
or
In
sec
defined
we
will
see
that
each
truth
value
in
our
logic
is
represented
as
tuple
where
describes
probability
distribution
and
is
the
confidence
Confidence
and
logic
To
see
how
confidence
is
defined
on
logical
formulae
it
is
easier
to
think
in
terms
of
the
support
The
support
of
rule
ie
logical
formula
containing
variables
can
be
defined
in
frequentist
manner
whereas
the
support
of
ground
fact
ie
formula
without
variables
is
always
derived
from
inference
For
example
the
rule
"Women
usually
have
long
hair
"female
long
hair
is
supported
by
number
of
instances
such
as
positive
example
female
mary
long
hair
mary
negative
example
female
jane
long
hair
jane
The
support
of
the
rule
is
the
total
number
of
such
instances
that
have
been
encountered
(Here
the
Raven's
paradox
or
Hempel's
paradox
is
relevant
Given
the
rule
that
"women
usually
have
long
hair
"we
can
also
state
conversely
that
"people
with
short
hair
are
usually
not
women
"Thus
man
with
short
hair
would
become
supportive
example
of
this
rule
which
is
counter
intuitive
TO
DO
resolve
this
)ground
fact
cannot
be
given
the
same
treatment
because
it
has
no
instances
being
itself
an
instance
and
its
confidence
must
be
inferred
Inference
of
confidence
is
treated
in
sec
confidenceInference
Inference
syllogism
has
parts
therefore
this
is
not
syllogism
By
inference
we
mean
deduction
and
abduction
induction
is
regarded
form
of
learning
What
are
the
computational
bottlenecks
How
to
speed
things
up
Some
background
Resolution
In
essence
the
resolution
rule
is
-----Resolution
is
popular
deduction
technique
for
binary
logic
we
will
not
use
it
but
it
is
helpful
to
know
Horn
clauses
and
Prolog
In
essence
Horn
clause
is
rule
of
the
form
The
Horn
form
makes
deduction
particularly
efficient
because
it
is
like
IF
THEN
rules
in
production
systems
Resolution
in
Horn
form
is
the
basis
of
Prolog
Forward
and
backward
chaining
These
are
deduction
algorithms
Forward
chaining
starts
with
the
premises
and
applies
deduction
steps
in
the
forward
direction
to
try
to
arrive
at
conclusion
goal
conclusion
may
be
specified
as
the
search
termination
criterion
if
not
it
is
called
goal
less
forward
chaining
Backward
chaining
starts
with
the
goal
to
be
proven
and
applies
deduction
steps
in
the
backward
direction
The
search
graph
terminates
with
nodes
that
are
the
premises
Complexity
of
inference
Some
facts
Satisfiability
in
binary
first
order
logic
is
semi
decidable
ie
it
terminates
in
finite
time
if
proof
exists
but
may
never
terminate
if
proof
does
not
exist
SAT
for
binary
propositional
logic
is
NP
complete
Propositional
Bayesian
network
inference
is
complete
ie
it
is
as
hard
as
returning
the
number
of
solutions
to
an
NP
complete
problem
The
naive
algorithm
for
propositional
Bayesian
network
inference
is
where
is
the
number
of
nodes
Pearl's
message
passing
algorithm
is
an
exact
algorithm
for
propositional
Bayesian
network
inference
therefore
it
also
requires
non
polynomial
time
in
the
worst
case
Pearl's
algorithm
when
operating
on
trees
is
linear
time
But
trees
seem
to
be
insufficient
for
AGI
reasoning
SAT
for
the
propositional
Horn
subclass
of
binary
logic
can
be
performed
in
linear
time
My
very
simple
algorithm
for
Bayesian
inference
is
analogous
to
Horn
deduction
(The
term
Horn
can
only
be
used
to
describe
binary
logic
Horn
clause
is
clause
with
exactly
one
positive
literal
logic
has
Horn
like
form
but
the
distinction
between
positive
and
negative
literals
disappears
in
logics
with
numerical
truth
values
)Prolog
owes
its
efficiency
to
SLD
resolution
which
is
also
Horn
based
Deduction
Predicate
logic
and
substitution
management
We
have
argued
that
propositional
th
order
logic
is
inadequate
for
AGI
The
situation
is
illustrated
as
follows
This
is
known
as
KBMC
knowledge
based
model
construction
What
it
means
is
that
we
need
to
fetch
logical
rules
from
the
KB
and
instantiate
those
rules
to
form
the
propositional
Bayesian
networks
upon
which
probabilities
are
propagated
Substitution
management
in
first
order
logic
provers
is
quite
complicated
This
is
famously
explained
in
classics
such
as
SICP
where
the
authors
advocated
using
lazy
sequences
also
known
as
streams
Think
of
sequence
of
solutions
attached
to
each
variable
node
of
the
proof
tree
Each
solution
consists
of
substitution
and
TV
When
we
carry
out
an
operation
between
variables
say
we
are
not
just
dealing
with
objects
but
with
sequences
of
possible
solutions
Let's
denote
the
sequences
as
and
For
each
solution
of
each
sequence
we
need
to
Match
the
substitutions
and
ie
see
if
they
are
compatible
and
if
so
merge
the
substitutions
Let's
denote
the
matching
operation
as
If
the
substitutions
are
compatible
perform
the
calculation
of
the
CPT
conditional
probability
table
Let's
denote
this
operation
as
Now
denote
the
combined
action
of
and
as
Then
what
we
need
to
calculate
can
be
expressed
as
where
is
Cartesian
product
and
is
extended
as
an
ary
operation
Note
that
each
is
lazy
sequence
so
this
formula
is
very
succinct
Further
we
can
"curry
"the
function
so
that
where
we
have
made
use
of
the
associativity
of
see
eqn
The
final
RHS
is
what
we
will
be
coding
It
is
quite
amazing
to
realize
that
the
proof
tree
in
our
program
is
actually
many
propositional
Bayes
nets
overlaid
together
Pure
probabilistic
inference
ie
Bayesian
network
belief
propagation
Backward
chaining
algorithms
in
binary
logic
are
relatively
familiar
to
us
as
they
are
explained
in
most
AI
textbooks
One
of
the
simplest
backward
chaining
algorithms
is
the
SLDNF
resolution
(SLDNF
stands
for
"Linear
Selection
of
Definite
clauses
with
Negation
as
Failure
")used
in
Prolog
see
sec
Prolog
Horn
intro
Originally
tried
to
adapt
SLDNF
resolution
for
probabilistic
inference
but
was
doing
rather
sloppy
job
of
it
Thanks
to
Abram
Demski
who
pointed
out
that
my
"Prolog
trick
"could
actually
be
replaced
with
exact
Bayesian
inference
via
technique
known
as
factor
graphs
our
inference
algorithm
is
now
on
very
good
theoretical
foundation
TO
DO
There
should
be
mechanism
to
prune
low
probability
or
should
it
be
low
confidence
branches
early
on
in
the
search
Factor
graph
algorithm
Factor
graphs
are
way
to
unify
probabilistic
graphical
models
such
as
Bayesian
networks
and
Markov
networks
The
important
thing
to
know
is
that
factor
graphs
are
isomorphic
to
the
proof
trees
we
construct
during
backward
chaining
and
thus
we
can
perform
Bayesian
belief
propagation
in
situ
in
the
proof
tree
very
conveniently
For
example
Simple
proof
tree
factor
graph
In
this
example
we
can
see
that
the
factor
node
black
square
is
located
exactly
as
the
"rule
node
"in
the
proof
tree
therefore
we
might
as
well
label
the
node
with
its
operator
""The
algorithm
What
we
need
to
calculate
for
goal
variable
node
is
where
is
the
background
knowledge
is
called
the
prior
distribution
of
When
the
graph
has
no
loops
then
it
is
tree
and
the
factor
graph
algorithm
begins
from
the
leaves
of
the
tree
to
the
root
which
is
the
goal
node
The
leaves
pass
messages
up
to
their
parents
with
each
factor
node
performing
sum
product
operation
until
the
messages
reach
the
root
Assume
that
we
have
rule
in
the
form
where
the
stand
for
"Heads
"This
is
actually
compact
way
of
stating
more
than
rule
-with
each
head
corresponding
to
rule
(To
do
explain
why
the
multiple
head
feature
is
desirable
)For
simplicity's
sake
we
assume
number
of
For
example
Simple
factor
graph
in
"normal
"direction
with
background
knowledge
The
link
to
simply
means
that
either
the
node's
value
is
"clamped
"or
it
is
passing
message
from
further
down
the
tree
In
the
"normal
"direction
our
is
so
the
calculation
is
pretty
straightforward
From
now
on
we
use
the
notational
convention
of
omitting
for
probabilities
ie
is
written
as
The
first
factor
on
the
RHS
is
the
"factor
"stored
in
the
factor
node
The
second
factor
is
the
messages
coming
from
each
node
This
is
simple
sum
product
operation
Next
case
the
direction
is
reversed
Simple
factor
graph
in
backward
direction
Notice
how
the
rule
is
not
pointing
to
the
goal
and
is
now
taking
the
place
of
We
need
to
apply
Bayes
rule
twice
First
work
out
the
node
case
where
don't
exist
General
case
Higher
order
Bayesian
logic
Note
here
"higher
order
"refers
to
the
predicate
logic
aspect
not
higher
order
probabilities
Essentially
what
we
need
to
do
is
to
instantiate
all
possible
propositional
Bayesian
networks
with
given
goal
at
the
root
when
given
set
of
higher
order
formulas
in
the
KB
I'm
not
sure
if
it
is
enumerable
but
assume
that
it
is
In
first
order
Horn
logic
the
instantiation
procedure
seems
pretty
easy
as
it
follows
the
SLDNF
resolution
algorithm
sec
Prolog
Horn
intro
With
higher
order
logic
the
task
is
complicated
by
the
fact
that
variables
can
instantiate
as
other
formulas
Our
slogan
is
"One
formula
one
Bayesian
network
link
one
factor
graph
node
"Or
in
symbols
To
do
Higher
order
unification
seems
to
work
fine
with
such
formulas
but
need
to
work
out
the
details
Loopy
inference
Hybrid
inference
Rules
which
are
the
building
blocks
of
inference
can
be
of
types
The
type
of
rule
is
determined
by
its
"head
"-eg
if
the
head
is
variable
then
the
rule
is
rule
Using
unified
truth
values
we
can
plug
variables
of
any
TV
type
into
any
rule
So
the
rule
type
only
affects
how
the
rule
is
interpreted
which
will
be
explained
below
An
example
of
rule
is
"almost
"means
"close
to
but
not
being
"The
TV
types
of
the
predicates
are
"almost
"-"close
to
"-"not
"-So
we
can
represent
it
with
rule
Despite
using
rule
the
result
of
inference
will
be
truth
value
with
binary
character
This
is
good
thing
because
the
idea
of
"almost
"can
be
fuzzy
in
some
cases
for
example
"heard
that
John
almost
got
killed
by
the
assassination
""Not
really
the
bullet
only
hit
his
toe
"To
simplify
things
consider
single
inference
steps
complete
proof
involves
series
of
such
steps
rule
The
rule
can
have
operators
and
All
we
need
to
do
is
to
show
how
to
evaluate
the
operators
for
values
An
example
rule
is
bachelor
male
married
Case
We
need
to
convert
the
distribution
to
one
with
binary
character
This
is
done
by
setting
the
variance
to
fixed
value
in
the
binary
regime
sec
unifying
Then
the
result
is
negated
by
transforming
the
mean
Case
Convert
the
value
to
via
eqn
Then
assuming
independence
The
variance
is
assigned
fixed
value
so
that
the
resulting
variable
has
binary
character
Note
that
the
resulting
variance
is
not
affected
by
the
variances
of
the
premises
because
the
aim
of
rule
is
to
form
binary
judgment
Case
Similar
to
above
with
again
assuming
independence
rule
rules
can
have
the
operators
min
max
and
the
fuzzy
modifier
At
this
stage
we
ignore
Soft
min
and
max
The
are
applied
before
the
other
operators
Case
If
and
is
given
by
the
probability
density
function
then
the
probability
density
of
would
be
given
by
which
is
explained
in
If
is
the
Gaussian
function
given
in
eqn
then
but
there
is
glitch
has
pieces
and
their
contributions
must
be
summed
up
piecewise
Sometimes
the
resulting
distribution
looks
irregular
for
example
but
we
can
approximate
it
with
Beta
distribution
by
preserving
the
mean
and
variance
The
irregular
appearance
does
not
matter
that
much
if
we
only
care
about
the
mean
and
variance
have
obtained
the
approximation
formula
by
numerical
integration
and
nonlinear
regression
on
randomly
generated
samples
as
follows
mean
of
input
mean
of
result
Gaussian
fuzzy
modifier
empirical
data
and
result
of
regression
Case
Simply
negate
the
distribution
by
eqn
with
variance
unchanged
Case
Given
ie
What
we
need
to
do
here
is
to
calculate
the
PDF
of
random
variable
given
as
function
of
two
other
random
variables
The
procedure
is
given
in
many
textbooks
Here
denotes
probability
measure
which
is
set
function
denotes
CDF's
denotes
PDF's
assuming
independent
where
The
result
we
want
is
and
we
need
to
apply
Leibnitz's
Rule
(For
function
of
defined
by
its
differentiation
is
given
by
Leibnitz's
Rule
)Then
we
get
This
function
has
an
interesting
property
it
appears
to
add
up
the
distributions
and
with
the
one
on
the
right
being
dominant
ie
giving
the
biggest
contribution
of
probability
mass
or
area
under
the
curve
Some
examples
are
as
follows
Once
again
we
approximate
by
preserving
the
mean
and
variance
The
irregular
appearance
is
not
so
important
By
looking
at
the
following
graph
we
can
see
that
the
mean
of
the
result
is
mostly
dominated
by
which
is
the
mean
of
the
input
further
to
the
right
ie
There
is
sometimes
slight
shift
to
the
right
relative
to
the
dots
above
the
line
but
it
seems
insignificant
and
may
be
ignored
In
other
words
the
fuzzy
inference
rule
is
very
simple
the
mean
of
the
result
is
just
the
greater
mean
of
the
inputs
similar
graph
shows
that
the
variance
of
the
result
is
also
approximately
equal
to
the
variance
of
the
rightmost
input
Plots
of
mean
and
variance
output
vs
input
Case
The
result
for
is
similar
again
assuming
independent
In
summary
the
fuzzy
inference
rules
are
For
For
For
For
where
the
are
regression
parameters
rule
rule
is
of
the
form
To
simplify
things
we
do
not
implement
the
Bayesian
network
algorithm
instead
we
only
perform
the
calculation
from
in
one
direction
This
is
an
extreme
simplification
of
probability
logic
but
it
may
already
be
sufficient
for
common
sense
reasoning
We
will
try
this
first
and
see
how
far
it
can
go
The
variables
can
be
converted
to
variables
via
eqn
Then
the
value
of
the
consequent
can
be
obtained
via
straightforward
application
of
probabilities
eg
where
Note
that
it
reduces
to
the
binary
case
when
The
result
is
then
converted
back
as
distribution
with
binary
character
rule
We
do
not
use
truly
rules
because
they
are
too
complex
and
not
needed
for
common
sense
reasoning
The
rules
we
have
are
simpler
operations
that
allow
us
to
manipulate
the
distributions
of
variables
We
can
directly
assign
the
mean
and
variance
to
variable
Or
we
can
fix
the
probability
at
particular
point
neighborhood
For
example
"the
probability
of
Mary
being
fat
is
"Inference
of
confidence
Confidence
was
introduced
in
sec
confidence
There
explained
that
The
confidence
of
rules
can
be
measured
in
frequentist
way
(Note
however
that
the
confidence
is
not
frequency
it
is
just
related
to
the
frequency
)The
confidence
of
ground
facts
must
be
inferred
The
problem
is
we
must
initially
know
the
confidence
of
at
least
some
ground
facts
in
order
to
infer
those
of
others
To
this
end
propose
convention
to
assign
all
raw
sensory
events
eg
"camera's
pixel
records
the
RGB
color
at
time
"to
have
confidence
and
thus
support
It
should
not
have
confidence
because
that
would
mean
informally
that
the
statement
is
so
weak
as
to
have
no
support
at
all
In
general
each
inference
step
involves
rule
of
the
form
where
the
rule
has
frequentist
confidence
and
the
antecedents
each
has
an
inferred
confidence
So
how
to
calculate
from
Let's
start
from
the
simplest
case
Consider
the
rule
where
confidence
of
the
rule
confidence
of
the
antecedent
and
we
seek
the
formula
for
By
considering
the
following
"boundary
"cases
the
numbers
are
confidences
we
surmise
that
degenerates
into
binary
AND
In
other
words
we're
seeking
an
operator
that
generalizes
binary
AND
Naturally
this
can
be
either
probabilistic
AND
the
product
rule
or
fuzzy
AND
min
rule
The
min
rule
epitomizes
the
proverb
"chain
is
only
as
strong
as
its
weakest
link
"Its
special
property
is
that
rule
of
confidence
can
be
repeatedly
applied
without
decreasing
the
conclusion's
confidence
The
probabilistic
rule
forces
confidence
to
decay
in
long
inference
chains
if
the
confidence
of
each
step
is
At
this
point
it
is
hard
to
say
which
method
is
superior
To
recap
Every
rule
has
frequentist
confidence
ground
fact's
confidence
is
inherited
from
the
rules
that
entail
it
Confidence
is
useful
in
ways
As
termination
criterion
for
proof
search
statement
of
low
probability
can
still
be
significant
eg
"it
is
highly
unlikely
that
an
AGI
can
be
run
on
an
APPLE
"statement
is
insignificant
if
its
confidence
is
close
to
In
belief
revision
when
inference
chains
arrive
at
different
conclusions
their
confidences
will
decide
which
has
the
greater
weight
Putting
it
all
together
the
deduction
algorithm
We
are
now
able
to
work
out
the
deduction
algorithm
This
algorithm
can
be
very
fast
because
the
logic
is
truth
functional
ie
it
ignores
long
range
probabilistic
dependencies
the
logic
is
algebraic
ie
it
does
not
use
the
binary
implication
operator
Every
rule
in
the
logic
is
of
the
conditional
form
where
is
one
of
the
operators
described
earlier
The
deduction
algorithm
will
be
given
query
goal
It
then
seeks
rules
in
the
KB
with
as
the
head
and
it
basically
performs
recursion
with
as
subgoals
simple
backward
chaining
knowledgebase
query
the
truth
value
of
alg
simple
backward
chaining
get
list
of
rules
potentially
applicable
to
algline
we
maintain
an
index
of
predicates
so
we
can
quickly
find
the
rules
with
the
same
head
predicate
as
select
an
applicable
rule
whose
head
unifies
with
recursion
has
gone
too
deep
the
incomplete
proof
is
already
too
long
'fail
evaluate
the
rule
algline
recurse
to
evaluate
the
variables
in
the
rule
body
The
search
space
of
this
algorithm
is
so
called
AND
OR
graph
or
tree
that
often
appears
in
logic
based
AI
search
Each
AND
is
represented
by
horizontal
line
whose
elements
are
the
arguments
of
an
operator
and
all
of
them
must
be
evaluated
each
OR
is
represented
by
set
of
child
branches
shown
in
red
in
the
diagram
whose
elements
are
rules
from
the
KB
and
only
one
needs
to
be
selected
Thus
the
search
is
basically
matter
of
selecting
which
rules
to
apply
Complexity
If
is
the
average
number
of
rules
applicable
to
head
predicate
is
the
average
number
of
arguments
for
each
rule
and
is
the
depth
of
the
search
tree
then
the
size
of
the
tree
is
Let
me
make
wild
guess
that
then
size
This
sounds
manageable
but
don't
forget
that
the
complexity
of
learning
is
the
exponentiation
of
this
So
it
is
important
to
keep
things
simple
at
this
stage
Efficient
deduction
Inference
is
often
grounded
by
facts
in
Working
Memory
ie
things
that
the
AGI
is
currently
paying
attention
to
though
it
may
also
be
grounded
by
facts
recalled
from
LTM
Long
Term
Memory
Perhaps
an
efficient
search
algorithm
should
simultaneously
use
backward
chaining
from
the
goal
and
forward
chaining
from
ground
facts
under
current
attention
Given
head
predicate
the
KB
server
can
quickly
retrieve
list
of
all
rules
with
that
predicate
by
looking
up
an
index
Also
this
list
can
be
assumed
to
be
sorted
in
order
of
confidence
remember
that
each
rule
is
associated
with
frequentist
confidence
This
order
may
provide
basis
for
best
first
heuristic
search
Maybe
the
search
should
be
randomized
and
maybe
it
can
start
from
the
"middle
"ie
we
generate
an
initial
proof
that
is
incorrect
or
incomplete
then
repeatedly
mutate
it
to
make
it
correct
(This
idea
is
inspired
by
the
GSAT
and
WalkSAT
algorithms
for
propositional
logic
)This
suggests
using
an
evolutionary
algorithm
but
EA's
may
be
slow
TO
DO
explore
this
idea
further
Solving
this
search
problem
is
very
important
because
inductive
learning
also
depends
on
such
search
Abduction
Logic
based
abduction
and
induction
are
closely
related
they
are
faces
of
the
same
coin
In
both
cases
we
seek
hypothesis
that
together
with
background
knowledge
entails
positive
example
the
only
difference
is
that
for
abduction
is
ground
ie
contains
no
variables
and
for
induction
is
non
ground
In
classical
logic
the
algorithm
for
abduction
is
identical
to
deduction
backward
chaining
except
that
the
termination
criterion
is
different
In
deduction
the
leaves
of
the
search
tree
should
be
ground
facts
in
the
KB
which
are
believed
to
be
true
For
abduction
the
leaves
should
be
so
called
abducibles
which
are
propositions
that
can
be
assumed
to
be
true
For
example
"rain
"is
common
occurrence
and
thus
can
be
assumed
to
be
true
in
order
to
account
for
the
grass
being
wet
Under
probabilistic
logic
we
no
longer
need
the
notion
of
abducibles
Interestingly
in
this
case
the
algorithm
for
abduction
becomes
almost
identical
to
that
of
deduction
with
the
exception
that
deduction
starts
with
an
unknown
fact
whose
truth
is
to
be
determined
whereas
abduction
starts
with
known
fact
in
need
of
explanations
The
search
spaces
are
exactly
the
same
Pattern
recognition
The
diagram
below
is
due
to
David
Marr
highly
simplified
example
of
pattern
recognition
is
the
visual
recognition
of
human
body
by
Prolog
rule
such
as
(Note
that
these
concepts
refer
to
visual
features
rather
than
ideas
The
abstract
concept
of
human
can
be
recognized
via
larger
set
of
rules
that
includes
the
visual
rules
The
visual
features
should
also
be
related
to
each
other
by
some
spatial
predicates
but
we
ignore
them
here
The
details
of
visual
recognition
is
explained
in
ch
vision
)human
head
torso
arm
arm
leg
leg
and
each
body
part
can
be
further
recognized
by
its
components
such
as
arm
upper
arm
forearm
hand
fingers
In
general
the
mechanism
for
pattern
recognition
is
forward
chaining
because
we
start
with
the
premises
sensory
input
and
we
do
not
know
the
desired
conclusions
in
advance
The
theory
based
theory
Concept
formation
or
"categorization
"in
the
cognitive
science
literature
is
the
task
of
using
machine
learning
to
learn
common
sense
concepts
Nakamura
has
summarized
the
following
properties
of
human
concepts
concepts
often
have
non
necessary
features
disjunctive
concepts
there
may
not
be
any
features
that
are
shared
by
all
members
of
concept
relational
information
seems
to
require
first
order
logic
to
represent
some
features
are
themselves
concepts
typicality
people
can
often
rank
examples
according
to
typicality
eg
the
most
typical
fruit
is
orange
basic
levels
people
are
more
adapt
at
categorization
at
certain
basic
levels
eg
naming
an
object
"chair
"rather
than
"office
chair
"or
"piece
of
furniture
"superordinate
distance
eg
"chicken
"is
rated
more
similar
to
"animal
"than
to
"bird
"unclear
cases
eg
"is
tomato
fruit
"context
dependent
effects
goal
dependent
effects
There
are
two
major
theories
of
categorization
In
the
Classical
view
concept
is
defined
by
set
of
defining
features
which
are
individually
necessary
and
sufficient
This
view
has
very
few
adherents
now
The
other
major
theory
is
the
Exemplar
view
which
classifies
instances
based
on
their
similarity
eg
distance
metric
to
set
of
existing
exemplars
In
my
opinion
also
shared
by
and
the
most
satisfactory
solution
aka
the
theory
based
theory
is
to
view
categorization
as
an
inference
process
where
concept
formation
means
constructing
explanations
of
why
certain
objects
belong
to
concept
Similarity
Similarity
is
an
essential
component
of
commonsense
reasoning
For
example
if
we
are
told
that
the
whale
is
like
fish
except
that
it
is
mammal
we
could
draw
the
conclusion
that
the
whale
can
swim
because
fishes
usually
can
swim
(But
we
would
not
conclude
that
the
whale
lays
eggs
because
mammals
don't
lay
eggs
and
we
know
that
the
whale
is
mammal
but
it
is
only
similar
to
fish
and
is
is
stronger
than
similar
to
)The
similarity
measure
may
also
help
in
associative
recall
sec
associative
memory
From
equality
to
similarity
Similarity
can
be
viewed
as
generalization
of
the
equality
relation
with
fuzzy
probabilistic
truth
For
example
we
can
write
Equality
can
be
defined
via
Leibniz's
axiom
of
extensionality
-which
states
that
functions
are
identical
if
their
extensions
are
the
same
In
higher
order
logic
it
can
be
expressed
as
)with
types
and
This
definition
of
is
given
by
Church's
type
theory
in
can
be
regarded
as
the
probabilistic
version
of
We
can
simply
replace
the
quantification
in
with
the
probabilistic
quantification
sec
probabilistic
quantifier
)as
in
Leibniz
extensionality
and
intensionality
Our
strategy
is
this
first
we
will
formulate
the
most
general
form
of
semantic
similarity
which
we
expect
to
be
computationally
intractable
then
we
try
to
approximate
it
Leibniz
extensionality
defines
equality
for
functions
or
predicates
but
not
for
individual
objects
For
the
latter
purpose
we
have
to
invert
extensionality
to
intensionality
as
follows
where
have
renamed
variables
to
stress
the
symmetry
The
second
axiom
says
that
entities
are
the
same
if
every
property
of
one
is
also
property
of
the
other
An
example
of
intensionality
which
may
be
more
common
than
extensionality
is
Notice
that
is
higher
order
variable
in
which
can
be
instantiated
as
ary
predicates
with
other
arguments
eg
so
together
with
extensionality
this
offers
very
powerful
way
to
express
all
possible
forms
of
similarities
and
may
be
regarded
as
logic
based
alternative
formulation
of
Ulf
Grenander's
Pattern
Theory
But
and
require
higher
order
unification
which
is
highly
intractable
In
our
new
"combinatory
"logic
there
is
no
need
to
deal
with
structured
terms
with
functions
since
all
terms
are
built
up
solely
via
composition
Thus
we
can
combine
extensionality
with
intensionality
into
where
denotes
context
ie
ie
term
with
hole
in
it
is
the
most
general
form
of
similarity
This
kind
of
pattern
matching
is
exactly
the
kind
handled
by
unification
unification
that
means
we
know
at
least
in
theory
the
inference
algorithm
to
calculate
similarities
though
it
would
be
inefficient
Now
the
question
is
to
approximate
it
Distance
metric
Our
goal
is
to
find
an
algorithm
that
calculates
the
distance
between
arbitrary
logic
terms
Using
the
sum
of
products
form
first
we
try
to
define
the
distance
between
products
One
way
to
do
this
may
be
to
assign
to
each
atomic
concept
matrix
so
they
form
basis
set
in
matrix
space
Then
product
would
have
matrix
value
Then
we
find
the
distance
by
calculating
as
the
distance
between
matrices
which
can
be
defined
via
the
inner
product
(The
inner
product
between
matrices
can
be
defined
as
for
real
matrices
and
for
complex
matrices
where
is
the
trace
and
is
the
adjoint
matrix
which
is
equal
to
the
conjugate
transpose
alias
Hermitian
transpose
of
ie
with
entries
So
)Next
the
distance
between
sums
and
can
be
defined
as
or
in
other
words
the
minimal
sum
of
distances
between
pairs
of
products
and
Then
we
can
use
learning
algorithm
to
tweak
the
values
of
the
basis
matrixes
For
example
cat
is
similar
to
dog
so
we
can
move
the
matrices
and
closer
to
each
other
Conversely
if
products
are
known
to
be
semantically
close
we
can
gradually
propagate
their
numerical
similarity
back
to
their
constituent
atomic
concepts
Is
it
always
possible
to
maintain
the
distances
in
such
matrix
space
consistent
with
our
notion
of
similarity
What
would
be
some
counter
examples
Formulas
with
variables
For
example
If
variable
can
take
any
matrix
value
the
product
map
end
up
in
any
position
in
the
matrix
space
But
we
may
be
able
to
establish
some
hard
or
soft
constraints
over
such
that
the
resulting
product
would
be
confined
in
certain
regions
It
seems
that
formula
with
variables
would
be
some
sort
of
"hyperplane
"in
the
matrix
space
We
can
still
define
the
distance
between
hyperplane
and
other
matrices
Examples
From
dog
cat
using
the
backward
direction
of
we
can
deduce
let
sleeping
dogs
lie
let
sleeping
cats
lie
but
the
result
is
incorrect
and
un
idiomatic
Does
it
break
our
matrix
based
method
The
following
idioms
are
similar
in
semantics
but
dissimilar
in
syntax
em
Chinese
idiom
Those
who
retreated
steps
laugh
at
those
who
retreated
steps
but
there
are
some
weak
fuzzy
correspondence
between
their
words
Does
it
break
our
matrix
based
method
If
not
how
can
we
back
propagate
the
similarities
to
their
atomic
concepts
We'd
be
interested
in
measuring
similarities
between
entities
with
certain
structures
This
has
been
considered
eg
in
chapters
in
an
ILP
setting
famous
example
is
the
similarity
between
the
solar
system
and
the
atom
the
important
point
is
that
both
systems
have
bodies
revolving
around
central
body
but
it
does
not
matter
what
their
size
mass
and
temperature
etc
are
The
text
that
follows
is
outdated
What
we
seek
is
correspondence
between
structured
entities
and
to
quantitatively
measure
the
strength
of
such
correspondence
Perhaps
we
can
count
the
number
of
relations
common
to
both
entities
"Marmite
is
similar
to
Vegemite
"So
propose
that
the
similarity
between
objects
can
be
calculated
by
considering
all
predicates
that
apply
to
both
objects
and
obtaining
the
ratio
between
the
number
of
identical
and
different
predicates
TO
DO
If
the
predicates
have
complex
structure
we
need
to
recursively
compare
the
other
arguments
and
if
the
latter
are
different
adjust
for
their
differences
The
above
calculation
can
also
be
weighted
by
information
utility
yielding
the
subjective
similarity
measure
NOTE
is
bad
example
and
should
be
replaced
by
something
else
"Lincoln
is
similar
to
Kennedy
"(In
this
example
have
used
my
own
knowledge
representation
scheme
Geniform
The
example
is
based
on
series
of
uncanny
coincidences
between
the
Lincoln
and
Kennedy
assassinations
that
are
often
cited
in
trivia
books
)Additionally
name
car
Lincoln
has
kennedy
secretary
name
secretary
Lincoln
The
additional
facts
do
not
increase
the
similarity
but
they
do
make
the
cases
seem
more
"connected
"Some
quantitative
examples
These
examples
can
be
represented
and
approximated
by
values
eg
with
fuzzy
pattern
recognition
Some
qualitative
examples
John
is
humorous
John
is
witty
junk
food
smoking
and
drinking
unhealthy
lifestyle
theft
burglary
complex
scene
fighting
in
bar
Belief
revision
Belief
revision
concerns
the
problem
of
conflicting
conclusions
and
in
the
extreme
contradictions
One
simple
question
is
If
we
arrive
at
two
distributions
from
two
separate
inference
chains
how
to
combine
the
answers
It
seems
that
this
is
related
to
statistical
inference
Suppose
we
have
population
set
of
observations
gives
the
estimate
of
the
mean
and
variance
Another
set
of
observations
gives
another
pair
of
estimates
The
question
is
to
find
reasonable
way
to
combine
the
sets
of
observations
to
give
new
estimate
In
other
words
how
to
express
in
terms
of
and
Maybe
the
is
just
the
weighted
average
of
and
can
be
assumed
to
be
equal
to
the
supports
which
can
be
calculated
from
the
confidences
Some
assumptions
in
the
above
may
not
be
entirely
justified
the
population
size
is
typically
small
and
the
samples
may
have
overlap
ie
not
independent
Justifications
Justifications
are
the
reasons
why
we
believe
in
something
Justification
based
Truth
Maintenance
Systems
JTMS
keep
track
of
justifications
explicitly
They
are
needed
because
otherwise
we
would
not
be
able
to
recall
why
certain
beliefs
are
in
the
mind
TO
DO
For
example
This
may
be
requirement
in
addition
to
probabilistic
reasoning
Many
worlds
representation
The
test
of
first
rate
intelligence
is
the
ability
to
hold
two
opposed
ideas
in
the
mind
at
the
same
time
and
still
retain
the
ability
to
function
-Scott
Fitzgerald
With
probabilistic
logic
it
may
be
possible
to
represent
multiple
possible
worlds
in
single
KB
by
storing
multiple
potentially
conflicting
assumptions
in
the
KB
For
example
in
the
Truman
Show
movie
there
may
be
competing
assumptions
in
Truman's
mind
Truman's
world
is
normal
Turman's
world
is
populated
by
fake
actors
In
binary
logic
we
can
only
accept
either
or
thus
the
belief
revision
problem
becomes
unnecessarily
much
harder
If
we
use
probabilistic
logic
then
all
we
need
is
to
impose
that
probabilities
of
alternative
assumptions
ie
mutually
exclusive
or
disjoint
events
sum
to
Disjoint
means
that
ie
and
cannot
be
true
at
the
same
time
The
general
rule
is
If
we
have
set
of
assumptions
such
that
then
Deliberative
assumptions
and
ATMS
Sometimes
we
make
deliberative
assumptions
for
more
efficient
reasoning
for
example
"Assuming
the
teacher
will
not
check
copy
my
classmate's
homework
"This
is
in
contrast
to
the
kind
of
implicit
assumptions
described
above
which
are
treated
probabilistically
for
example
"The
teacher
may
check
my
homework
with
probability
"The
difference
is
that
we
deliberately
force
the
reasoner
to
speculate
on
the
consequences
of
an
assumption
as
if
its
probability
is
This
kind
of
reasoning
is
similar
to
binary
logic
in
which
we
must
be
careful
about
keeping
track
of
conflicting
assumptions
Assumption
based
Truth
Maintenance
Systems
ATMS
are
developed
specifically
for
this
purpose
ATMS
seems
somewhat
redundant
if
we
have
probabilistic
logic
where
we
can
deal
with
multiple
assumptions
without
probabilistic
conflict
which
in
binary
logic
would
result
in
conflicts
If
an
assumption
is
particularly
important
probabilistic
reasoner
can
take
expected
utilities
into
consideration
Consistency
check
few
types
of
consistency
checks
are
needed
Conflict
resolution
Ie
what
to
do
when
an
inconsistency
is
found
Theory
revision
See
sec
inductive
learner
Inductive
learning
Rewards
and
punishment
is
the
lowest
form
of
education
-Zhuangzi
th
Century
BC
Note
reinforcement
learning
is
discussed
in
sec
RL
"Learn
by
being
told
"This
is
probably
the
most
efficient
learning
method
As
Martin
Magnusson
pointed
to
me
explored
the
possibility
where
human
is
able
to
teach
an
AI
via
natural
language
conversations
The
paper
contains
hypothetical
examples
of
some
such
conversations
The
Tell
Learn
loop
Translate
natural
language
sentence
into
logical
formula
Then
the
logical
formula
will
directly
enter
the
KB
as
knowledge
which
is
very
efficient
way
of
learning
The
main
difficulties
in
this
loop
are
The
assimilation
of
the
fact
into
the
KB
which
requires
belief
revision
The
translation
from
natural
language
to
logic
sometimes
this
require
abductive
reasoning
even
when
the
NL
parser
does
an
adequate
job
Surprisingly
inductive
learning
is
also
needed
see
below
The
base
logic
should
be
simple
for
machine
learning
close
to
NL
for
easy
translation
from
NL
to
logic
What
is
inductive
logic
programming
Inductive
learning
using
logic
is
studied
under
the
heading
ILP
inductive
logic
programming
an
excellent
and
up
to
date
survey
of
which
is
The
area
known
as
theory
revision
is
also
essential
to
AGI
an
excellent
new
book
on
ILP
that
discusses
theory
revision
is
Other
notable
books
on
induction
ILP
are
Induction
is
the
dual
of
abduction
both
seeks
hypothesis
to
explain
an
example
abduction
seeks
an
that
is
ground
fact
induction
seeks
an
that
is
general
rule
ie
containing
variables
Why
is
ILP
needed
From
my
observations
when
people
use
natural
language
to
express
ideas
they
usually
leave
many
gaps
that
must
be
filled
by
abductive
or
inductive
reasoning
parser
can
translate
NL
sentences
into
logic
literally
but
the
resulting
KB
would
be
incapable
of
reasoning
Filling
in
the
logical
gaps
is
very
hard
for
humans
because
the
inference
of
the
gaps
are
inaccessible
to
conscious
thinking
Therefore
an
inductive
machine
learner
is
required
to
perform
this
function
Examples
"Women
usually
have
long
hair
"Example
facts
Mary
is
girl
Mary
has
long
hair
Jane
is
girl
Jane
has
long
hair
John
is
guy
John
has
short
hair
etc
To
learn
female
long
hair
This
example
illustrates
the
MDL
principle
The
general
rule
covers
many
examples
and
thus
is
compressive
-In
some
situations
we
may
forget
individually
which
woman
has
long
hair
while
remembering
group
of
women
as
mostly
having
long
hair
eg
after
seeing
group
of
choir
singers
We
can
make
this
example
more
complex
by
adding
the
role
of
speaker
The
teacher
tells
me
"Mary
has
long
hair
"The
teacher
tells
me
"Mary
is
girl
"The
teacher
tells
me
"Jane
has
long
hair
"The
teacher
tells
me
"Jane
is
girl
"etc
We
can
have
the
general
rule
(In
this
example
we
have
used
tricks
propositions
as
variables
quoted
formulae
equality
just
for
the
sake
of
illustration
The
actual
KR
scheme
is
still
undecided
)"If
speaker
is
credible
source
then
what
he
says
can
be
assumed
to
be
true
"credible
says
But
notice
that
can
cover
the
girls
examples
only
if
we
include
the
quoted
sentences
Thus
we
can
further
compress
the
right
hand
sides
with
this
rule
female
long
hair
or
compress
what
the
teacher
says
says
teacher
female
long
hair
This
shows
that
even
when
an
example
is
explained
entailed
by
general
rule
it
may
still
require
compression
This
is
different
from
the
traditional
ILP
goal
which
is
simply
to
"cover
"ie
entail
positive
examples
Now
we
need
to
find
all
possible
ways
to
compress
the
KB
Roman
numerals
Example
facts
Roman
numeral
is
Roman
numeral
is
II
Roman
numeral
is
III
To
learn
Roman
numeral
is
which
is
wrong
but
is
reasonable
in
common
sense
Structure
of
the
hypothesis
space
very
important
concept
is
that
the
logical
hypothesis
space
is
structured
by
the
subsumption
order
An
interesting
phenomenon
is
that
the
probabilistic
truth
value
of
branch
of
hypotheses
follows
peaking
pattern
as
we
move
along
the
specialization
direction
Actually
the
truth
value
can
stay
constant
along
branch
that
specializes
by
adding
synonymous
conjuncts
until
such
synonyms
run
out
Also
the
truth
value
may
actually
decrease
when
conjunct
is
added
and
then
increase
again
when
other
conjuncts
are
added
For
example
non
obvious
idea
that
spreads
virally
will
make
startup
successful
but
each
conjunct
alone
may
decrease
the
truth
value
So
the
search
of
rules
cannot
be
pruned
prematurely
But
that
is
the
situation
of
rules
search
During
deduction
our
concern
is
of
rule
satisfaction
During
deduction
there
is
no
need
to
try
rules
that
are
past
the
maximum
they
may
constitute
the
frontier
of
rules
search
Complexity
Inducing
one
hypothesis
The
main
task
of
ILP
is
to
search
in
the
hypothesis
space
specified
partly
by
the
logical
syntax
such
as
logic
and
partly
by
background
knowledge
eg
what
kind
of
predicates
are
present
The
hypothesis
space
is
structured
into
lattice
with
partial
order
usually
denoted
The
partial
order
can
be
one
of
subsumption
LGG
least
general
generalization
or
logical
entailment
These
have
subtle
differences
with
logical
entailment
being
the
most
rigorous
The
searcher
can
traverse
up
and
down
the
lattice
corresponding
to
generalization
and
specialization
of
the
hypothesis
These
are
known
as
refinement
operators
Because
there
are
so
many
ways
to
generate
hypotheses
in
FOL
with
many
predicates
to
choose
from
and
the
possibility
of
introducing
variables
as
desired
the
branching
factor
is
extraordinarily
high
Also
the
lattice
can
be
infinite
so
we
need
to
limit
its
depth
Another
problem
is
that
once
hypothesis
has
been
generated
it
must
be
tested
against
the
rest
of
the
KB
for
consistency
The
consistency
check
must
also
be
limited
in
depth
but
even
then
it
is
very
time
consuming
Much
of
the
complexity
of
induction
occurs
in
the
"inventive
step
"whereby
new
rule
is
generated
The
inventive
step
can
draw
upon
knowledge
from
the
entire
KB
which
makes
it
combinatorially
explosive
Currently
the
most
promising
idea
have
is
to
"recall
similar
examples
"-that
means
we
perform
an
associative
memory
search
to
recall
examples
similar
to
the
current
experience
and
only
then
do
we
try
to
generalize
from
the
experience
plus
the
recalled
examples
But
this
strategy
merely
pushed
the
responsibility
to
the
associative
memory
to
recall
only
the
most
salient
relevant
interesting
examples
Still
this
trick
seems
to
be
more
efficient
than
blind
search
Another
promising
idea
is
that
some
of
the
more
intensive
processing
can
be
performed
during
sleep
Inducing
whole
theory
Let
denote
the
hypothesis
space
ie
space
of
all
logic
formulae
The
size
of
is
huge
see
above
logical
theory
is
set
of
hypotheses
Note
that
which
is
hugely
huge
space
Let
denote
the
set
of
examples
that
should
be
covered
by
the
target
theory
So
we're
seeking
an
optimal
theory
such
that
it
covers
all
examples
and
is
succinct
ie
and
is
minimal
There
is
hope
since
we
can
seed
the
initial
theory
with
human
knowledge
In
other
words
we
can
start
with
which
contains
large
number
of
logic
statements
translated
from
NL
Even
if
we
do
that
we
can
realistically
only
store
one
theory
in
memory
When
we
evaluate
new
hypothesis
to
cover
an
example
ie
to
test
whether
we
have
to
bear
in
mind
that
we
are
just
testing
the
hypothesis
one
background
theory
This
means
that
may
have
different
score
when
changes
but
unfortunately
we
cannot
realistically
keep
track
of
different
scores
of
against
different
theories
In
other
words
the
scores
we
keep
will
be
somewhat
inaccurate
and
are
relative
to
constantly
changing
candidate
theory
TO
DO
Due
to
the
use
of
probabilistic
logic
we
can
store
multiple
sets
of
hypotheses
simultaneously
Revise
this
paragraph
Compression
In
general
compression
is
achieved
via
pattern
recognition
ch
pattern
recognition
Upon
the
arrival
of
new
sensory
input
the
Pattern
Recognizer
is
invoked
to
recognize
patterns
in
the
data
the
process
is
forward
chaining
Due
to
the
use
of
fuzzy
pattern
recognition
the
result
may
be
lossy
which
is
good
Decompression
is
special
form
of
abduction
starting
from
the
recognized
patterns
facts
we
try
to
abduce
the
original
low
level
sensory
facts
Though
we're
usually
not
interested
in
low
level
facts
during
normal
abduction
Inductive
learning
is
kind
of
dual
of
pattern
recognition
whereas
pattern
recognition
recognizes
patterns
using
some
logic
rules
inductive
learning
learns
those
rules
The
job
of
the
Inductive
Learner
is
to
generalize
from
regularities
in
the
sensory
input
whenever
possible
adding
new
rules
to
the
KB
In
summary
compression
pattern
recognition
forward
chaining
decompression
abduction
backward
chaining
learning
to
compress
backward
chaining
with
rule
invention
Formal
definition
The
goal
is
to
find
theory
to
compress
set
of
examples
such
that
Note
the
entailment
should
be
fuzzy
probabilistic
About
coverage
Usually
an
example
can
be
covered
entailed
by
number
of
alternative
proofs
This
is
probably
very
common
For
example
but
we
can
further
compress
the
right
hand
sides
by
everyone
is
unhappy
even
though
the
right
hand
sides
were
entailed
by
different
reasons
Having
large
number
of
general
rules
enables
better
reconstruction
of
past
memories
Better
reconstruction
is
clearly
an
advantage
from
utilitarian
perspective
as
we
often
do
not
know
in
advance
what
memories
may
be
useful
later
Overlapping
coverage
may
be
by
product
of
having
large
number
of
rules
We
can
define
information
utility
as
the
utility
of
information
items
(An
infon
is
informally
defined
as
discrete
information
item
In
practice
it
can
be
any
proposition
in
the
KB
)as
generalization
of
utility
over
states
Then
the
goal
of
compression
is
to
minimize
reconstruction
errors
weighted
by
information
utility
with
the
smallest
theory
This
is
in
keeping
with
the
MDL
principle
In
traditional
ILP
with
binary
logic
we
settle
at
finding
theory
that
covers
all
positive
examples
and
none
of
the
negative
examples
In
the
compression
setting
mere
coverage
is
not
enough
we
should
generate
new
rules
even
when
the
data
are
already
entailed
by
some
existing
rules
because
the
new
rules
can
further
compress
data
Plateau
phenomenon
local
minima
Some
changes
in
may
increase
the
error
first
but
further
changes
in
that
direction
may
result
in
smaller
error
This
phenomenon
ie
local
minima
is
common
in
ILP
and
makes
learning
more
difficult
An
example
is
given
in
for
the
learning
of
the
Prolog
function
append
The
positive
and
negative
examples
are
And
the
target
clause
is
append
list
head
tail
append
cons
The
learning
of
the
first
literal
list
would
indeed
have
positive
information
gain
(The
definition
of
information
gain
is
that
used
by
the
ILP
program
FOIL
See
the
book
for
details
)as
it
excludes
the
last
negative
examples
but
the
addition
of
the
following
literals
would
have
small
or
even
negative
information
gain
until
the
last
literal
is
reached
and
all
positive
and
negative
examples
are
covered
Minimum
description
length
An
interesting
observation
noted
in
Chapter
and
is
the
equivalence
of
MDL
and
Bayesian
likelihood
maximization
Given
the
data
The
goal
of
perception
is
to
choose
the
hypothesis
that
maximizes
According
to
Bayes
theorem
this
is
equivalent
to
choosing
the
that
maximizes
where
represents
some
sort
of
subjective
prior
bias
The
that
maximizes
is
the
same
that
minimizes
the
negative
of
the
logarithm
of
this
term
which
can
be
written
By
Shannon's
coding
theorem
the
optimal
code
length
for
probability
is
so
we
can
interpret
the
above
as
codelength
codelength
Note
the
MDL
terminology
is
different
from
the
logic
terminology
TO
DO
How
does
this
shed
light
on
the
logical
compression
algorithm
Algorithm
Cf
algo
for
the
top
level
algorithm
of
the
logical
reasoner
The
goal
of
the
learning
algorithm
is
to
produce
useful
generalizations
or
we
can
say
the
most
compressive
generalizations
Practical
inductive
learner
incoming
facts
or
more
hypotheses
alg
practical
inductive
learner
Try
to
recall
similar
examples
from
memory
Invent
hypothesis
to
generalize
the
new
fact
examples
look
at
the
problem
from
perspective
that
is
slightly
different
from
the
one
prevalent
in
the
ILP
literature
Most
ILP
methods
search
in
the
hypothesis
space
but
find
it
easier
to
think
about
the
problem
in
proof
space
My
algorithm
searches
for
explanations
of
one
incoming
fact
while
inventing
possibly
many
hypotheses
along
the
way
The
hypothesis
space
algorithm
on
the
other
hand
focuses
on
one
hypothesis
It
would
pick
some
examples
past
or
incoming
facts
and
move
around
the
hypothesis
lattice
in
order
to
cover
them
ie
keeping
score
of
the
numbers
of
positive
and
negative
examples
that
are
covered
or
not
And
it
seeks
the
hypothesis
with
the
best
score
Natural
language
The
fish
trap
exists
because
of
the
fish
once
you've
gotten
the
fish
you
can
forget
the
trap
The
rabbit
snare
exists
because
of
the
rabbit
once
you've
gotten
the
rabbit
you
can
forget
the
snare
Language
exists
because
of
meaning
once
you've
gotten
the
meaning
you
can
forget
language
-Zhuangzi
th
Century
BC
Natural
language
is
not
essential
to
AGI
Our
approach
is
to
define
logic
that
can
faithfully
render
arbitrary
natural
language
texts
We
call
such
logical
form
Geniform
The
translation
of
NL
to
Geniform
can
itself
be
encoded
as
logical
rules
thus
the
parsing
of
NL
can
be
achieved
via
logical
inference
as
forward
chaining
in
bottom
up
manner
And
since
logical
inference
is
reversible
the
generation
of
NL
will
also
come
as
free
All
the
irregularities
of
NL
will
be
taken
care
of
via
machine
learning
Background
unification
based
grammars
The
family
of
unification
based
grammars
includes
LFG
Lexical
Functional
Grammar
HPSG
Head
Driven
Phrase
Structure
Grammar
and
PATR
grammar
The
unification
algorithm
used
in
unification
based
grammar
is
the
same
as
the
unification
algorithm
used
in
logic
This
is
further
evidence
that
the
brain
employs
abstract
symbolic
processing
similar
to
formal
logic
Background
cognitive
linguistics
Our
approach
is
closer
to
"formal
semantics
"Many
authors
most
famously
Richard
Montague
have
argued
that
natural
languages
are
formal
languages
Some
have
also
advocated
the
use
of
natural
language
as
knowledge
representation
in
AI
Thus
the
line
between
formal
and
natural
languages
is
blurred
How
may
cognitive
linguistics
affect
natural
language
processing
in
Genifer
Abduction
as
interpretation
detailed
example
The
general
sequence
is
tokenization
POS
tagging
syntax
parsing
semantic
parsing
which
should
be
familiar
to
everyone
with
experience
in
NL
processing
Let's
begin
with
John
loves
Mary
The
grammar
Sentence
NP
VP
NP
Noun
VP
Verb
NP
Verb
loves
Noun
John
Mary
Though
it
can
only
recognize
sentences
they
are
arguably
the
most
important
sentences
in
natural
language
The
crucial
thing
is
that
we
represent
everything
in
logic
based
framework
First
we
represent
the
sentence
as
raw
data
ignoring
tenses
to
simplify
matters
where
the
are
entities
logical
constants
the
entities
and
are
words
follows
means
word
follows
another
word
in
sentence
Up
to
now
all
we
have
is
sentence
as
raw
text
without
meanings
The
next
step
is
to
recognize
parts
of
speech
nouns
verbs
adjectives
etc
We
can
use
logical
rules
to
do
this
An
example
logical
rule
is
which
simply
means
that
the
word
Mary
is
noun
is
variable
implicitly
universally
quantified
is
new
entity
which
instantiates
to
when
the
rule
is
applied
We
can
also
use
logical
rules
to
parse
syntax
We
can
perform
"VP
"with
this
rule
(We
need
this
special
rule
)which
creates
new
entity
which
is
VP
Assume
that
eventually
we
have
parse
of
the
sentence
Notice
that
up
to
now
it
is
all
syntactic
parsing
Next
we
perform
semantic
parsing
The
key
is
to
generate
partial
meanings
for
phrases
such
as
the
verb
phrase
"loves
Mary
"Lambda
operator
In
formal
semantics
it
is
customary
to
use
the
operator
to
represent
the
meaning
of
phrases
The
reason
is
that
first
order
logic
do
not
have
the
expressive
power
to
represent
such
phrases
For
example
the
VP
"loves
Mary
"denotes
"somebody's
loving
Mary
"which
may
be
represented
as
but
that
is
not
well
formed
formula
in
FOL
Instead
we
can
represent
it
using
the
expression
Composition
of
concepts
For
details
see
sec
composition
Under
this
method
all
phrases
are
represented
by
compositions
For
example
the
VP
"loves
Mary
"can
be
represented
by
or
in
short
form
This
composite
is
first
order
object
ie
first
class
citizen
in
the
logic
which
can
be
further
manipulated
or
modified
eg
when
we
compose
it
with
we
get
which
is
equivalent
to
the
FOL
statement
think
this
method
is
superior
to
because
the
meanings
of
phrases
can
be
represented
by
first
order
objects
It
also
makes
semantic
parsing
very
intuitive
Explain
semantic
parsing
with
examples
Comprehensive
grammatical
categories
of
English
Here
are
some
NL
grammatical
categories
intended
to
cover
as
broadly
as
possible
and
typical
examples
in
English
Designers
of
NL
interfaces
can
supply
translations
to
these
examples
The
contents
in
this
section
and
all
the
examples
are
taken
directly
from
the
book
English
Grammar
by
Peter
Collins
Addison
Wesley
NOTE
This
may
be
copyright
infringement
but
have
contacted
the
author
without
getting
his
reply
This
page
is
intended
to
provide
simple
comparison
of
NL
interfaces
not
meant
to
be
standard
grammar
for
AGI
purposes
hang
blue
subsection
subsubsubsection
subsubsection
Nouns
Common
nouns
eg
chair
chairs
eg
importance
Proper
nouns
eg
Australia
eg
The
Isle
of
Man
Pronouns
eg
he
eg
our
chair
possessive
as
determiner
eg
theirs
eg
each
other
one
another
reciprocal
pronouns
eg
this
that
demonstrative
pronouns
eg
who
whom
whatever
interrogative
and
relative
pronouns
eg
some
both
any
each
none
nobody
indefinite
pronouns
Verbs
Main
verbs
eg
kick
grow
eg
to
kick
kicked
kicking
kicks
tenses
and
other
inflections
Auxiliary
verbs
eg
can
have
haven't
eg
She
is
dancing
the
lambada
eg
He
has
driven
for
hours
Operators
eg
they
will
try
their
best
Sam
won't
play
chess
Adjectives
Attributive
modifies
head
noun
eg
the
loud
bell
Predicative
eg
the
bell
was
loud
Gradation
eg
very
loud
bit
loud
Comparison
eg
louder
loudest
Adverbs
eg
The
policeman
acted
decisively
eg
He
is
somewhat
shy
young
man
eg
Actually
many
of
the
lifeboats
have
been
removed
eg
Patrick
has
lost
games
however
he
could
still
win
eg
more
surprisingly
most
surprisingly
Determinatives
Articles
eg
the
an
Demonstratives
eg
this
that
those
Interrogatives
eg
which
what
Cardinal
numerals
eg
one
two
Quantifiers
eg
both
all
every
any
no
much
less
Prepositions
Time
eg
after
our
match
during
the
exam
Place
eg
in
the
kitchen
against
the
wall
Manner
eg
with
ease
Agency
eg
by
the
mechanic
Recipience
eg
to
friend
Subordinates
Time
eg
until
it
rains
as
she
spoke
Condition
eg
if
we
win
Lotto
unless
catastrophe
occurs
Concession
eg
although
she
likes
coffee
while
it
was
undoubtedly
entertaining
Contrast
eg
whereas
Japan
is
creditor
nation
Exception
eg
except
he
didn't
have
the
strength
Reason
eg
because
the
bus
broke
down
Comparison
eg
you
ate
more
peanuts
than
did
Coordinators
eg
Peter
went
to
Paris
but
his
family
stayed
home
eg
Barbie
was
wearing
new
bracelet
and
diamond
ring
eg
Should
we
come
before
or
after
lunch
eg
Both
Bill
and
his
wife
deny
the
allegations
eg
She
neither
hates
him
nor
loves
him
eg
It's
not
for
you
but
for
me
Phrases
Noun
phrase
NP
eg
large
birds
with
sharp
claws
eg
writers
of
science
fiction
who
are
here
for
conference
Verb
phrase
VP
eg
Lisa
trains
times
week
eg
There
am
minding
my
own
business
when
this
tall
blonde
walks
up
and
asks
me
for
cigarette
eg
Ken
said
have
enough
money
eg
If
could
have
my
life
over
again
would
not
change
anything
eg
They
should
have
reached
the
peak
by
now
eg
Ken
is
building
new
house
eg
Many
footballers
get
injured
Adjective
phrase
AdjP
eg
too
rebellious
quite
surprisingly
intelligent
very
slow
eg
slower
than
wet
week
large
for
goldfish
eg
fiercer
than
expected
eg
sorry
that
he
hurt
her
feelings
noun
clause
eg
aware
of
the
consequences
of
PP
Adverb
phrase
AdvP
eg
very
quickly
most
reluctantly
eg
more
defiantly
than
they
had
predicted
Prepositional
phrase
PP
eg
in
the
water
eg
with
spoon
eg
way
below
standard
just
before
the
start
eg
Scott
arrived
in
limousine
with
his
girlfriend
Genitive
phrase
GP
eg
The
princess's
popularity
was
greater
than
her
husband's
Clauses
Object
vs
predicative
complements
eg
Mary
contacted
police
officer
object
eg
Mary
was
police
officer
predicative
was
copula
Subjective
and
objective
predicatives
eg
Indira
was
generous
subjective
eg
We
considered
Indira
generous
objective
Patterns
of
complementation
intransive
eg
Sue
stumbled
monotransitive
Od
eg
Sue
sipped
martini
Copulative
PCs
eg
Sue
seems
drunk
Ditransitive
Oi
Od
eg
Sue
offered
her
guests
martini
Complex
transitive
Od
PCo
eg
Sue
made
them
drunk
Other
complements
PP
complements
of
prepositional
verbs
eg
They
have
decided
on
short
vacation
eg
He
approves
entirely
of
your
decision
eg
Geoff
protected
his
little
brother
from
the
bullies
eg
She
blames
all
their
problems
on
his
incompetence
Adverbs
as
complement
of
phrasal
verbs
eg
Vera
cried
out
eg
We
give
up
eg
The
umpires
have
called
off
the
match
eg
Please
take
the
garbage
out
eg
We
have
come
up
with
an
alternative
plan
Non
finite
complements
of
catenative
verbs
eg
plan
to
keep
applying
for
jobs
eg
You
should
stop
trying
to
get
invited
Adjuncts
AdvP
eg
very
carefully
sometimes
moreover
PP
eg
over
the
road
with
torch
Subordinate
clause
eg
because
it
is
raining
after
the
music
stopped
NP
eg
next
Easter
this
Friday
types
Time
eg
at
Easter
in
week's
time
Frequency
eg
each
Friday
on
the
hour
Place
eg
in
Bandung
on
the
summit
Purpose
eg
in
order
to
test
the
hypothesis
for
break
Reason
eg
because
he
is
shy
on
the
grounds
of
sanity
Condition
eg
if
you
agree
unless
it
is
convenient
Manner
eg
more
politely
with
spade
Degree
eg
in
my
opinion
to
be
fair
unfortunately
Connective
eg
consequently
in
other
words
however
Mood
Declarative
mood
eg
He
is
serious
Interrogative
mood
eg
Is
he
serious
eg
Anastasia
has
sent
another
mail
hasn't
she
eg
Whose
shirt
is
that
eg
Who
can
help
me
eg
Where
are
my
keys
Imperative
mood
eg
Be
serious
eg
Give
me
some
more
money
eg
Don't
forget
to
write
eg
Let's
apply
for
loan
Exclamative
mood
eg
How
serious
he
is
eg
How
many
times
have
asked
you
Negation
clausal
negation
eg
Courtney
is
not
reading
eg
We
have
heard
nothing
eg
Nobody
likes
loser
eg
We
barely
made
it
eg
They
seldom
celebrate
birthdays
sub
clausal
negation
eg
Karen
is
very
unfriendly
eg
The
ambulance
arrived
not
long
ago
Subordination
and
coordination
Subordinative
clause
complex
sentence
eg
know
that
Ella
lives
in
Sydney
eg
He
knows
woman
who
lives
in
Sydney
eg
He
asked
whether
anyone
had
submitted
nomination
Adverbial
clauses
time
eg
Angelica
will
not
agree
to
it
until
his
attitude
improves
place
eg
he
goes
wherever
his
fancy
takes
him
etc
Relative
clauses
eg
Is
that
the
boy
who
you
were
referring
to
eg
remember
the
days
when
none
of
us
had
care
in
the
world
eg
know
who
is
replacing
you
eg
Whatever
is
now
developing
can
only
cause
harm
Comparative
clauses
eg
He
performed
worse
than
you
did
eg
Tim
has
won
larger
grant
than
have
eg
She
is
as
tall
as
we
had
anticipated
Non
finite
clauses
infinitival
eg
Sharon
wants
to
apply
next
year
present
participle
eg
Sharon
favors
applying
next
year
past
participle
eg
Sharon
has
her
application
lodged
already
past
participle
eg
Anyone
caught
smoking
here
can
be
prosecuted
Verbless
clauses
eg
If
in
doubt
consult
your
solicitor
eg
He
visits
his
parents
whenever
possible
eg
With
Claudia
in
charge
we
should
be
able
to
regain
control
Coordinated
clause
compound
sentence
eg
Ella
lives
in
Sydney
but
she
was
born
in
China
eg
Daphne
like
classical
music
but
her
husband
prefers
jazz
Information
packaging
Topic
eg
Leonardo
da
Vinci
painted
the
Mona
Lisa
vs
The
Mona
Lisa
was
painted
by
Leonardo
da
Vinci
Focus
eg
The
judges
gave
near
maximum
points
to
Lipinsky
vs
The
judges
gave
Lipinsky
near
maximum
points
Weight
more
weight
near
end
of
sentence
eg
It
surprised
us
that
an
Australian
skier
could
win
medal
in
the
slalom
event
at
the
Nagano
Olympics
vs
That
an
Australian
skier
could
win
medal
in
the
slalom
event
at
the
Nagano
Olympics
surprised
us
Voice
active
eg
Sergeant
Rogerson
arrested
the
thief
passive
eg
The
thief
was
arrested
by
Sergeant
Rogerson
eg
Each
specimen
was
carefully
dissected
Cleft
sentences
eg
Whitlam
recalled
the
remaining
troops
from
vietnam
It
was
Whitlam
who
recalled
the
remaining
troops
from
Vietnam
It
was
the
remaining
troops
that
Whitlam
recalled
from
vietnam
It
was
from
Vietnam
that
Whitlam
recalled
the
remaining
troops
eg
Salt
air
can
cause
rust
What
can
cause
rust
is
salt
air
Salt
air
is
what
can
cause
rust
Extraposition
eg
It
is
pity
that
CDs
are
so
expensive
Existential
sentences
eg
There
is
fly
in
my
soup
eg
There's
been
another
oil
spill
eg
There's
present
for
you
eg
There
followed
new
round
of
toasts
Reordering
Subject
complement
reversal
eg
Tom
is
the
short
one
The
short
one
is
Tom
Topicalization
eg
He
rejects
totally
the
corruption
charge
The
corruption
charge
he
rejects
totally
eg
It
rained
for
most
of
the
day
last
Saturday
Last
Saturday
it
rained
for
most
of
the
day
Locative
inversion
eg
Another
appeared
from
behind
the
clouds
From
behind
the
clouds
appeared
another
Dislocation
eg
Dr
Davidson's
never
available
on
Wednesdays
As
for
Dr
Davidson
he's
never
available
on
Wednesdays
Dative
movement
eg
Steve
gave
red
rose
to
his
girlfriend
Steve
gave
his
girlfriend
red
rose
Extraposition
from
NP
eg
position
for
someone
with
programming
expertise
is
available
position
is
available
for
someone
with
programming
expertise
Some
grammatical
devices
Co
reference
eg
Your
sister
rang
yesterday
She
asked
if
you
could
call
her
back
eg
It
is
inadvisable
to
visit
Jakarta
at
present
Civil
unrest
has
broken
out
there
Ellipsis
eg
Come
whenever
you
can
come
eg
Somebody
should
help
dad
I'll
ask
Barry
to
help
dad
Substitution
eg
have
spare
tickets
for
the
concert
Would
you
like
one
eg
She
achieved
less
this
year
than
she
did
last
year
eg
I'll
collect
the
kids
after
school
if
you
don't
get
the
opportunity
to
do
so
hang
blue
Geniform
-logical
form
for
natural
language
The
Geniform
logical
form
is
special
in
that
it
can
represent
all
natural
language
grammatical
categories
elegantly
To
illustrate
this
consider
John
loves
Mary
which
is
rendered
in
predicate
logic
as
loves
john
mary
ignoring
tense
But
what
if
we
want
to
say
Loving
Mary
is
foolish
The
verb
phrase
loving
Mary
is
grammatical
unit
that
has
specific
meaning
but
it
cannot
be
represented
in
predicate
logic
In
computational
linguistics
we
usually
use
the
abstraction
to
represent
it
loves
mary
but
as
you
can
see
this
form
is
not
very
human
readable
Instead
of
expressions
Geniform
uses
combinators
to
represent
concepts
resulting
in
more
elegant
syntax
that
uses
only
application
and
pairing
(For
details
about
combinatory
concept
composition
please
see
sec
composition
)So
John
loves
Mary
would
be
mary
loves
john
For
simplicity
we
can
omit
the
and
write
mary
loves
john
And
because
of
associativity
we
can
also
omit
the
parentheses
mary
loves
john
Notice
that
this
logic
formula
looks
exactly
like
English
in
reverse
Geniform
can
express
any
NL
category
For
example
chair
chair
wooden
chair
wooden
chair
chair
chair
chairs
plural
chair
chairs
plural
chair
wooden
chairs
plural
wooden
chair
any
chair
any
chair
no
chair
no
chair
Notice
that
chair
does
not
denote
one
chair
it
is
just
an
instantiation
of
the
abstract
concept
of
"chair
"additional
predicates
are
needed
to
specify
it
For
example
the
application
of
or
the
specifies
that
chair
is
singular
and
the
application
of
specifies
that
chair
numbers
It
may
stretch
your
understanding
bit
to
think
of
"any
chair
"and
"no
chair
"as
instances
of
"chair
"but
think
the
idea
is
sound
More
examples
expected
did
expect
(It
is
perhaps
interesting
to
note
that
linguists
believe
the
English
past
tense
such
as
expected
has
evolved
from
the
form
expect
did
)unexpectedly
ly
not
did
expect
We
may
but
I'm
not
sure
yet
need
way
to
differentiate
parts
of
speech
eg
He
loves
her
her
pronoun
loves
verb
he
pronoun
If
the
logic
has
types
we
can
use
intrinsic
types
to
represent
parts
of
speech
tags
otherwise
we
can
use
predicates
to
emulate
types
Currently
have
decided
that
Genifer's
logic
is
untyped
There
may
be
additional
problems
in
rendering
NL
into
logical
form
NL
notions
of
and
and
or
are
not
exactly
the
same
as
logical
NL
notions
of
not
are
not
exactly
the
same
as
logical
NL
notions
of
for
all
and
there
is
are
not
exactly
the
same
as
logical
But
speculate
that
these
minor
differences
can
be
corrected
manually
or
via
machine
learning
Nouns
noun
phrases
Common
nouns
chair
chair
wooden
chair
wooden
chair
chairs
chair
chair
plural
chair
Proper
nouns
China
china
Pronouns
Currently
the
Genifer
prototype
doesn't
try
to
resolve
pronoun
references
she
she
Verbs
Main
verbs
John
smiles
present
tense
smile
john
John
loves
Mary
mary
present
tense
love
john
Auxiliary
verbs
John
has
tried
did
try
has
john
or
has
did
try
john
but
believe
the
first
version
is
correct
She
is
dancing
the
lambada
the
lambada
dancing
is
she
He
has
not
slept
for
days
plural
day
for
past
participle
sleep
not
has
he
Adjectives
Attributive
young
girl
young
girl
Predicative
Evelyn
is
male
male
evelyn
It
is
interesting
that
the
syntactic
form
for
the
sentence
Evelyn
is
male
and
the
noun
phrase
the
male
Evelyn
are
identical
But
if
we
consider
the
sentence
The
male
Evelyn
won
the
prize
we
will
see
that
its
logical
form
consists
of
sub
statements
Evelyn
won
the
prize
and
the
Evelyn
who
won
the
prize
is
male
Gradation
Mary
is
very
shy
very
shy
mary
Comparison
Mary
is
prettier
more
pretty
mary
Mary
is
prettiest
most
pretty
mary
Adverbs
Tense
ignored
John
talks
fast
fast
talks
john
John
walks
slowly
ly
slow
walks
john
It
seems
that
the
modifier
ly
is
redundant
and
the
logical
form
can
be
simply
slow
walks
john
Determinatives
Articles
The
cat
is
black
black
the
cat
Demonstratives
This
cat
is
black
black
this
cat
Interrogatives
have
not
thought
about
the
representation
of
questions
in
logic
This
is
tentative
Which
cat
is
black
black
which
cat
We
need
to
add
some
sort
of
tag
to
mark
this
as
question
rather
than
an
ordinary
statement
Cardinal
numerals
Plurals
ignored
mangoes
mango
John
has
mangoes
mango
has
john
Quantifiers
Plurals
ignored
All
men
are
mortal
mortal
all
man
Most
AGI
researchers
are
crazy
crazy
most
agi
researcher
Every
person
in
the
room
is
happy
every
the
room
in
happy
person
Prepositions
John
is
in
the
kitchen
the
kitchen
in
john
John
stands
before
Mary
mary
before
stands
john
John
programs
in
Lisp
lisp
in
programs
john
John
eats
spaghetti
with
chop
sticks
chopSticks
with
spaghetti
eats
john
With
tense
John
cheated
during
the
exam
the
exam
during
did
cheat
john
Subordinates
They
danced
until
it
rains
until
did
dance
they
present
rain
it
where
the
are
statements
As
general
rule
every
syntactic
structure
must
separate
when
they
minimally
qualify
as
proposition
ie
having
truth
value
Paul
will
go
if
John
goes
if
go
will
paul
present
go
john
There
is
slight
irregularity
in
the
above
translation
We
could
have
written
future
go
paul
Pete
is
taller
than
John
john
than
more
tall
pete
Water
conducts
electricity
because
it
contains
ions
because
electricity
present
conduct
water
plural
ion
present
contain
it
Coordinators
John
prefers
Lisp
but
Pete
prefers
Java
but
lisp
present
prefer
john
java
present
prefer
pete
She
neither
hates
him
nor
loves
him
neither
nor
him
hates
she
him
loves
she
bit
unsatisfactory
John
is
male
or
John
is
female
or
male
john
female
john
male
john
female
john
Phrases
Noun
phrase
large
birds
with
sharp
claws
sharp
plural
claw
with
large
plural
bird
writers
of
science
fiction
who
are
here
for
conference
conference
for
here
science
fiction
of
plural
writer
Verb
phrase
Lisa
trains
times
week
week
per
times
present
train
or
week
times
present
train
would
not
change
anything
anything
not
change
would
or
anything
not
would
change
The
first
version
seems
correct
Many
footballers
get
injured
injured
get
Adjective
phrase
too
rebellious
too
rebellious
fiercer
than
expected
than
more
fierce
did
expect
where
is
an
introduced
constant
sorry
that
he
hurt
her
feelings
that
sorry
her
plural
feeling
did
hurt
he
aware
of
the
consequences
the
plural
consequence
of
aware
Adverb
phrase
most
reluctantly
most
ly
reluctant
more
defiantly
than
they
had
predicted
than
more
ly
defiant
past
participle
predict
had
they
Prepositional
phrase
with
spoon
spoon
with
way
below
standard
standard
way
below
Genitive
phrase
The
princess's
popularity
the
princess
popularity
or
the
princess
of
popularity
or
the
princess
of
popular
nounification
I'm
too
lazy
to
work
out
the
rest
of
the
examples
Clauses
Object
vs
predicative
complements
Mary
contacted
police
officer
Mary
was
police
officer
Subjective
and
objective
predicatives
Indira
was
generous
We
considered
Indira
generous
Complements
Sue
stumbled
Sue
sipped
martini
Sue
seemed
drunk
Sue
offered
her
guests
martini
She
made
them
drunk
He
approves
entirely
of
your
decision
We
give
up
plan
to
keep
applying
for
jobs
Adjuncts
in
order
to
test
hypothesis
Mood
He
is
serious
Is
he
serious
Be
serious
How
serious
is
he
Negation
Courtney
is
not
reading
We
have
heard
nothing
Nobody
likes
loser
They
seldom
celebrate
birthdays
Karen
is
very
unfriendly
The
ambulance
arrived
not
long
ago
Subordination
and
coordination
Subordination
know
that
Ella
lives
in
Sydney
He
knows
woman
who
lives
in
Sydney
He
asked
whether
anyone
had
submitted
nomination
Angelica
will
not
agree
to
it
until
his
attitude
improves
remember
the
days
when
none
of
us
had
care
in
the
world
She
is
as
tall
as
we
had
anticipated
Sharon
wants
to
apply
next
year
Anyone
caught
smoking
here
can
be
prosecuted
If
in
doubt
consult
your
solicitor
Coordination
Ella
lives
in
Sydney
but
she
was
born
in
China
Information
packaging
To
do
Minor
word
classes
Formulaic
words
eg
okay
thanks
bye
Some
example
English
sentences
If
Oswald
hadn't
shot
Kennedy
someone
else
would
have
implies
kennedy
did
shoot
not
did
have
Oswald
kennedy
did
shoot
have
did
will
someone
The
underlined
part
should
be
shared
The
if
then
syntax
is
not
rendered
naturally
There
am
minding
my
own
business
when
this
tall
blonde
walks
up
and
asks
me
for
cigarette
when
my
own
business
present
mind
there
am
cigarette
for
me
present
ask
up
present
walk
this
tall
blonde
Bison
that
rounded
snake
while
grazing
met
stack
of
green
hay
But
the
bison
waited
for
the
hay
to
turn
yellow
before
it
started
to
eat
Once
it
turns
yellow
hay
becomes
very
tasty
(This
sentence
was
made
up
by
Ivan
Vodi
ek
during
mailing
list
discussion
)gazing
while
snake
did
round
green
hay
of
stack
did
meet
bison
yellow
turn
to
the
hay
for
did
wait
bison
but
before
eat
to
did
start
bison
yellow
present
turn
hay
once
very
tasty
present
become
hay
This
diagram
illustrates
the
structure
of
the
first
formula
Notice
that
the
links
in
the
graph
are
directed
as
the
application
operator
is
non
commutative
ie
Memory
organization
and
optimization
Computer
science
has
only
three
ideas
cache
hash
trash
-Greg
Ganger
CMU
Introduction
We
need
way
to
organize
the
KB
for
efficient
information
retrieval
This
is
critically
important
during
inference
and
learning
where
we
need
to
pick
some
candidates
for
the
next
step
of
deduction
or
inductive
learning
given
the
current
inference
context
Efficient
rule
fetching
The
current
context
is
set
of
propositions
that
are
true
currently
Together
with
the
goal
which
is
the
query
Let
us
focus
on
deduction
for
the
time
being
the
same
technique
that
speeds
up
deduction
will
also
speed
up
inductive
learning
An
idea
of
Ben
Goertzel's
is
that
we
can
use
inference
traces
to
train
classifier
to
predict
the
best
inference
candidate
typical
training
example
would
be
given
as
The
goal
ie
the
conclusion
of
proof
The
context
or
premises
ie
facts
residing
in
Working
Memory
Try
to
predict
The
next
rule
involved
in
the
proof
So
the
function
we
need
is
fact
set
of
facts
rule
Using
multidimensional
scaling
we
can
map
fact
to
its
coordinates
in
high
dimensional
space
Objective
function
Our
objective
is
to
reach
the
goal
in
proof
tree
All
the
leaf
nodes
must
be
facts
drawn
from
KB
red
nodes
with
some
that
are
highlighted
by
the
current
context
purple
nodes
If
we
are
backward
chaining
BC
the
goal
is
given
If
forward
chaining
FC
we
can
find
many
goals
and
they
can
be
scored
by
interestingness
The
difficulty
of
the
rule
recommendation
problem
is
that
at
each
iteration
we
do
not
know
the
ultimate
score
of
the
conclusion
we
may
reach
The
situation
may
be
similar
to
chess
game
Factors
affecting
rule
recommendation
are
How
likely
is
the
rule
to
be
satisfied
given
the
current
context
ie
the
number
of
literals
in
the
rule
that
remains
to
be
satisfied
Look
ahead
to
see
what
the
rule
can
lead
to
if
satisfied
This
is
also
non
static
as
it
depends
on
the
current
inference
context
How
much
interestingness
the
rule
can
contribute
to
the
conclusion
Or
more
precisely
we
want
to
maximize
expected
interestingness
given
an
inference
context
The
difficulty
is
in
estimating
Notice
that
the
reachable
conclusions
given
rule
is
relatively
static
what
varies
is
the
inference
context
and
to
lesser
extent
the
facts
in
KB
Indeed
given
the
context
we
already
have
enough
information
to
derive
all
possible
conclusions
Interestingness
The
contribution
to
interestingness
by
single
rule
can
be
measured
by
How
specific
general
the
rule
is
the
most
interesting
rules
select
about
half
of
its
candidate
objects
rule
that
is
too
general
or
too
specific
is
not
very
useful
How
high
level
the
rule
is
-rule
assigns
conceptual
labels
to
its
objects
such
concepts
occupy
certain
places
in
the
ontology
Genetic
evolutionary
methods
We
can
construe
this
problem
as
game
The
moves
are
the
selection
of
rules
The
score
is
given
by
the
interestingness
of
final
conclusion
and
the
time
needed
to
arrive
at
them
Perhaps
we
need
some
way
to
classify
inference
contexts
and
rules
to
aid
the
selection
of
rules
ie
make
the
game
easier
to
play
The
genetic
algorithm
needs
to
solve
these
Organize
contexts
hierarchically
Organize
KB
hierarchically
Given
context
fetch
KB
item
using
the
above
hierarchies
Construct
proof
tree
using
unification
Alternatively
we
can
partition
the
high
dimensional
space
into
grids
and
to
each
grid
assign
bucket
of
rules
We
need
some
way
to
partition
the
high
dimensional
space
But
we
can
also
partition
the
space
of
data
points
into
many
categories
KB
organization
new
idea
is
to
organize
all
rules
in
subsumption
hierarchy
and
somehow
filter
the
context
through
it
The
Rules
Tree
has
at
its
root
the
most
general
rule
"everything
is
true
"and
grows
downward
with
successively
more
specialized
rules
In
theory
we
can
embed
the
entire
current
state
of
the
KB
in
the
Rules
Tree
by
marking
which
conjuncts
in
the
rules
are
satisfied
by
the
current
KB
but
this
may
be
impractical
due
to
the
large
number
of
instantiations
When
we
have
new
inference
context
we
can
traverse
down
the
Rules
Tree
to
the
most
specialized
satisfiable
rules
that
will
yield
the
most
specialized
derivable
conclusions
from
that
rule
Each
of
these
conclusions
will
be
fed
through
the
Rules
Tree
again
recursively
until
we
reach
enough
interesting
conclusions
The
advantage
of
searching
the
Rules
Tree
is
that
we
can
terminate
the
search
on
branch
early
on
if
the
parent
node
is
past
its
maximum
truth
value
thanks
to
subsumption
ordering
and
peaking
see
sec
subsumption
peaking
This
technique
should
be
augmented
with
rule
indexing
based
on
current
context
so
it
can
be
quickly
determined
which
rules
are
possibly
unifiable
with
the
context's
proposition
Or
perhaps
use
stochastic
local
search
that
begins
in
the
middle
of
the
Rules
Tree
But
an
interesting
conclusion
can
be
of
high
or
low
probability
In
the
next
iteration
we
have
the
previous
conclusion
The
new
conclusion
depends
on
that
Perhaps
we
can
back
track
when
stuck
-but
how
would
back
tracking
increase
new
conclusions
In
general
is
it
easier
to
satisfy
general
rules
first
Then
we
can
successively
refine
the
conclusions
But
that
is
not
true
if
probability
is
the
criterion
The
following
is
an
older
idea
What
we
need
is
in
essence
recommendation
system
given
an
inference
context
suggest
set
of
candidate
rules
An
inference
context
is
set
of
complete
or
partial
propositions
The
required
mapping
is
many
to
many
multiple
propositions
can
recommend
rule
and
rule
can
be
recommended
by
more
than
one
proposition
The
KB
is
organized
as
an
ontology
left
which
we
can
abbreviate
as
triangle
right
Each
logic
formula
can
be
classified
into
one
of
the
ontology
nodes
context
is
usually
made
up
of
several
logic
formulas
either
complete
or
partial
propositions
The
mapping
we
seek
is
Below
is
detailed
view
of
one
possible
scheme
In
this
scheme
each
proposition
in
the
context
is
mapped
to
bag
of
rules
Thus
we
ignore
the
combinatorial
effects
of
propositions
in
the
context
In
this
scheme
obviously
for
each
proposition
we
can
recommend
those
rules
that
share
at
least
one
atom
with
the
proposition
-because
only
those
rules
are
unifiable
with
the
proposition
This
is
the
maximum
set
of
rules
that
are
sensible
to
recommend
from
which
we
can
heuristically
trim
down
Among
these
rules
only
some
would
lead
to
our
desired
result
The
desired
choice
of
rules
will
depend
on
the
next
inference
steps
which
are
opaque
to
us
All
we
have
are
the
clues
from
the
current
inference
context
if
forward
chaining
or
additionally
the
goal
if
backward
chaining
Planning
and
acting
"Procedural
subsumes
Declarative
"So
far
we
have
only
talked
about
the
declarative
aspect
of
AGI
Now
we
need
to
consider
the
procedural
aspect
It
seems
natural
that
the
procedural
system
should
subsume
the
declarative
system
The
procedural
system
communicates
with
the
declarative
system
via
various
types
of
querying
The
declarative
system
itself
has
no
means
of
performing
actions
its
only
function
is
question
answering
There
may
be
exceptions
but
the
general
rule
can
be
captured
by
the
slogan
"Procedural
subsumes
Declarative
"The
action
language
We
have
represented
declarative
knowledge
using
logical
language
now
we
should
represent
procedural
knowledge
with
similar
action
language
The
following
are
examples
of
"actions
"say
hello
open
file
print
repeat
printing
until
the
user
presses
the
Esc
key
Do
they
sound
like
programming
language
tasks
Thinking
along
this
line
it
seems
advantageous
that
the
action
language
should
be
conventional
programming
language
such
as
Java
Lisp
Prolog
etc
So
the
procedural
system
expresses
actions
in
programming
language
Those
actions
can
be
executed
via
the
compiler
or
interpreter
of
that
language
In
some
situations
it
is
more
convenient
if
the
language
can
be
interpreted
The
action
output
of
the
Procedural
System
can
be
of
forms
statement
that
is
immediately
interpreted
and
executed
program
that
needs
to
be
compiled
program
fragment
that
becomes
part
of
the
Procedural
System
is
actually
form
of
procedural
learning
Procedural
learning
When
the
procedural
language
is
complex
learning
may
be
more
difficult
So
there
may
be
need
to
restrict
the
procedural
language
Learned
procedures
can
be
inserted
into
the
Procedural
System
at
certain
hook
points
Each
hook
point
represents
context
for
example
"We
get
here
when
the
user
presses
the
Esc
key
"The
procedural
learning
algorithm
should
take
such
contexts
into
consideration
"learn
by
being
told
"is
also
applicable
to
procedural
learning
Reinforcement
learning
The
goal
of
RL
is
to
learn
an
optimal
policy
which
is
function
from
the
set
of
states
to
the
set
of
actions
RL
can
learn
to
conversate
with
humans
eg
in
chat
rooms
write
programs
crawl
the
web
to
learn
things
and
do
all
these
without
the
need
for
human
programming
so
it
is
cost
effective
See
also
sec
combine
DP
RL
on
combining
deductive
planning
and
RL
Relational
reinforcement
learning
cf
One
problem
with
RL
is
that
the
number
of
states
in
general
intelligent
agent
is
too
large
in
fact
infinite
suggest
using
FOL
to
describe
the
states
so
the
formulation
can
be
more
compact
state
would
be
conjunction
of
ground
literals
such
as
horny
john
good
looking
john
has
money
john
and
the
policy
could
be
rule
that
recommends
an
action
from
state
good
looking
has
money
try
buy
cybernetic
body
Such
logical
rule
is
not
exactly
function
because
More
than
one
state
can
be
true
at
the
same
time
If
states
are
both
true
and
and
try
action
try
action
then
both
actions
will
be
recommended
when
is
true
We
need
conflict
resolution
scheme
to
resolve
this
TO
DO
RL
and
logical
reasoning
The
relationship
between
RL
and
logical
reasoning
is
very
fascinating
we
can
regard
logical
deduction
as
the
task
of
searching
for
the
proof
of
query
then
RL
can
be
used
to
perform
deduction
via
learning
the
policy
search
state
search
state
From
the
previous
section
we
see
that
relational
RL
can
vastly
increase
the
expressive
power
of
RL
through
generalization
of
states
So
the
search
states
of
the
logical
proof
space
can
be
generalized
to
what
we
call
cognitive
states
and
the
job
of
RL
is
to
learn
the
policy
cognitive
state
cognitive
state
In
this
setting
the
KB
is
part
of
the
environment
and
we'll
have
actions
that
manipulate
the
KB
such
as
adding
subtracting
and
searching
KB
items
Thus
the
entire
logical
reasoner
can
be
implemented
within
RL
The
question
now
is
how
to
represent
cognitive
states
and
their
transitions
Means
ends
analysis
MEA
MEA
is
planning
method
proposed
first
by
for
GPS
General
Problem
Solver
They
believed
that
MEA
occurs
in
human
problem
solving
It
is
forward
chaining
feature
space
(The
feature
space
is
similar
to
the
state
space
except
that
set
of
features
only
partially
describe
state
and
thus
can
economically
encompass
many
states
This
is
virtue
in
view
of
an
agent's
limited
knowledge
of
the
complex
world
)search
ie
applying
operators
forwardly
until
the
goal
state
is
reached
MEA
selects
difference
between
the
current
state
and
the
goal
state
then
selects
an
operator
that
may
reduce
the
difference
and
attempts
to
apply
the
operator
If
the
operator's
preconditions
are
not
met
MEA
recursively
calls
itself
to
transform
the
current
state
into
one
that
meets
those
conditions
If
the
conditions
are
met
MEA
applies
the
operator
and
then
recurse
to
transform
the
new
state
into
the
goal
state
from
Wikipedia
"Note
that
in
order
for
MEA
to
be
effective
the
goal
seeking
system
must
have
means
of
associating
to
any
kind
of
detectable
difference
those
actions
that
are
relevant
to
reducing
that
difference
"TO
DO
how
to
combine
MEA
with
RL
and
deductive
planning
Deductive
planning
classical
planning
problem
is
represented
by
states
and
operators
actions
Each
operator
has
its
precondition
and
effect
In
deductive
planning
based
on
situation
calculus
actions
are
represented
as
terms
with
the
special
predicates
do
and
poss
An
advantage
of
deductive
planning
is
that
the
Procedural
System
only
needs
to
be
an
inference
engine
and
nothing
else
One
difficulty
here
is
how
to
represent
states
actions
preconditions
and
effects
especially
actions
can
put
the
cup
on
the
table
The
current
state
is
represented
implicitly
by
all
the
facts
in
KB
The
actions
are
special
class
of
facts
related
to
possibilities
"Water
conducts
electricity
"is
fact
But
"water
can
conduct
electricity
"is
possible
action
Also
DP
has
problem
in
that
it
only
pursues
one
goal
at
time
Example
try
to
kill
Frank
by
tossing
radio
in
the
bathtub
while
he
is
in
it
This
may
work
because
know
that
water
conducts
electricity
and
electricity
can
kill
etc
Deductive
planning
allows
the
AGI
to
draw
general
knowledge
from
the
KB
to
represent
the
planning
problem
goal
kill
Frank
current
state
Frank
in
bathtub
radio
nearby
possible
actions
toss
radio
into
bathtub
slash
Frank's
wrist
with
knife
etc
background
knowledge
water
conducts
electricity
etc
Combining
reinforcement
learning
and
deductive
planning
DP
says
"If
do
will
probably
happen
as
result
"RL
says
"At
state
if
perform
action
the
reward
is
probably
high
"We
cannot
simply
have
planners
co
existing
-the
actions
recommended
by
DP
may
conflict
with
those
recommended
by
RL
We
may
simply
stipulate
that
the
logical
reasoner
since
it
is
cognitively
more
sophisticated
has
priority
over
RL
RL
may
invoke
DP
for
recommendation
of
action
When
DP
fails
to
find
recommendation
RL
will
act
Program
synthesis
The
most
critical
milestone
of
our
project
is
to
create
system
that
can
perform
simple
automated
programming
Traditional
program
synthesis
systems
are
hard
to
use
because
They
require
formal
specifications
of
program
requirements
which
are
often
as
hard
to
write
as
the
programs
themselves
sometimes
even
harder
The
proof
search
is
too
slow
for
any
problem
of
practical
size
or
the
systems
often
require
human
interaction
for
guidance
My
proposal
to
solve
these
two
problems
are
respectively
Allow
informal
specification
of
programming
goals
This
means
using
restricted
natural
language
including
vague
probabilistic
language
In
order
to
speed
up
the
program
search
it
seems
that
the
only
viable
solution
is
to
use
knowledge
to
guide
the
search
This
is
something
that
has
been
recognized
in
the
AI
community
for
long
time
-no
clever
search
algorithm
or
heuristic
can
improve
the
search
significantly
without
domain
specific
background
knowledge
Machine
learning
can
be
used
to
acquire
such
knowledge
but
it
is
often
an
extremely
difficult
problem
in
itself
and
we
are
also
talking
about
large
body
of
background
knowledge
As
have
argued
in
sec
learn
by
being
told
the
most
efficient
way
to
acquire
knowledge
is
to
give
up
machine
learning
and
simply
learn
by
being
told
As
with
many
AI
problems
the
program
synthesis
problem
can
be
construed
as
search
and
we
have
the
usual
choice
of
top
down
and
bottom
up
Formal
program
synthesis
Value
judgments
The
real
question
is
not
whether
machines
think
but
whether
men
do
-Skinner
My
stance
on
AGI
friendliness
do
not
have
rigorous
theory
for
AGI
friendliness
and
I'd
be
very
suspicious
of
any
such
theory
But
believe
that
AGI
technology
should
be
made
widely
available
to
the
general
public
because
the
distribution
of
power
is
the
best
safeguard
against
abuses
of
AGI
Other
than
that
the
AGI
should
be
subject
to
certain
laws
that
prevent
exploitation
eg
unlimited
growth
or
taking
up
of
physical
resources
That
would
be
the
job
of
governments
Sentient
vs
non
sentient
AGI
Vision
perception
Note
This
chapter
is
transferred
from
my
older
web
site
and
needs
some
polishing
Many
links
are
broken
It
still
represents
my
current
view
of
how
AGI
vision
should
be
achieved
but
my
focus
is
no
longer
on
vision
Design
philosophy
The
vision
module
has
the
following
features
Analytical
Visual
recognition
follows
reductionistic
scheme
in
which
visual
objects
are
broken
down
into
smaller
components
There
is
no
black
box
in
this
analytical
scheme
Holistic
The
intelligent
agent
has
complete
access
to
all
visual
details
down
to
the
smallest
features
However
this
does
not
mean
that
the
system
is
flooded
with
irrelevant
details
the
attentional
mechanism
selects
salient
features
in
scene
but
the
intelligent
agent
is
free
to
focus
attention
on
any
minute
detail
Complete
It
has
the
complete
range
of
visual
functions
to
serve
as
the
eyes
of
an
intelligent
agent
Cognitive
Visual
recognition
often
involves
reasoning
Therefore
the
vision
module
interacts
closely
with
the
cognitive
aspects
of
the
intelligent
agent
Relation
between
cognition
and
vision
What
Is
general
intelligence
The
GI
vision
approach
What
is
general
cognition
fundamental
idea
of
cognition
is
compression
of
information
Sensory
experience
in
its
raw
form
is
like
long
movie
The
Visual
Module
compresses
this
movie
into
semi
symbolic
representation
The
Cognitive
Module
further
compresses
this
information
Compression
and
achieved
by
statistical
pattern
recognition
For
example
in
the
Lena
image
the
object
in
the
center
can
be
recognized
as
face
GI
is
organized
hierarchically
with
higher
abstraction
at
the
top
At
very
high
level
the
Lena
image
may
be
compressed
symbolically
as
young
woman
wearing
hat
The
internal
representation
of
GI
may
be
data
structure
containing
objects
properties
of
objects
relations
between
objects
Some
examples
objects
face
an
apple
the
letter
properties
face
sex
female
apple
color
red
letter
font
Times
New
Roman
relations
hat
above
face
apple
on
table
AGI
is
system
that
can
automatically
invent
concepts
of
new
objects
properties
and
relations
These
new
concepts
are
in
turn
applied
to
the
internal
representation
For
example
it
may
learn
the
new
property
rectangular
by
looking
at
lot
of
rectangular
shapes
The
cognitive
vision
approach
Vision
is
basically
the
problem
of
compressing
the
sensory
input
stream
in
meaningful
way
young
woman
wearing
hat
But
there
may
be
several
levels
of
representation
As
explained
in
the
last
section
each
level
of
representation
consists
of
objects
properties
relations
The
pixel
level
is
level
not
representation
Level
consists
of
basic
syntactic
features
edges
lines
includes
straight
lines
and
curves
regions
of
uniform
color
intensity
gradients
of
color
intensity
textures
These
features
are
represented
as
objects
For
example
object
curve
property
curve
thickness
thin
relation
curve
connected
to
curve
At
level
we
may
perform
character
recognition
or
object
recognition
and
so
on
The
key
thing
here
is
that
the
vision
problem
is
solved
by
combining
GI
and
low
level
processing
Once
we
have
represented
the
image
as
objects
properties
relations
we
can
assume
that
GI
can
handle
it
Level
processing
is
hand
programmed
because
it
starts
with
pixels
Then
we
can
rely
on
the
GI
to
discover
higher
level
features
eg
the
concept
cube
The
GI
will
also
allow
us
to
hand
program
some
rules
known
as
knowledge
insertion
so
long
periods
of
learning
may
be
skipped
Machine
vision
-general
theory
and
background
The
background
information
on
this
page
is
adapted
from
ref
Bruce
Green
Georgeson
except
otherwise
stated
Background
of
general
vision
theories
Marr
Nishihara
Many
people
are
familiar
with
Marr's
vision
theory
so
won't
go
into
details
here
This
is
diagram
explaining
Marr's
general
vision
framework
from
ref
Herman
Gomes
webpage
Another
famous
diagram
from
ref
Marr
Nishihara
redrawn
by
ref
Herman
Gomes
Marr
Nishihara's
theory
is
restricted
to
describing
objects
using
set
of
generalized
cones
after
Binford
Biederman
and
geons
ref
Biederman
web
page
http
geon
usc
edu
Ebiederman
geon
usc
edu
biederman
proposed
the
recognition
by
components
theory
which
is
closely
related
to
Marr
and
Nishihara's
earlier
theory
In
Biederman's
theory
complex
objects
are
described
as
spatial
arrangements
of
basic
component
parts
known
as
geons
Geons
are
defined
by
properties
that
are
invariant
over
different
views
Some
example
geons
are
taken
from
ref
Kirkpatrick
web
page
In
particular
ref
Biederman
Hummel
proposed
neural
network
system
to
recognize
geon
based
objects
Our
approach
Re
Marr's
Biederman's
theories
Our
approach
roughly
conforms
to
Marr's
framework
of
primal
sketch
reduction
but
we
describe
the
reduction
as
It
may
not
be
such
big
difference
so
I'll
skip
the
discussion
of
this
issue
Another
similarity
with
Marr
is
that
think
the
high
level
representation
is
in
nature
But
do
not
restrict
the
representation
to
generalized
cones
only
My
theory
is
that
any
object
can
be
defined
by
surfaces
and
this
theory
is
not
restricted
to
generalized
cones
or
Biederman's
geons
The
following
examples
may
convince
you
that
some
shapes
are
not
representable
by
common
geons
In
all
of
the
above
cases
the
objects
have
parts
that
are
defined
by
some
irregular
surfaces
The
geon
theory
may
fail
in
such
cases
because
Biederman
et
al
use
neural
networks
to
learn
the
geons
and
the
geons
are
statistically
characterized
by
vertices
blobs
and
axes
This
kind
of
learning
may
be
slow
and
recognition
may
be
erratic
What
we
need
is
more
robust
theory
that
can
represent
any
shape
The
solution
is
to
use
logical
rules
to
define
geons
in
terms
of
lines
and
junctions
This
method
is
more
robust
and
can
recognize
things
other
than
common
geons
such
as
the
highheel
Shape
from
shading
and
from
texture
number
of
algorithms
have
been
developed
to
recover
shape
from
shading
with
the
aim
of
describing
the
shape
given
only
the
pattern
of
reflected
light
intensities
for
example
ref
Horn
Brooks
and
shape
from
texture
Shape
from
texture
is
particularly
hard
problem
so
we
will
handle
it
later
But
the
framework
of
reduction
still
holds
for
shape
from
It
is
relatively
easy
to
describe
shapes
in
terms
of
surfaces
and
surfaces
in
terms
of
contours
but
it
is
very
difficult
to
jump
from
set
of
pixels
straight
to
description
This
is
probably
why
many
prior
vision
theories
failed
For
example
to
recognize
the
nose
one
should
first
recognize
the
shades
and
highlights
and
contours
as
features
The
conjunction
of
these
elements
allows
us
to
recognize
that
the
nose
is
protrusion
from
the
face
and
its
particular
shape
is
jointly
defined
by
the
shapes
of
elements
It
seems
that
common
mistake
is
to
assume
that
the
brain
immediately
recognizes
the
nose
as
shape
from
pixel
level
data
without
going
through
the
intermediate
stages
because
the
brain
is
usually
unconscious
of
those
intermediate
stages
Recognizing
shading
as
features
will
require
special
algorithms
so
does
recognizing
textures
At
first
we
will
focus
on
using
exclusively
edge
detection
contours
to
recognize
objects
References
Biederman
Recognition
by
components
theory
of
human
image
understanding
Psychological
Review
Biederman
Hummel
Dynamic
binding
in
neural
network
for
shape
recognition
Psychological
Review
Binford
Visual
perception
by
computer
Paper
presented
at
the
IEEE
Conference
on
Systems
and
Control
December
Miami
Bruce
Green
Georgeson
Visual
Perception
-Physiology
Psychology
and
Ecology
Psychology
Press
NY
Gomes
Herman
Gomes
web
page
Marr's
Theory
From
primal
sketch
to
models
http
homepages
inf
ed
ac
uk
rbf
CVonline
LOCAL
COPIES
GOMES
marr
html
http
homepages
inf
ed
ac
uk
rbf
CVonline
LOCAL
COPIES
GOMES
marr
html
Horn
Brooks
Shape
from
shading
MIT
Press
Cambridge
MA
Kirkpatrick
Web
page
on
object
recognition
http
www
pigeon
psy
tufts
edu
avc
print
kirkpatrick
kirkpatrick
figprint
htm
http
www
pigeon
psy
tufts
edu
avc
print
kirkpatrick
kirkpatrick
figprint
htm
Marr
Nishihara
Representation
and
recognition
of
the
spatial
organization
of
shapes
Proceeding
of
the
Royal
Society
of
London
series
Requirements
of
general
vision
Basic
vision
scheme
Our
approach
to
solving
the
general
vision
problem
is
to
combine
vision
with
GI
general
intelligence
See
Vis
Cognition
htm
an
introduction
to
GI
vision
The
basic
tenets
of
my
vision
theory
are
Break
down
the
image
via
primitives
Apply
machine
learning
to
represent
the
image
symbolically
have
thought
about
the
vision
problem
for
years
and
studied
many
real
images
to
conclude
that
everything
under
the
sun
can
be
recognized
by
this
method
paper
will
be
published
to
explain
the
theory
in
detail
DReduction
reduction
LogicalRepresentation
Logical
representation
MachineLearning
Machine
learning
Example
Example
quadrilateral
ApproximateRecognition
Approximate
recognition
and
feedback
Attention
Searchlight
attention
Relations
Relations
between
objects
Architecture
Architecture
of
the
vision
module
reduction
The
premise
is
that
any
object
or
its
geon
like
components
can
be
defined
by
surfaces
which
are
in
turn
defined
by
lines
Please
refer
to
Vis
Background
htm
General
vision
theory
and
background
for
the
justification
of
this
point
The
image
is
first
broken
down
into
primitives
such
as
edgels
and
lines
Then
elements
are
recognized
such
as
regions
and
surfaces
Then
objects
are
recognized
blocks
geons
etc
The
decompositions
are
represented
by
graphical
data
structures
ie
nodes
and
links
Nodes
are
primitive
elements
Links
represent
the
spatial
relations
between
them
Details
of
Vis
Reduction
htm
reduction
Our
levels
may
coincide
with
David
Marr's
primal
sketch
level
Here
we
reframe
these
levels
under
the
Vis
PrimalSketch
htm
primal
sketch
framework
Logical
representation
The
first
step
is
to
transform
the
image
to
logical
representation
On
the
left
hand
side
is
the
image
we
have
applied
Sobel
edge
detection
On
the
right
hand
side
is
the
logical
representation
lines
junctions
The
blue
lines
represent
how
the
elements
are
connected
Other
details
at
the
background
have
not
been
represented
For
simple
uncluttered
scenes
this
scheme
will
work
fine
The
idea
is
that
every
detail
no
matter
how
irregular
would
be
represented
using
this
logical
representation
using
elements
such
as
blobs
and
shades
in
addition
to
lines
junctions
etc
This
approach
requires
lot
of
patience
but
ultimately
we
would
be
able
to
analyse
everything
in
the
world
Other
approaches
such
as
neural
network
or
SIFT
are
not
as
general
or
comprehensive
We
may
use
neural
networks
at
the
lowest
level
for
recognizing
edgels
Then
the
next
stage
is
to
join
the
edgels
to
recognize
longer
lines
Machine
learning
What
we
need
is
special
kind
of
machine
learning
known
as
inductive
learning
as
opposed
to
deductive
learning
In
inductive
learning
system
tries
to
induce
general
rules
from
set
of
observed
instances
learning
by
examples
There
should
be
an
underlying
knowledge
representation
KR
or
calculus
that
encompasses
what
kinds
of
rules
are
possible
The
most
common
KR
scheme
is
first
order
predicate
logic
often
abbreviated
as
FOL
first
order
logic
Other
options
include
neural
networks
semantic
networks
conceptual
graphs
Bayesian
networks
etc
It
is
not
easy
to
determine
what
kind
of
KR
is
adequate
for
our
task
at
hand
visual
recognition
Therefore
we
start
with
something
simple
and
similar
to
FOL
and
see
if
it
needs
to
be
expanded
or
modified
Details
of
Vis
InductiveLearning
htm
inductive
learning
Example
quadrilateral
Any
figure
with
sides
straight
lines
Define
the
predicate
Terminates
edge
vertex
to
indicate
when
an
edge
terminates
with
vertex
This
results
in
the
set
of
logical
statements
Terminates
edge
vertex
true
Terminates
edge
vertex
true
Terminates
edge
vertex
true
Terminates
edge
vertex
true
Terminates
edge
vertex
true
Terminates
edge
vertex
true
Terminates
edge
vertex
true
Terminates
edge
vertex
true
Perhaps
we
can
introduce
new
predicate
Connects
edge
vertex
vertex
to
simplify
the
above
to
Connects
edge
vertex
vertex
true
Connects
edge
vertex
vertex
true
Connects
edge
vertex
vertex
true
Connects
edge
vertex
vertex
true
Assuming
that
the
universe
is
connected
graph
that
the
system
is
currently
paying
attention
to
now
we
can
easily
define
the
ary
predicate
Quadrilateral
using
typed
logic
Quadrilateral
edge
edge
edge
edge
vertex
vertex
vertex
vertex
Connects
Connects
Connects
Connects
This
is
just
an
example
Please
refer
to
this
page
concerning
various
issues
of
Vis
InductiveLearning
htm
inductive
learning
Some
other
issues
specific
to
vision
are
discussed
as
follows
Approximate
recognition
and
feedback
One
problem
is
that
primitive
features
are
often
fuzzy
and
should
be
approximately
recognized
For
example
in
the
image
below
edges
and
vertex
are
almost
invisible
yet
given
the
current
context
they
should
be
interpreted
as
edges
and
vertex
The
context
is
important
because
very
weak
features
would
be
regarded
as
noise
otherwise
The
feedback
mechanism
should
work
this
way
When
the
Recognizer
finds
that
concept
is
almost
recognized
eg
with
the
majority
of
conjuncts
being
true
it
will
select
the
remaining
features
that
are
not
yet
matched
and
send
them
to
the
lower
level
Recognizer
which
would
then
lower
its
threshold
for
recognizing
those
features
This
requires
things
The
Recognizer
should
measure
degree
of
certainty
associated
with
each
feature
being
recognized
The
Recognizer
at
the
lower
level
should
be
able
to
use
an
feedback
cue
to
look
for
certain
features
This
may
require
performing
an
inversion
of
the
cue
detailed
explanation
of
the
feedback
mechanism
will
be
presented
soon
Searchlight
attention
Another
problem
is
that
real
world
images
are
often
composed
of
many
cluttered
elements
so
we
need
to
use
searchlight
to
look
for
individual
objects
in
cluttered
scene
Searchlight
attention
is
closely
related
to
the
feedback
mechanism
outlined
above
Due
to
limited
computational
resources
we
can
only
extract
features
within
fovea
of
attention
If
concept
is
detected
to
be
almost
complete
the
searchlight
will
direct
the
fovea
to
focus
on
areas
that
are
likely
to
complete
the
concept
with
concomitant
decrease
of
attention
to
other
areas
This
requires
the
searchlight
to
know
where
to
search
for
the
feedback
cue
In
sense
this
also
requires
inversion
of
the
cue
Relations
between
objects
The
vision
system
not
only
has
to
recognize
individual
objects
but
also
relations
among
them
represented
as
links
The
way
to
achieve
this
is
to
pay
attention
to
individual
objects
sequentially
Relations
are
then
recognized
by
the
identities
of
the
objects
in
the
sequence
and
by
how
the
searchlight
moved
In
our
logical
formulation
an
object
is
recognized
by
ary
predicate
such
as
Cube
Then
we
have
to
bring
this
object
to
the
next
level
of
recognition
where
it
is
represented
by
variable
such
as
cube
Only
then
we
would
be
allowed
to
denote
relation
like
Above
cube
cube
at
this
level
Recognition
at
each
level
is
independent
of
recognition
at
other
levels
except
for
the
feedback
mechanism
The
main
loop
of
the
Recognizer
would
be
using
the
searchlight
to
scan
around
the
image
and
recognizing
individual
objects
When
the
searchlight
moves
its
movement
will
be
recorded
and
later
used
to
form
the
link
between
the
current
object
and
the
next
object
Architecture
of
the
vision
module
The
operation
of
the
above
module
is
typical
of
rule
based
system
except
that
there
is
loop
in
the
lower
right
corner
that
repeats
the
pattern
matching
process
at
multiple
levels
What
this
means
is
that
the
raw
sensory
experience
goes
through
multiple
stages
of
memory
consolidation
via
pattern
matching
For
details
please
refer
to
GI
MemorySystems
htm
Memory
systems
This
architecture
has
to
be
integrated
with
the
larger
GI
general
intelligence
framework
to
form
complete
intelligent
agent
Please
refer
to
GI
architecture
htm
GI
architecture
Details
of
reduction
An
illustrated
example
cube
is
Vis
Example
htm
here
incomplete
The
following
is
very
tentative
outline
the
order
of
steps
may
be
wrong
and
additional
steps
may
be
needed
Stage
Analysis
Edge
detection
-distinguish
between
single
pixel
lines
and
thick
lines
Vis
DTaxonomy
htm
Classification
of
lines
straight
lines
curves
junctions
From
this
point
on
the
representation
is
collection
of
discrete
lines
and
junctions
qualitative
structural
rather
than
quantitative
spatial
continuous
Good
continuation
-line
curve
completion
Contour
simplification
-the
image
is
represented
by
both
simple
and
detailed
contours
allowing
redundency
in
representation
Some
things
can
be
recognized
at
this
stage
text
handwriting
graphics
Stage
Analysis
Characterization
of
regions
according
to
color
shading
or
textures
segmentation
Classification
of
shapes
Extract
gestalt
induced
contours
-numerous
small
identical
similar
elements
may
induce
contours
the
absence
of
features
induces
invisible
contours
Deal
with
occlusion
Some
graphics
may
be
recognized
at
this
stage
Stage
Analysis
Vis
DTaxonomy
htm
Classification
of
lines
surfaces
Identify
objects
according
to
templates
or
algorithms
The
question
is
how
to
define
an
object
such
as
the
object
classes
bottle
books
faces
etc
Primal
sketch
project
Background
According
to
Marr
the
primal
sketch
represents
changes
in
light
intensity
occuring
over
space
in
the
image
It
also
organises
these
local
descriptions
of
intensity
change
into
representation
of
image
regions
and
the
boundaries
between
them
Specifically
the
primal
sketch
may
include
elements
such
as
edges
curves
or
straight
lines
color
blobs
ends
junctions
of
edges
texons
etc
Marr's
vision
scheme
consists
of
levels
primal
sketch
My
Vis
BasicScheme
htm
vision
scheme
is
slightly
different
which
goes
from
to
This
difference
may
not
be
all
that
important
what
is
important
is
how
best
to
recognize
features
at
various
levels
from
computational
viewpoint
The
primal
sketch
is
an
essential
stage
of
any
complete
vision
system
What
propose
is
that
the
output
of
the
primal
sketch
should
be
represented
as
symbolic
web
with
links
that
are
logical
predicates
Developing
such
low
level
layer
would
make
tremendous
contribution
towards
computer
vision
What
we're
trying
to
do
Obtaining
the
primal
sketch
One
solution
to
the
primal
sketch
problem
have
not
surveyed
the
topic
completely
is
presented
in
Ref
Gao
Zhu
Wu
Primal
Sketch
Integrating
Texture
and
Structure
http
www
stat
ucla
edu
sczhu
papers
primal
sketch
pdf
pdf
by
researchers
in
the
UCLA
Center
for
Image
and
Vision
Science
As
explained
in
the
paper
an
input
image
is
separated
into
regimes
one
sketchable
and
the
other
non
sketchable
The
sketchable
regime
is
one
of
low
entropy
where
the
image
can
be
represented
by
sparse
coding
sum
of
primitive
features
the
non
sketchable
regime
is
of
relatively
higher
entropy
where
sparse
coding
is
no
longer
practical
and
those
areas
of
the
image
should
be
represented
as
textures
In
our
project
we
were
trying
to
deal
with
the
type
of
low
level
feature
extraction
in
the
sketchable
regime
while
temporarily
ignoring
textures
I'm
currently
trying
to
have
collaboration
with
the
UCLA
group
or
possibly
license
their
technology
Output
an
attributed
graph
After
obtaining
the
primal
sketch
the
second
task
is
to
represent
the
image
as
an
attributed
graph
This
has
been
partly
accomplished
in
Ref
Han
Zhu
Bottom
up
Top
down
Parsing
with
Attributed
Graph
Grammar
http
www
stat
ucla
edu
sczhu
papers
PAMI
Grammar
rectangle
pdf
pdf
for
some
simple
shapes
The
goal
of
our
project
is
to
output
attributed
graphs
similar
to
the
above
kind
for
all
sorts
of
images
The
expressiveness
of
the
attributed
graph
may
be
comparable
to
that
of
first
order
predicate
logic
Then
the
next
step
is
to
delegate
the
tasks
of
recognition
and
inference
to
an
intelligent
agent
or
cognitive
architecture
Handover
to
intelligent
agent
The
following
tasks
may
be
handled
by
the
intelligent
agent
pattern
recognition
attentional
mechanism
eg
focusing
attention
to
various
features
or
objects
learning
of
concepts
and
facts
declarative
and
episodic
memory
complex
inference
for
example
chair
is
anything
that
can
be
sat
on
may
involve
procedural
memory
The
division
of
labor
can
avoid
repeating
these
tasks
for
vision
and
for
general
cognition
Currently
we
are
considering
the
following
intelligent
agents
for
integration
with
vision
Novamente
has
an
architecture
integrating
perception
and
cognition
Soar
has
provisions
for
sensory
perception
Cyc
depends
on
its
proprietary
inference
engine
ACT
EPIC
IDA
intelligent
distribution
agent
Stan
Franklin
Basically
any
general
cognitive
architecture
that
can
handle
sensory
perception
can
use
our
vision
module
References
Guo
Zhu
Wu
Primal
Sketch
Integrating
Texture
and
Structure
Computer
Vision
and
Image
Understanding
Accepted
for
the
Special
Issue
on
Generative
Model
Based
Vision
http
www
stat
ucla
edu
sczhu
papers
primal
sketch
pdf
pdf
Han
Zhu
Bottom
up
Top
down
Image
Parsing
with
Attribute
Graph
Grammar
Statistical
preprint
Submitted
to
PAMI
http
www
stat
ucla
edu
sczhu
papers
PAMI
Grammar
rectangle
pdf
pdf
Classification
of
Shapes
Here
attempt
to
classify
all
shapes
contours
into
primitive
elements
lines
curves
junctions
My
view
is
that
the
vision
problem
especially
the
recognition
of
objects
can
be
solved
via
process
All
objects
can
be
defined
by
elements
surfaces
and
all
shapes
areas
can
be
defined
by
contours
Main
Classes
All
shapes
can
be
decomposed
into
Properties
thickness
color
texture
Straight
Lines
Properties
position
length
orientation
end
points
Curves
subtypes
Simple
Curves
Curves
with
no
inflections
Properties
position
length
orientation
end
points
curvature
Ellipses
Circles
Properties
position
orientation
size
axes
ratio
Complex
Curves
Properties
position
length
end
points
of
inflections
curvatures
Complex
Loops
Properties
position
area
shape
description
of
inflections
Junctions
Properties
position
of
lines
angles
subtypes
May
have
lines
of
convergence
Examples
The
following
examples
illustrate
the
concept
of
self
aggregation
which
means
edgel
detectors
or
neurons
aggregating
with
others
with
similar
attributes
such
as
color
orientation
etc
line
with
slight
bend
the
bending
is
noticed
even
though
it
is
very
slight
because
the
parts
are
very
straight
in
contrast
This
line
has
many
bends
but
is
still
recognized
as
continuous
line
For
the
following
line
simple
algorithm
may
classify
it
as
curve
but
people
will
describe
it
as
two
slightly
curved
lines
bending
at
point
This
illustrates
the
context
dependent
aspect
of
human
vision
The
following
is
yet
another
context
dependent
example
we
see
shaky
but
straight
lines
bending
at
an
angle
What
is
straight
line
is
relative
is
straighter
than
but
in
the
above
context
is
also
considered
straight
line
People
will
describe
the
following
as
regular
sine
curve
This
is
also
simple
curve
that
people
can
describe
with
basic
features
such
as
crest
and
trough
and
being
smooth
People
can
also
characterize
line
junctions
with
special
attributes
eg
smooth
junction
This
can
be
considered
as
line
with
colors
This
is
also
considered
simple
curve
with
internal
texture
Classification
of
features
Classification
of
Shapes
There
are
limited
number
of
possible
contours
and
junctions
The
technique
of
labeling
all
contours
and
junctions
consistently
is
known
as
relaxation
labeling
and
has
grown
into
large
area
of
work
We
should
work
out
the
rules
of
arbitrary
object
construction
Edges
discontinuity
of
distance
of
object
discontinuity
of
normal
of
object
surface
DC
discontinuity
of
distance
of
object
continuous
normal
of
object
surface
discontinuity
of
normal
of
object
surface
change
of
reflectance
shadow
Each
type
of
contour
has
its
characteristics
such
as
shading
Such
characteristics
help
to
infer
its
type
Junctions
There
are
topologically
possible
line
junctions
in
the
trihedral
blocks
world
and
indicates
whether
crease
is
convex
or
concave
respectively
An
arrow
indicates
blade
formed
by
discontinuity
of
distance
same
as
the
type
contour
above
The
direction
of
the
arrow
indicates
which
side
is
the
occluding
surface
which
by
convention
is
to
the
right
We
should
extend
this
analysis
to
arbitrary
shapes
Inductive
learning
for
vision
Inductive
logic
programming
seems
to
be
the
harder
part
of
our
project
in
fact
ILP
is
considered
one
of
the
hardest
areas
in
computer
science
Background
to
ILP
inductive
logic
programming
Books
and
internet
resources
Machine
Learning
Mitchell
has
chapter
on
Learning
Sets
of
Rules
which
is
good
introduction
AI
Modern
Approach
talks
about
ILP
in
chapter
This
book
by
Lavrac
and
Dzeroski
is
free
for
download
http
www
ai
ijs
si
SasoDzeroski
ILPBook
Inductive
Logic
Programming
Techniques
and
Applications
Stephen
Muggleton's
website
on
ILP
http
www
doc
ic
ac
uk
shm
www
doc
ic
ac
uk
shm
Peter
Flach's
website
http
www
cs
bris
ac
uk
flach
www
cs
bris
ac
uk
flach
and
his
PowerPoint
on
http
macflach
cs
bris
ac
uk
flach
presentations
CL
HTML
CL
ppt
Knowledge
Representation
and
ILP
many
more
simple
ILP
example
To
learn
the
relation
Daughter
database
may
have
the
following
facts
Male
mary
false
Female
mary
true
Mother
mary
louise
true
Father
mary
bob
true
Daughter
bob
mary
true
etc
After
presenting
large
number
of
such
facts
this
rule
may
be
learned
IF
Father
Female
THEN
Daughter
Existing
ILP
software
FOIL
CIGOL
logic
spelt
backwards
GOLEM
PROGOL
LINUS
etc
Special
concerns
related
to
vision
The
fovea
focuses
on
particular
area
or
scale
of
an
image
Low
level
feature
extraction
results
in
graphical
representation
of
the
scene
The
nodes
and
links
of
this
graphical
representation
is
then
converted
to
set
of
logical
statements
This
set
is
then
taken
to
be
the
model
possible
world
of
the
logic
Low
level
feature
extraction
has
to
use
the
searchlight
mechanism
to
discover
the
spatial
relations
among
features
and
then
outputs
graphical
logical
representation
for
the
next
level
The
universe
of
the
logic
at
each
level
is
distinct
from
each
other
Why
use
logic
Our
scheme
is
to
reduce
to
to
to
Along
this
route
many
concepts
need
to
be
defined
First
we
have
the
image
decomposed
into
edgels
which
are
the
lowest
level
features
Then
we
define
lines
and
curves
as
collections
of
contiguous
edgels
From
to
line
conjunction
of
edgels
Then
we
define
polygons
parallelograms
squares
etc
as
collections
of
lines
From
to
surface
conjunction
of
lines
Then
we
can
define
cylinders
blocks
etc
as
collection
of
surfaces
From
to
volume
conjunction
of
surfaces
Then
we
define
objects
such
as
bicycles
wheels
etc
as
composed
of
primitive
geons
If
we
can
enter
all
these
things
as
rules
in
knowledgebase
we
have
vision
system
that
operates
by
the
rules
Our
strategy
is
to
encode
these
rules
as
logical
formulas
Why
not
neural
networks
The
use
of
logic
to
represent
rules
definitions
has
tremendous
advantage
over
neural
networks
For
example
quadrilateral
is
defined
as
any
figure
with
sides
Quadrilaterals
can
appear
in
all
sizes
and
shapes
yet
we
have
no
problem
recognizing
any
of
them
With
predicate
logic
we
can
easily
define
quadrilateral
in
terms
of
edges
and
vertices
an
example
is
given
in
Vis
BasicScheme
htm
Vision
scheme
The
resulting
formula
is
very
succinct
and
is
capable
of
covering
all
possible
cases
ie
it
has
low
algorithmic
complexity
On
the
other
hand
in
neural
networks
we
have
to
learn
the
concept
of
quadrilaterals
in
high
dimensional
space
populated
with
many
instances
of
quadrilaterals
Such
statistical
learning
method
continually
molds
the
classification
space
into
the
right
shape
with
incremental
steps
which
is
extremely
inefficient
and
prone
to
errors
In
other
words
it
fails
to
reduce
algorithmic
complexity
significantly
The
essense
of
logic
is
that
it
can
represent
objects
and
their
relations
Objects
are
variables
and
relations
are
predicates
The
use
of
variables
allows
logic
to
represent
many
things
succinctly
For
example
can
use
the
predicate
Kicks
to
mean
kicks
The
predicate
can
be
used
to
denote
boy
kicks
ball
girl
kicks
dog
or
John
kicks
Mary
even
though
these
events
are
very
dissimilar
Thus
the
great
expressiveness
of
predicate
logic
Most
artificial
neural
networks
nowadays
cannot
express
things
with
variables
You
may
argue
that
the
brain
is
neural
network
and
so
they
must
be
capable
of
achieving
vision
and
cognition
guess
the
answer
is
that
the
brain
probably
uses
neurons
to
perform
some
sort
of
symbolic
logical
computations
rather
than
statistical
learning
as
many
neuroscientists
now
assume
Unfortunately
how
the
cortex
performs
computation
is
still
terra
incognita
What's
the
use
of
inductive
learning
For
very
simple
vision
system
inductive
learning
is
not
needed
All
we
need
is
to
patiently
enter
definitions
of
shapes
objects
into
logical
knowledgebase
Then
the
system
can
recognize
those
things
Inductive
learning
is
needed
for
purposes
teaching
by
show
and
tell
automatically
discovering
useful
concepts
Show
and
tell
It
is
far
easier
to
show
the
computer
number
of
examples
of
an
object
eg
pineapple
than
to
explicitly
define
the
general
appearance
of
that
object
The
inductive
learner
will
automatically
generalize
from
the
exemplars
Automatic
discovery
of
useful
concepts
This
is
an
advanced
topic
but
of
great
practical
value
useful
concept
is
one
that
helps
meaningfully
to
characterize
broad
class
of
objects
For
example
the
concept
of
leaf
may
characterize
the
leaves
often
attached
to
many
types
of
fruits
and
may
also
apply
to
flowers
and
trees
So
it
is
useful
concept
Another
useful
concept
is
parallelogram
which
often
occurs
in
the
views
of
rectangular
blocks
If
we
do
not
use
the
concept
of
parallelogram
then
the
description
of
rectangular
blocks
may
become
cumbersome
In
other
words
useful
concept
reduces
description
lengths
But
if
we
are
talking
about
the
total
description
length
of
knowledgebase
then
the
discovery
of
good
concepts
requires
global
searching
This
is
therefore
computational
hard
problem
Inductive
learning
methods
We
are
currently
looking
for
researchers
in
ILP
to
collaborate
with
us
to
develop
the
Inductive
Learner
Outstanding
questions
Do
we
need
some
special
inference
operations
outside
of
predicate
logic
Such
as
stochastic
logic
When
doing
higher
level
recognition
does
the
Recognizer
need
to
be
aware
of
the
constituents
of
high
level
features
Eg
that
line
is
collection
of
edgels
Depth
distance
estimation
Motion
and
event
detection
Texture
Stereopsis
Sample
images
Sample
images
are
currently
hosted
at
http
www
geocities
com
genericai
Vis
SampleImages
htm
but
the
site
will
soon
be
closed
around
October
will
try
to
relocate
them
somewhere
Implementation
The
sooner
you
start
coding
your
program
the
longer
it's
going
to
take
-Ledgard
Premature
optimization
is
the
root
of
all
evil
-Donald
Knuth
The
First
Rule
of
Program
Optimization
Don't
do
it
The
Second
Rule
of
Program
Optimization
for
experts
only
Don't
do
it
yet
-Michael
Jackson
Choice
of
programming
languages
Background
breif
survey
of
programming
languages
as
relates
to
AGI
Lisp
is
the
single
most
important
programming
language
ever
invented
in
the
history
of
computer
science
Much
of
the
research
code
in
classical
AI
was
written
in
Lisp
and
to
this
day
Lisp
and
dialects
like
Scheme
remains
very
practical
language
Genifer's
first
rapid
prototyping
is
in
Lisp
Lisp
was
first
implemented
as
an
interpreter
on
an
IBM
Lisp
is
based
on
calculus
which
is
also
important
in
the
study
of
logic
Prolog
seems
not
as
good
as
Lisp
because
it
imposes
the
restriction
of
Horn
clauses
over
full
first
order
logic
and
forces
the
programmer
to
use
SLD
resolution
with
depth
first
search
strategy
Though
Peter
Norvig
pointed
out
this
is
not
severe
limitation
of
Prolog
as
compared
to
Lisp
Interestingly
logic
based
AGI
itself
is
like
an
advanced
version
Prolog
interpreter
enhanced
with
better
search
strategies
eg
best
first
search
probabilities
fuzziness
eg
fuzzy
Prolog
higher
order
unification
as
in
Prolog
abduction
as
abductive
logic
programming
induction
as
inductive
logic
programming
etc
Thus
good
understanding
of
Prolog
is
essential
to
the
study
of
AGI
ML
was
created
by
Robin
Milner
in
the
for
the
purpose
of
automated
theorem
proving
ML's
type
system
helps
to
ensure
that
theorems
are
proved
correctly
ML
is
used
to
develop
the
LCF
logic
of
computable
functions
series
of
theorem
provers
which
influenced
HOL
Isabelle
and
HOL
Light
OCaml
is
derived
from
ML
The
CAM
in
OCaml
stands
for
"categorical
abstract
machine
"which
is
based
on
categorical
combinatory
logic
variant
of
combinatory
logic
influenced
by
category
theory
Haskell
is
descended
from
ML
and
has
an
elegant
syntax
very
close
to
mathematics
The
optimizing
compiler
built
by
Simon
Peyton
Jones
at
Glasgow
makes
it
very
fast
language
in
recent
benchmarks
Lazy
evaluation
is
also
strong
point
when
implementing
symbolic
AI
algorithms
Low
level
languages
Object
orientation
is
not
particularly
natural
for
some
software
architectures
Java
is
preferable
to
for
its
cross
platform
maturity
may
be
too
old
But
personally
prefer
to
Not
bad
for
AGI
in
my
opinion
is
also
the
choice
of
OpenCog
Bootstrapping
Genifer
in
Genifer
The
design
of
Genifer
is
indifferent
to
the
choice
of
programming
languages
My
latest
idea
is
to
bootstrap
Genifer
in
its
own
language
Business
aspects
Sure
now
what's
worth
doing
is
worth
doing
for
money
-Wall
Street
movie
Capitalism
AI
and
the
Singularity
Of
course
it
is
impossible
to
predict
things
past
the
Singularity
but
my
view
of
the
Singularity
is
more
conservative
in
the
sense
that
believe
human
life
will
remain
unchanged
in
some
essential
aspects
My
premise
is
that
capitalism
will
persist
after
the
advent
of
AI
My
political
stance
As
an
AGI
entrepreneur
need
not
be
involved
in
politics
and
have
no
intention
to
do
so
but
some
people
are
frequently
trying
to
politicize
the
issue
to
gain
an
advantage
over
me
so
am
forced
to
clarify
few
things
here
Firstly
am
not
anti
American
My
goal
is
to
create
global
AGI
project
without
bias
towards
particular
nationality
This
does
not
mean
that
am
advocating
to
put
an
end
to
nationalism
The
internet
allows
us
to
work
across
geographic
boundaries
so
the
conditions
are
right
for
this
kind
of
trans
national
collaboration
Secondly
think
open
source
as
software
business
model
is
flawed
Companies
need
to
protect
new
ideas
to
give
people
incentives
to
innovate
Collaborative
platform
From
April
onwards
all
http
spreadsheets
google
com
ccc
key
Ah
SExak
dFA
YTQ
anRfOWxrR
JMM
toekpvVEE
hl
en
accounting
information
of
this
project
are
disclosed
Virtual
credits
BitCoin
Voting
scheme
Quick
start
guide
to
AGI
This
is
the
quickest
way
to
get
up
to
speed
from
to
AGI
Warning
These
recommendations
are
subjective
If
you
want
to
get
quick
understanding
of
neuroscience
strictly
speaking
you
may
not
even
need
to
Browse
but
don't
read
book
about
the
human
brain
such
as
http
www
amazon
com
Human
Brain
Introduction
Functional
Anatomy
dp
ref
sr
ie
UTF
books
qid
sr
The
Human
Brain
textbook
of
neurochemistry
such
as
http
www
amazon
com
Basic
Neurochemistry
Seventh
Molecular
Cellular
dp
ref
sr
ie
UTF
books
qid
sr
Basic
Neurochemistry
textbook
on
the
neuron
such
as
http
www
amazon
com
Neuron
Cell
Molecular
Biology
dp
ref
sr
ie
UTF
books
qid
sr
The
Neuron
book
on
modeling
single
neuron
such
as
http
www
amazon
com
Biophysics
Computation
Information
Computational
Neuroscience
dp
ref
sr
ie
UTF
books
qid
sr
Biophysics
of
Computation
Information
Processing
in
Single
Neurons
book
on
modeling
the
whole
brain
such
as
http
www
amazon
com
Memory
Attention
Decision
Making
computational
neuroscience
dp
ref
ntt
at
ep
dpt
Memory
Attention
and
Decision
Making
unifying
computational
neuroscience
approach
Then
you
will
get
an
idea
of
the
complexity
of
wetware
and
general
sense
of
how
intractable
it
is
to
re
engineer
the
brain
except
by
brute
force
So
we
should
give
it
up
Note
the
analogy
it
is
easier
to
engineer
flying
machine
with
novel
design
rather
than
exactly
copying
the
bird
To
learn
the
basics
of
AI
the
status
quo
of
current
AI
get
one
of
these
AI
bibles
AIMA
Luger
Winston
Nilsson
To
learn
logic
Chang
Lee
Fitting
higher
order
logic
lambda
calculus
combinatory
logic
term
rewriting
Recommended
books
to
learn
programming
languages
in
the
context
of
AI
Prolog
Ivan
Bratko
Lisp
PAIP
or
Winston
ML
Paulson
ML
for
the
Working
Programmer
Haskell
"Algorithms
"by
Rabhi
Lapalme
Recommended
books
on
Inductive
logic
programming
de
Raedt
Fuzzy
logic
Bayesian
networks
Pearl
Knowledge
representation
Levesque
Brachman
Natural
language
understanding
Jurafsky
Martin
Symbols
name
ATMS
description
assumption
based
truth
maintenance
system
name
ATP
description
automated
theorem
proving
name
backward
chaining
description
deduction
algorithm
that
makes
deduction
steps
backwards
starting
with
the
goal
to
be
proved
name
BDI
description
belief
desire
intention
architecture
name
BN
description
Bayesian
network
name
CDF
description
cumulative
distribution
function
name
CNF
description
conjunctive
normal
form
name
DNF
description
disjunctive
normal
form
name
DP
description
deductive
planning
name
EA
description
evolutionary
algorithm
name
FOL
description
first
order
logic
name
forward
chaining
description
deduction
algorithm
that
performs
deduction
steps
starting
from
the
premises
name
ground
description
logic
formula
is
ground
if
it
does
not
contain
variables
name
HO
description
higher
order
name
HOL
description
higher
order
logic
name
IE
description
inference
engine
name
ILP
description
inductive
logic
programming
name
JTMS
description
justification
based
truth
maintenance
system
name
KB
description
knowledge
base
name
KR
description
knowledge
representation
name
LBAI
description
logic
based
AI
name
LGG
description
least
general
generalization
name
literal
description
logic
literal
is
an
atomic
formula
or
its
negation
name
LTM
description
Long
Term
Memory
name
MDL
description
minimum
description
length
name
MEA
description
means
end
analysis
name
NL
description
natural
language
name
NP
description
noun
phrase
name
NR
description
natural
reasoning
common
sense
human
like
reasoning
name
PCA
description
principal
component
analysis
name
PDF
description
probability
density
function
name
predicate
logic
description
any
logic
of
first
order
or
above
ie
not
propositional
logic
name
RL
description
reinforcement
learning
name
RRL
description
relational
reinforcement
learning
name
RSI
description
recursive
self
improvement
name
rule
description
logic
formula
with
variables
opposite
of
ground
name
SAT
description
logical
satisfiability
name
SLD
description
selective
linear
definite
clause
resolution
name
SLS
description
stochastic
local
search
name
SVM
description
support
vector
machines
name
term
description
terms
is
recursively
defined
as
either
variable
constant
or
applications
of
functions
to
arguments
which
are
terms
name
TRS
description
term
rewriting
system
name
TV
description
truth
value
name
VP
description
verb
phrase
name
WM
description
Working
Memory
part
of
the
KB
that
has
faster
processing
speed
name
ZOL
description
zeroth
order
logic
propositional
logic
plainnat
Acknowledgements
In
addition
to
the
people
listed
on
the
title
page
I'd
like
to
thank
the
AGI
mailing
list
participants
for
years
of
discussions
